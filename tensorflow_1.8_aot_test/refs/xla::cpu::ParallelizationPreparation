<inh f='tensorflow/tensorflow/compiler/xla/service/hlo_pass_interface.h' l='29' c='xla::HloPassInterface'/>
<def f='tensorflow/tensorflow/compiler/xla/service/cpu/cpu_parallelization_preparation.h' l='34' ll='75'/>
<size>48</size>
<doc f='tensorflow/tensorflow/compiler/xla/service/cpu/cpu_parallelization_preparation.h' l='26'>// This pass prepares an HLO module for parallel execution by transforming
// subgraphs of the top-level computation into embedded computations which can
// be executed in parallel.
// TODO(b/29630486): Currently, it is limited to turning all instructions (which
// are not constants or parameters) in the entry computation into embedded
// computations.  However, it could make sense to coarsen the parallelization to
// improve cache locality.  Also, we will need to do something to intelligently
// handle While constructs.</doc>
<fun r='_ZN3xla3cpu26ParallelizationPreparationC1ExRKSt8functionIFxRKNS_5ShapeEEE'/>
<fun r='_ZN3xla3cpu26ParallelizationPreparationD1Ev'/>
<fun r='_ZNK3xla3cpu26ParallelizationPreparation4nameEv'/>
<fun r='_ZN3xla3cpu26ParallelizationPreparation3RunEPNS_9HloModuleE'/>
<fun r='_ZN3xla3cpu26ParallelizationPreparation25RunParallelTaskAssignmentEPNS_9HloModuleE'/>
<fun r='_ZN3xla3cpu26ParallelizationPreparation32OutlineParallelizableInstructionEPNS_14HloInstructionE'/>
<fun r='_ZN3xla3cpu26ParallelizationPreparation18CanOutlineWithUserEPNS_14HloInstructionE'/>
<fun r='_ZN3xla3cpu26ParallelizationPreparation21AssignedParallelTasksEPNS_14HloInstructionE'/>
<mbr r='xla::cpu::ParallelizationPreparation::max_parallelism_' o='64' t='const int64'/>
<mbr r='xla::cpu::ParallelizationPreparation::shape_size_' o='128' t='const HloCostAnalysis::ShapeSizeFunction'/>
