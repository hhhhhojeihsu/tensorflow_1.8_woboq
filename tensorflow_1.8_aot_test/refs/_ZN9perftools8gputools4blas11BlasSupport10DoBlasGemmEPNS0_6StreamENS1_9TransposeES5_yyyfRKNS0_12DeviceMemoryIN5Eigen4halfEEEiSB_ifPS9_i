<dec f='tensorflow/tensorflow/stream_executor/blas.h' l='953' type='bool perftools::gputools::blas::BlasSupport::DoBlasGemm(perftools::gputools::Stream * stream, blas::Transpose transa, blas::Transpose transb, uint64 m, uint64 n, uint64 k, float alpha, const DeviceMemory&lt;Eigen::half&gt; &amp; a, int lda, const DeviceMemory&lt;Eigen::half&gt; &amp; b, int ldb, float beta, DeviceMemory&lt;Eigen::half&gt; * c, int ldc)'/>
<doc f='tensorflow/tensorflow/stream_executor/blas.h' l='942'>// Computes a matrix-matrix product with general matrices:
  //
  //     c &lt;- alpha * op(a) * op(b) + beta * c,
  //
  // op(X) is one of op(X) = X, or op(X) = X&apos;, or op(X) = conj(X&apos;); alpha and
  // beta are scalars; a, b, and c are matrices; op(a) is an m-by-k matrix;
  // op(b) is a k-by-n matrix; c is an m-by-n matrix.
  //
  // Note: The half interface uses float precision internally; the version
  // that uses half precision internally is not yet supported. There is no
  // batched version of the half-precision interface.</doc>
<use f='tensorflow/tensorflow/stream_executor/stream.cc' l='3610' u='a' c='_ZN9perftools8gputools6Stream12ThenBlasGemmENS0_4blas9TransposeES3_yyyfRKNS0_12DeviceMemoryIN5Eigen4halfEEEiS9_ifPS7_i'/>
