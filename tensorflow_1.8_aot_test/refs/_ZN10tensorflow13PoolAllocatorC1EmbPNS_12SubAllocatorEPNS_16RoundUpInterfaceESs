<dec f='tensorflow/tensorflow/core/common_runtime/gpu/pool_allocator.h' l='59' type='void tensorflow::PoolAllocator::PoolAllocator(size_t pool_size_limit, bool auto_resize, tensorflow::SubAllocator * allocator, tensorflow::RoundUpInterface * size_rounder, string name)'/>
<def f='tensorflow/tensorflow/core/common_runtime/gpu/pool_allocator.cc' l='34' ll='48' type='void tensorflow::PoolAllocator::PoolAllocator(size_t pool_size_limit, bool auto_resize, tensorflow::SubAllocator * allocator, tensorflow::RoundUpInterface * size_rounder, string name)'/>
<doc f='tensorflow/tensorflow/core/common_runtime/gpu/pool_allocator.h' l='49'>// &quot;pool_size_limit&quot; is the maximum number of returned, re-usable
  // memory buffers to keep in the pool.  If pool_size_limit == 0, the
  // pool is effectively a thin wrapper around the allocator.
  // If &quot;auto_resize&quot; is true, then the pool_size_limit will gradually
  // be raised so that deallocations happen very rarely, if at all.
  // Transitory start-up objects may deallocate, but the long-term
  // working-set should not. Auto-resizing can raise pool_size_limit
  // but will never lower it.
  // &quot;allocator&quot; is the object that performs the underlying memory
  // malloc/free operations.  This object takes ownership of allocator.</doc>
<use f='tensorflow/tensorflow/core/common_runtime/gpu/process_state.cc' l='211' u='c' c='_ZN10tensorflow12ProcessState15GetCPUAllocatorEi'/>
