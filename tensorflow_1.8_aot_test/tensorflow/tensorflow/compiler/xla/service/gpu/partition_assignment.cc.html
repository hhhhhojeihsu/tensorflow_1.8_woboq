<!doctype html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><title>partition_assignment.cc source code [tensorflow/tensorflow/compiler/xla/service/gpu/partition_assignment.cc] - Woboq Code Browser</title>
<link rel="stylesheet" href="https://code.woboq.org/data/qtcreator.css" title="QtCreator"/>
<link rel="alternate stylesheet" href="https://code.woboq.org/data/kdevelop.css" title="KDevelop"/>
<script type="text/javascript" src="https://code.woboq.org/data/jquery/jquery.min.js"></script>
<script type="text/javascript" src="https://code.woboq.org/data/jquery/jquery-ui.min.js"></script>
<script>var file = 'tensorflow/tensorflow/compiler/xla/service/gpu/partition_assignment.cc'; var root_path = '../../../../../..'; var data_path = 'https://code.woboq.org/data';</script>
<script src='https://code.woboq.org/data/codebrowser.js'></script>
</head>
<body><div id='header'><h1 id='breadcrumb'><span>Browse the source code of </span><a href='../../../../..'>tensorflow</a>/<a href='../../../..'>tensorflow</a>/<a href='../../..'>compiler</a>/<a href='../..'>xla</a>/<a href='..'>service</a>/<a href='./'>gpu</a>/<a href='partition_assignment.cc.html'>partition_assignment.cc</a></h1></div>
<hr/><div id='content'><table class="code">
<tr><th id="1">1</th><td><i>/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.</i></td></tr>
<tr><th id="2">2</th><td><i></i></td></tr>
<tr><th id="3">3</th><td><i>Licensed under the Apache License, Version 2.0 (the "License");</i></td></tr>
<tr><th id="4">4</th><td><i>you may not use this file except in compliance with the License.</i></td></tr>
<tr><th id="5">5</th><td><i>You may obtain a copy of the License at</i></td></tr>
<tr><th id="6">6</th><td><i></i></td></tr>
<tr><th id="7">7</th><td><i>    <a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></i></td></tr>
<tr><th id="8">8</th><td><i></i></td></tr>
<tr><th id="9">9</th><td><i>Unless required by applicable law or agreed to in writing, software</i></td></tr>
<tr><th id="10">10</th><td><i>distributed under the License is distributed on an "AS IS" BASIS,</i></td></tr>
<tr><th id="11">11</th><td><i>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</i></td></tr>
<tr><th id="12">12</th><td><i>See the License for the specific language governing permissions and</i></td></tr>
<tr><th id="13">13</th><td><i>limitations under the License.</i></td></tr>
<tr><th id="14">14</th><td><i>==============================================================================*/</i></td></tr>
<tr><th id="15">15</th><td></td></tr>
<tr><th id="16">16</th><td><u>#include <a href="partition_assignment.h.html">"tensorflow/compiler/xla/service/gpu/partition_assignment.h"</a></u></td></tr>
<tr><th id="17">17</th><td></td></tr>
<tr><th id="18">18</th><td><u>#include <a href="../../../../../../include/c++/5/ostream.html">&lt;ostream&gt;</a></u></td></tr>
<tr><th id="19">19</th><td><u>#include <a href="../../../../../../include/c++/5/string.html">&lt;string&gt;</a></u></td></tr>
<tr><th id="20">20</th><td></td></tr>
<tr><th id="21">21</th><td><u>#include <a href="../../map_util.h.html">"tensorflow/compiler/xla/map_util.h"</a></u></td></tr>
<tr><th id="22">22</th><td><u>#include <a href="../../ptr_util.h.html">"tensorflow/compiler/xla/ptr_util.h"</a></u></td></tr>
<tr><th id="23">23</th><td><u>#include <a href="../hlo_computation.h.html">"tensorflow/compiler/xla/service/hlo_computation.h"</a></u></td></tr>
<tr><th id="24">24</th><td><u>#include <a href="../hlo_opcode.h.html">"tensorflow/compiler/xla/service/hlo_opcode.h"</a></u></td></tr>
<tr><th id="25">25</th><td><u>#include <a href="../../shape_util.h.html">"tensorflow/compiler/xla/shape_util.h"</a></u></td></tr>
<tr><th id="26">26</th><td><u>#include <a href="../../types.h.html">"tensorflow/compiler/xla/types.h"</a></u></td></tr>
<tr><th id="27">27</th><td><u>#include <a href="../../util.h.html">"tensorflow/compiler/xla/util.h"</a></u></td></tr>
<tr><th id="28">28</th><td><u>#include <a href="../../../../core/lib/core/bits.h.html">"tensorflow/core/lib/core/bits.h"</a></u></td></tr>
<tr><th id="29">29</th><td><u>#include <a href="../../../../core/lib/strings/stringprintf.h.html">"tensorflow/core/lib/strings/stringprintf.h"</a></u></td></tr>
<tr><th id="30">30</th><td><u>#include <a href="../../../../core/platform/logging.h.html">"tensorflow/core/platform/logging.h"</a></u></td></tr>
<tr><th id="31">31</th><td></td></tr>
<tr><th id="32">32</th><td><b>namespace</b> <span class="namespace">se</span> = <span class="namespace">::perftools::</span><span class="namespace">gputools</span>;</td></tr>
<tr><th id="33">33</th><td></td></tr>
<tr><th id="34">34</th><td><b>namespace</b> <span class="namespace">xla</span> {</td></tr>
<tr><th id="35">35</th><td><b>namespace</b> <span class="namespace">gpu</span> {</td></tr>
<tr><th id="36">36</th><td></td></tr>
<tr><th id="37">37</th><td><span class="namespace">std::</span><a class="typedef" href="../../../../../../include/c++/5/iosfwd.html#std::ostream" title='std::ostream' data-type='basic_ostream&lt;char&gt;' data-ref="std::ostream">ostream</a>&amp; <dfn class="decl def" id="_ZN3xla3gpulsERSoRKNS0_16LaunchDimensionsE" title='xla::gpu::operator&lt;&lt;' data-ref="_ZN3xla3gpulsERSoRKNS0_16LaunchDimensionsE"><b>operator</b>&lt;&lt;</dfn>(<span class="namespace">std::</span><a class="typedef" href="../../../../../../include/c++/5/iosfwd.html#std::ostream" title='std::ostream' data-type='basic_ostream&lt;char&gt;' data-ref="std::ostream">ostream</a>&amp; <dfn class="local col1 decl" id="1out" title='out' data-type='std::ostream &amp;' data-ref="1out">out</dfn>,</td></tr>
<tr><th id="38">38</th><td>                         <em>const</em> <a class="type" href="partition_assignment.h.html#xla::gpu::LaunchDimensions" title='xla::gpu::LaunchDimensions' data-ref="xla::gpu::LaunchDimensions">LaunchDimensions</a>&amp; <dfn class="local col2 decl" id="2launch_dims" title='launch_dims' data-type='const xla::gpu::LaunchDimensions &amp;' data-ref="2launch_dims">launch_dims</dfn>) {</td></tr>
<tr><th id="39">39</th><td>  <a class="local col1 ref" href="#1out" title='out' data-ref="1out">out</a> <a class="ref" href="../../../../../../include/c++/5/bits/basic_string.h.html#_ZStlsRSt13basic_ostreamIT_T0_ERKSbIS0_S1_T1_E" title='std::operator&lt;&lt;' data-ref="_ZStlsRSt13basic_ostreamIT_T0_ERKSbIS0_S1_T1_E">&lt;&lt;</a> <span class="namespace">tensorflow::strings::</span><a class="ref" href="../../../../core/lib/strings/stringprintf.h.html#_ZN10tensorflow7strings6PrintfEPKcz" title='tensorflow::strings::Printf' data-ref="_ZN10tensorflow7strings6PrintfEPKcz">Printf</a>(<q>"[block: %lld, thread: %lld]"</q>,</td></tr>
<tr><th id="40">40</th><td>                                     <a class="local col2 ref" href="#2launch_dims" title='launch_dims' data-ref="2launch_dims">launch_dims</a>.<a class="ref" href="partition_assignment.h.html#_ZNK3xla3gpu16LaunchDimensions11block_countEv" title='xla::gpu::LaunchDimensions::block_count' data-ref="_ZNK3xla3gpu16LaunchDimensions11block_countEv">block_count</a>(),</td></tr>
<tr><th id="41">41</th><td>                                     <a class="local col2 ref" href="#2launch_dims" title='launch_dims' data-ref="2launch_dims">launch_dims</a>.<a class="ref" href="partition_assignment.h.html#_ZNK3xla3gpu16LaunchDimensions17threads_per_blockEv" title='xla::gpu::LaunchDimensions::threads_per_block' data-ref="_ZNK3xla3gpu16LaunchDimensions17threads_per_blockEv">threads_per_block</a>());</td></tr>
<tr><th id="42">42</th><td>  <b>return</b> <a class="local col1 ref" href="#1out" title='out' data-ref="1out">out</a>;</td></tr>
<tr><th id="43">43</th><td>}</td></tr>
<tr><th id="44">44</th><td></td></tr>
<tr><th id="45">45</th><td><i>// Calculates the launch dimensions used to invoke `hlo`.</i></td></tr>
<tr><th id="46">46</th><td><a class="type" href="partition_assignment.h.html#xla::gpu::LaunchDimensions" title='xla::gpu::LaunchDimensions' data-ref="xla::gpu::LaunchDimensions">LaunchDimensions</a> <dfn class="decl def" id="_ZN3xla3gpu25CalculateLaunchDimensionsERKNS_5ShapeERKN9perftools8gputools17DeviceDescriptionE" title='xla::gpu::CalculateLaunchDimensions' data-ref="_ZN3xla3gpu25CalculateLaunchDimensionsERKNS_5ShapeERKN9perftools8gputools17DeviceDescriptionE">CalculateLaunchDimensions</dfn>(</td></tr>
<tr><th id="47">47</th><td>    <em>const</em> <span class='type' title='xla::Shape' data-ref="xla::Shape">Shape</span>&amp; <dfn class="local col3 decl" id="3shape" title='shape' data-type='const xla::Shape &amp;' data-ref="3shape">shape</dfn>, <em>const</em> <span class="namespace">se::</span><a class="type" href="../../../../stream_executor/device_description.h.html#perftools::gputools::DeviceDescription" title='perftools::gputools::DeviceDescription' data-ref="perftools::gputools::DeviceDescription">DeviceDescription</a>&amp; <dfn class="local col4 decl" id="4device_desc" title='device_desc' data-type='const se::DeviceDescription &amp;' data-ref="4device_desc">device_desc</dfn>) {</td></tr>
<tr><th id="48">48</th><td>  <a class="typedef" href="../../../../core/platform/default/integral_types.h.html#tensorflow::int64" title='tensorflow::int64' data-type='long long' data-ref="tensorflow::int64">int64</a> <dfn class="local col5 decl" id="5num_elements" title='num_elements' data-type='int64' data-ref="5num_elements">num_elements</dfn> = <a class="type" href="../../shape_util.h.html#xla::ShapeUtil" title='xla::ShapeUtil' data-ref="xla::ShapeUtil">ShapeUtil</a>::<a class="ref" href="../../shape_util.h.html#_ZN3xla9ShapeUtil10ElementsInERKNS_5ShapeE" title='xla::ShapeUtil::ElementsIn' data-ref="_ZN3xla9ShapeUtil10ElementsInERKNS_5ShapeE">ElementsIn</a>(<a class="local col3 ref" href="#3shape" title='shape' data-ref="3shape">shape</a>);</td></tr>
<tr><th id="49">49</th><td>  <b>if</b> (<a class="local col5 ref" href="#5num_elements" title='num_elements' data-ref="5num_elements">num_elements</a> &lt;= <var>1</var>) {</td></tr>
<tr><th id="50">50</th><td>    <b>return</b> <a class="type" href="partition_assignment.h.html#xla::gpu::LaunchDimensions" title='xla::gpu::LaunchDimensions' data-ref="xla::gpu::LaunchDimensions">LaunchDimensions</a><a class="ref" href="partition_assignment.h.html#_ZN3xla3gpu16LaunchDimensionsC1Ev" title='xla::gpu::LaunchDimensions::LaunchDimensions' data-ref="_ZN3xla3gpu16LaunchDimensionsC1Ev">(</a>);</td></tr>
<tr><th id="51">51</th><td>  }</td></tr>
<tr><th id="52">52</th><td></td></tr>
<tr><th id="53">53</th><td>  <i>// Since we don't do any inter-warp communication, we're free to choose any</i></td></tr>
<tr><th id="54">54</th><td><i>  // block size we want, subject to hardware constraints.  We choose the</i></td></tr>
<tr><th id="55">55</th><td><i>  // smallest block size that allows the GPU to reach full occupancy (assuming</i></td></tr>
<tr><th id="56">56</th><td><i>  // the kernel uses sufficiently few registers).  This gives us max performance</i></td></tr>
<tr><th id="57">57</th><td><i>  // when the kernel uses few registers, and lets us scale down gracefully as</i></td></tr>
<tr><th id="58">58</th><td><i>  // the kernel uses more registers.</i></td></tr>
<tr><th id="59">59</th><td><i>  //</i></td></tr>
<tr><th id="60">60</th><td><i>  // Specifically, we choose the number of threads per block such that</i></td></tr>
<tr><th id="61">61</th><td><i>  //</i></td></tr>
<tr><th id="62">62</th><td><i>  //   &lt;num threads per block&gt; * &lt;max blocks per core&gt; = &lt;max threads per core&gt;</i></td></tr>
<tr><th id="63">63</th><td></td></tr>
<tr><th id="64">64</th><td>  <em>auto</em> <dfn class="local col6 decl" id="6threads_per_core" title='threads_per_core' data-type='unsigned long long' data-ref="6threads_per_core">threads_per_core</dfn> = <a class="local col4 ref" href="#4device_desc" title='device_desc' data-ref="4device_desc">device_desc</a>.<a class="ref" href="../../../../stream_executor/device_description.h.html#_ZNK9perftools8gputools17DeviceDescription22threads_per_core_limitEv" title='perftools::gputools::DeviceDescription::threads_per_core_limit' data-ref="_ZNK9perftools8gputools17DeviceDescription22threads_per_core_limitEv">threads_per_core_limit</a>();</td></tr>
<tr><th id="65">65</th><td>  <em>auto</em> <dfn class="local col7 decl" id="7blocks_per_core" title='blocks_per_core' data-type='unsigned long long' data-ref="7blocks_per_core">blocks_per_core</dfn> = <a class="local col4 ref" href="#4device_desc" title='device_desc' data-ref="4device_desc">device_desc</a>.<a class="ref" href="../../../../stream_executor/device_description.h.html#_ZNK9perftools8gputools17DeviceDescription21blocks_per_core_limitEv" title='perftools::gputools::DeviceDescription::blocks_per_core_limit' data-ref="_ZNK9perftools8gputools17DeviceDescription21blocks_per_core_limitEv">blocks_per_core_limit</a>();</td></tr>
<tr><th id="66">66</th><td>  <a class="typedef" href="../../../../core/platform/default/integral_types.h.html#tensorflow::int64" title='tensorflow::int64' data-type='long long' data-ref="tensorflow::int64">int64</a> <dfn class="local col8 decl" id="8threads_per_block" title='threads_per_block' data-type='int64' data-ref="8threads_per_block">threads_per_block</dfn>;</td></tr>
<tr><th id="67">67</th><td>  <b>if</b> (<a class="local col6 ref" href="#6threads_per_core" title='threads_per_core' data-ref="6threads_per_core">threads_per_core</a> != <var>0</var> &amp;&amp; <a class="local col7 ref" href="#7blocks_per_core" title='blocks_per_core' data-ref="7blocks_per_core">blocks_per_core</a> != <var>0</var>) {</td></tr>
<tr><th id="68">68</th><td>    <a class="local col8 ref" href="#8threads_per_block" title='threads_per_block' data-ref="8threads_per_block">threads_per_block</a> = <a class="local col4 ref" href="#4device_desc" title='device_desc' data-ref="4device_desc">device_desc</a>.<a class="ref" href="../../../../stream_executor/device_description.h.html#_ZNK9perftools8gputools17DeviceDescription22threads_per_core_limitEv" title='perftools::gputools::DeviceDescription::threads_per_core_limit' data-ref="_ZNK9perftools8gputools17DeviceDescription22threads_per_core_limitEv">threads_per_core_limit</a>() /</td></tr>
<tr><th id="69">69</th><td>                        <a class="local col4 ref" href="#4device_desc" title='device_desc' data-ref="4device_desc">device_desc</a>.<a class="ref" href="../../../../stream_executor/device_description.h.html#_ZNK9perftools8gputools17DeviceDescription21blocks_per_core_limitEv" title='perftools::gputools::DeviceDescription::blocks_per_core_limit' data-ref="_ZNK9perftools8gputools17DeviceDescription21blocks_per_core_limitEv">blocks_per_core_limit</a>();</td></tr>
<tr><th id="70">70</th><td>  } <b>else</b> {</td></tr>
<tr><th id="71">71</th><td>    <em>static</em> <span class="namespace">std::</span><a class="type" href="../../../../../../include/c++/5/bits/atomic_base.h.html#std::atomic" title='std::atomic' data-ref="std::atomic">atomic</a>&lt;<a class="typedef" href="../../../../core/platform/default/integral_types.h.html#tensorflow::int64" title='tensorflow::int64' data-type='long long' data-ref="tensorflow::int64">int64</a>&gt; <dfn class="local col9 decl" id="9log_count" title='log_count' data-type='std::atomic&lt;int64&gt;' data-ref="9log_count">log_count</dfn><a class="ref" href="../../../../../../include/c++/5/atomic.html#_ZNSt6atomicIxEC1Ex" title='std::atomic&lt;long long&gt;::atomic' data-ref="_ZNSt6atomicIxEC1Ex">{</a><var>0</var>};</td></tr>
<tr><th id="72">72</th><td>    <b>if</b> (<a class="local col9 ref" href="#9log_count" title='log_count' data-ref="9log_count">log_count</a>.<a class="ref" href="../../../../../../include/c++/5/bits/atomic_base.h.html#_ZNSt13__atomic_base9fetch_addET_St12memory_order" title='std::__atomic_base::fetch_add' data-ref="_ZNSt13__atomic_base9fetch_addET_St12memory_order">fetch_add</a>(<var>1</var>) &lt; <var>8</var>) {</td></tr>
<tr><th id="73">73</th><td>      <a class="macro" href="../../../../core/platform/default/logging.h.html#77" title="::tensorflow::internal::LogMessage(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/compiler/xla/service/gpu/partition_assignment.cc&quot;, 73, ::tensorflow::WARNING)" data-ref="_M/LOG">LOG</a>(WARNING) <a class="ref" href="../../../../../../include/c++/5/ostream.html#_ZStlsOSt13basic_ostreamIT_T0_ERKT1_" title='std::operator&lt;&lt;' data-ref="_ZStlsOSt13basic_ostreamIT_T0_ERKT1_">&lt;&lt;</a> <q>"Attempting to calculate launch dimensions for GPU "</q></td></tr>
<tr><th id="74">74</th><td>                      <q>"without full information about its capabilities.  "</q></td></tr>
<tr><th id="75">75</th><td>                      <q>"StreamExecutor's PopulateDeviceDescription should be "</q></td></tr>
<tr><th id="76">76</th><td>                      <q>"updated for this device."</q>;</td></tr>
<tr><th id="77">77</th><td>    }</td></tr>
<tr><th id="78">78</th><td>    <a class="local col8 ref" href="#8threads_per_block" title='threads_per_block' data-ref="8threads_per_block">threads_per_block</a> = <a class="local col4 ref" href="#4device_desc" title='device_desc' data-ref="4device_desc">device_desc</a>.<a class="ref" href="../../../../stream_executor/device_description.h.html#_ZNK9perftools8gputools17DeviceDescription16threads_per_warpEv" title='perftools::gputools::DeviceDescription::threads_per_warp' data-ref="_ZNK9perftools8gputools17DeviceDescription16threads_per_warpEv">threads_per_warp</a>();</td></tr>
<tr><th id="79">79</th><td>    <b>if</b> (<a class="local col8 ref" href="#8threads_per_block" title='threads_per_block' data-ref="8threads_per_block">threads_per_block</a> == <var>0</var>) {</td></tr>
<tr><th id="80">80</th><td>      <i>// Fall back to *something* if we can't even get num threads per warp.</i></td></tr>
<tr><th id="81">81</th><td>      <a class="local col8 ref" href="#8threads_per_block" title='threads_per_block' data-ref="8threads_per_block">threads_per_block</a> = <var>32</var>;</td></tr>
<tr><th id="82">82</th><td>    }</td></tr>
<tr><th id="83">83</th><td>  }</td></tr>
<tr><th id="84">84</th><td></td></tr>
<tr><th id="85">85</th><td>  <b>if</b> (<a class="local col5 ref" href="#5num_elements" title='num_elements' data-ref="5num_elements">num_elements</a> &lt; <a class="local col8 ref" href="#8threads_per_block" title='threads_per_block' data-ref="8threads_per_block">threads_per_block</a>) {</td></tr>
<tr><th id="86">86</th><td>    <a class="local col8 ref" href="#8threads_per_block" title='threads_per_block' data-ref="8threads_per_block">threads_per_block</a> = <a class="local col5 ref" href="#5num_elements" title='num_elements' data-ref="5num_elements">num_elements</a>;</td></tr>
<tr><th id="87">87</th><td>    <a class="macro" href="../../../../core/platform/default/logging.h.html#89" title="if ((__builtin_expect(((2) &lt;= ::tensorflow::internal::LogMessage::MinVLogLevel()), 0))) ::tensorflow::internal::LogMessage(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/compiler/xla/service/gpu/partition_assignment.cc&quot;, 87, tensorflow::INFO)" data-ref="_M/VLOG">VLOG</a>(<var>2</var>) <a class="ref" href="../../../../../../include/c++/5/ostream.html#_ZStlsOSt13basic_ostreamIT_T0_ERKT1_" title='std::operator&lt;&lt;' data-ref="_ZStlsOSt13basic_ostreamIT_T0_ERKT1_">&lt;&lt;</a> <q>"Update # of threads per block to the element count ("</q></td></tr>
<tr><th id="88">88</th><td>            <a class="ref" href="../../../../../../include/c++/5/ostream.html#_ZNSt13basic_ostreamlsEx" title='std::basic_ostream::operator&lt;&lt;' data-ref="_ZNSt13basic_ostreamlsEx">&lt;&lt;</a> <a class="local col8 ref" href="#8threads_per_block" title='threads_per_block' data-ref="8threads_per_block">threads_per_block</a> <a class="ref" href="../../../../../../include/c++/5/ostream.html#_ZStlsRSt13basic_ostreamIcT_EPKc" title='std::operator&lt;&lt;' data-ref="_ZStlsRSt13basic_ostreamIcT_EPKc">&lt;&lt;</a> <q>") because the latter is smaller."</q>;</td></tr>
<tr><th id="89">89</th><td>  }</td></tr>
<tr><th id="90">90</th><td></td></tr>
<tr><th id="91">91</th><td>  <a class="typedef" href="../../../../core/platform/default/integral_types.h.html#tensorflow::int64" title='tensorflow::int64' data-type='long long' data-ref="tensorflow::int64">int64</a> <dfn class="local col0 decl" id="10block_count" title='block_count' data-type='int64' data-ref="10block_count">block_count</dfn> = <a class="ref" href="../../util.h.html#_ZN3xla11CeilOfRatioET_S0_" title='xla::CeilOfRatio' data-ref="_ZN3xla11CeilOfRatioET_S0_">CeilOfRatio</a>(<a class="local col5 ref" href="#5num_elements" title='num_elements' data-ref="5num_elements">num_elements</a>, <a class="local col8 ref" href="#8threads_per_block" title='threads_per_block' data-ref="8threads_per_block">threads_per_block</a>);</td></tr>
<tr><th id="92">92</th><td>  <a class="macro" href="../../../../core/platform/default/logging.h.html#89" title="if ((__builtin_expect(((2) &lt;= ::tensorflow::internal::LogMessage::MinVLogLevel()), 0))) ::tensorflow::internal::LogMessage(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/compiler/xla/service/gpu/partition_assignment.cc&quot;, 92, tensorflow::INFO)" data-ref="_M/VLOG">VLOG</a>(<var>2</var>) <a class="ref" href="../../../../../../include/c++/5/ostream.html#_ZStlsOSt13basic_ostreamIT_T0_ERKT1_" title='std::operator&lt;&lt;' data-ref="_ZStlsOSt13basic_ostreamIT_T0_ERKT1_">&lt;&lt;</a> <span class="namespace">tensorflow::strings::</span><a class="ref" href="../../../../core/lib/strings/stringprintf.h.html#_ZN10tensorflow7strings6PrintfEPKcz" title='tensorflow::strings::Printf' data-ref="_ZN10tensorflow7strings6PrintfEPKcz">Printf</a>(</td></tr>
<tr><th id="93">93</th><td>      <q>"Initialized the block count to ceil(# of elements / threads per "</q></td></tr>
<tr><th id="94">94</th><td>      <q>"block) = ceil(%lld/%lld) = %lld"</q>,</td></tr>
<tr><th id="95">95</th><td>      <a class="local col5 ref" href="#5num_elements" title='num_elements' data-ref="5num_elements">num_elements</a>, <a class="local col8 ref" href="#8threads_per_block" title='threads_per_block' data-ref="8threads_per_block">threads_per_block</a>, <a class="local col0 ref" href="#10block_count" title='block_count' data-ref="10block_count">block_count</a>);</td></tr>
<tr><th id="96">96</th><td></td></tr>
<tr><th id="97">97</th><td>  <b>return</b> <a class="type" href="partition_assignment.h.html#xla::gpu::LaunchDimensions" title='xla::gpu::LaunchDimensions' data-ref="xla::gpu::LaunchDimensions">LaunchDimensions</a><a class="ref" href="partition_assignment.h.html#_ZN3xla3gpu16LaunchDimensionsC1Exx" title='xla::gpu::LaunchDimensions::LaunchDimensions' data-ref="_ZN3xla3gpu16LaunchDimensionsC1Exx">(</a><a class="local col0 ref" href="#10block_count" title='block_count' data-ref="10block_count">block_count</a>, <a class="local col8 ref" href="#8threads_per_block" title='threads_per_block' data-ref="8threads_per_block">threads_per_block</a>);</td></tr>
<tr><th id="98">98</th><td>}</td></tr>
<tr><th id="99">99</th><td></td></tr>
<tr><th id="100">100</th><td>}  <i>// namespace gpu</i></td></tr>
<tr><th id="101">101</th><td>}  <i>// namespace xla</i></td></tr>
<tr><th id="102">102</th><td></td></tr>
</table><hr/><p id='footer'>
Generated on <em>2018-Aug-16</em> from project tensorflow revision <em>v1.8</em><br />Powered by <a href='https://woboq.com'><img alt='Woboq' src='https://code.woboq.org/woboq-16.png' width='41' height='16' /></a> <a href='https://code.woboq.org'>Code Browser</a> 2.1
<br/>Generator usage only permitted with license.</p>
</div></body></html>
