<!doctype html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><title>gpu_device.cc source code [tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc] - Woboq Code Browser</title>
<link rel="stylesheet" href="https://code.woboq.org/data/qtcreator.css" title="QtCreator"/>
<link rel="alternate stylesheet" href="https://code.woboq.org/data/kdevelop.css" title="KDevelop"/>
<script type="text/javascript" src="https://code.woboq.org/data/jquery/jquery.min.js"></script>
<script type="text/javascript" src="https://code.woboq.org/data/jquery/jquery-ui.min.js"></script>
<script>var file = 'tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc'; var root_path = '../../../../..'; var data_path = 'https://code.woboq.org/data';</script>
<script src='https://code.woboq.org/data/codebrowser.js'></script>
</head>
<body><div id='header'><h1 id='breadcrumb'><span>Browse the source code of </span><a href='../../../..'>tensorflow</a>/<a href='../../..'>tensorflow</a>/<a href='../..'>core</a>/<a href='..'>common_runtime</a>/<a href='./'>gpu</a>/<a href='gpu_device.cc.html'>gpu_device.cc</a></h1></div>
<hr/><div id='content'><table class="code">
<tr><th id="1">1</th><td><i>/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.</i></td></tr>
<tr><th id="2">2</th><td><i></i></td></tr>
<tr><th id="3">3</th><td><i>Licensed under the Apache License, Version 2.0 (the "License");</i></td></tr>
<tr><th id="4">4</th><td><i>you may not use this file except in compliance with the License.</i></td></tr>
<tr><th id="5">5</th><td><i>You may obtain a copy of the License at</i></td></tr>
<tr><th id="6">6</th><td><i></i></td></tr>
<tr><th id="7">7</th><td><i>    <a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></i></td></tr>
<tr><th id="8">8</th><td><i></i></td></tr>
<tr><th id="9">9</th><td><i>Unless required by applicable law or agreed to in writing, software</i></td></tr>
<tr><th id="10">10</th><td><i>distributed under the License is distributed on an "AS IS" BASIS,</i></td></tr>
<tr><th id="11">11</th><td><i>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</i></td></tr>
<tr><th id="12">12</th><td><i>See the License for the specific language governing permissions and</i></td></tr>
<tr><th id="13">13</th><td><i>limitations under the License.</i></td></tr>
<tr><th id="14">14</th><td><i>==============================================================================*/</i></td></tr>
<tr><th id="15">15</th><td><i></i></td></tr>
<tr><th id="16">16</th><td><i>// TODO(opensource): Use a more generic sounding preprocessor name than</i></td></tr>
<tr><th id="17">17</th><td><i>// GOOGLE_CUDA</i></td></tr>
<tr><th id="18">18</th><td><u>#<span data-ppcond="18">if</span> GOOGLE_CUDA</u></td></tr>
<tr><th id="19">19</th><td></td></tr>
<tr><th id="20">20</th><td><u>#define EIGEN_USE_GPU</u></td></tr>
<tr><th id="21">21</th><td></td></tr>
<tr><th id="22">22</th><td><u>#include "tensorflow/core/common_runtime/gpu/gpu_device.h"</u></td></tr>
<tr><th id="23">23</th><td></td></tr>
<tr><th id="24">24</th><td><u>#include &lt;stdlib.h&gt;</u></td></tr>
<tr><th id="25">25</th><td><u>#include &lt;string.h&gt;</u></td></tr>
<tr><th id="26">26</th><td><u>#include &lt;algorithm&gt;</u></td></tr>
<tr><th id="27">27</th><td><u>#include &lt;list&gt;</u></td></tr>
<tr><th id="28">28</th><td><u>#include &lt;map&gt;</u></td></tr>
<tr><th id="29">29</th><td><u>#include &lt;tuple&gt;</u></td></tr>
<tr><th id="30">30</th><td><u>#include &lt;vector&gt;</u></td></tr>
<tr><th id="31">31</th><td></td></tr>
<tr><th id="32">32</th><td><u>#include "third_party/eigen3/unsupported/Eigen/CXX11/Tensor"</u></td></tr>
<tr><th id="33">33</th><td><u>#include "tensorflow/core/common_runtime/device_factory.h"</u></td></tr>
<tr><th id="34">34</th><td><u>#include "tensorflow/core/common_runtime/gpu/gpu_event_mgr.h"</u></td></tr>
<tr><th id="35">35</th><td><u>#include "tensorflow/core/common_runtime/gpu/gpu_id.h"</u></td></tr>
<tr><th id="36">36</th><td><u>#include "tensorflow/core/common_runtime/gpu/gpu_id_manager.h"</u></td></tr>
<tr><th id="37">37</th><td><u>#include "tensorflow/core/common_runtime/gpu/gpu_id_utils.h"</u></td></tr>
<tr><th id="38">38</th><td><u>#include "tensorflow/core/common_runtime/gpu/gpu_init.h"</u></td></tr>
<tr><th id="39">39</th><td><u>#include "tensorflow/core/common_runtime/gpu/gpu_stream_util.h"</u></td></tr>
<tr><th id="40">40</th><td><u>#include "tensorflow/core/common_runtime/gpu/gpu_util.h"</u></td></tr>
<tr><th id="41">41</th><td><u>#include "tensorflow/core/common_runtime/gpu/process_state.h"</u></td></tr>
<tr><th id="42">42</th><td><u>#include "tensorflow/core/common_runtime/gpu_device_context.h"</u></td></tr>
<tr><th id="43">43</th><td><u>#include "tensorflow/core/common_runtime/local_device.h"</u></td></tr>
<tr><th id="44">44</th><td><u>#include "tensorflow/core/framework/allocator.h"</u></td></tr>
<tr><th id="45">45</th><td><u>#include "tensorflow/core/framework/device_base.h"</u></td></tr>
<tr><th id="46">46</th><td><u>#include "tensorflow/core/framework/op_kernel.h"</u></td></tr>
<tr><th id="47">47</th><td><u>#include "tensorflow/core/framework/tensor.h"</u></td></tr>
<tr><th id="48">48</th><td><u>#include "tensorflow/core/framework/tensor.pb.h"</u></td></tr>
<tr><th id="49">49</th><td><u>#include "tensorflow/core/framework/types.h"</u></td></tr>
<tr><th id="50">50</th><td><u>#include "tensorflow/core/framework/variant_op_registry.h"</u></td></tr>
<tr><th id="51">51</th><td><u>#include "tensorflow/core/graph/types.h"</u></td></tr>
<tr><th id="52">52</th><td><u>#include "tensorflow/core/lib/core/errors.h"</u></td></tr>
<tr><th id="53">53</th><td><u>#include "tensorflow/core/lib/core/status.h"</u></td></tr>
<tr><th id="54">54</th><td><u>#include "tensorflow/core/lib/gtl/stl_util.h"</u></td></tr>
<tr><th id="55">55</th><td><u>#include "tensorflow/core/lib/strings/numbers.h"</u></td></tr>
<tr><th id="56">56</th><td><u>#include "tensorflow/core/lib/strings/str_util.h"</u></td></tr>
<tr><th id="57">57</th><td><u>#include "tensorflow/core/lib/strings/strcat.h"</u></td></tr>
<tr><th id="58">58</th><td><u>#include "tensorflow/core/platform/cuda.h"</u></td></tr>
<tr><th id="59">59</th><td><u>#include "tensorflow/core/platform/logging.h"</u></td></tr>
<tr><th id="60">60</th><td><u>#include "tensorflow/core/platform/macros.h"</u></td></tr>
<tr><th id="61">61</th><td><u>#include "tensorflow/core/platform/stream_executor.h"</u></td></tr>
<tr><th id="62">62</th><td><u>#include "tensorflow/core/platform/tracing.h"</u></td></tr>
<tr><th id="63">63</th><td><u>#include "tensorflow/core/platform/types.h"</u></td></tr>
<tr><th id="64">64</th><td><u>#include "tensorflow/core/public/session_options.h"</u></td></tr>
<tr><th id="65">65</th><td><u>#include "tensorflow/core/util/device_name_utils.h"</u></td></tr>
<tr><th id="66">66</th><td><u>#include "tensorflow/core/util/env_var.h"</u></td></tr>
<tr><th id="67">67</th><td><u>#include "tensorflow/core/util/stream_executor_util.h"</u></td></tr>
<tr><th id="68">68</th><td></td></tr>
<tr><th id="69">69</th><td><u>#if !defined(PLATFORM_GOOGLE)</u></td></tr>
<tr><th id="70">70</th><td><u>#include "cuda/cuda_config.h"</u></td></tr>
<tr><th id="71">71</th><td><u>#endif</u></td></tr>
<tr><th id="72">72</th><td></td></tr>
<tr><th id="73">73</th><td><b>namespace</b> tensorflow {</td></tr>
<tr><th id="74">74</th><td></td></tr>
<tr><th id="75">75</th><td><i>// Eigen Ops directly allocate memory only for temporary buffers used</i></td></tr>
<tr><th id="76">76</th><td><i>// during OpKernel::Compute().  The recommended way of allocating such</i></td></tr>
<tr><th id="77">77</th><td><i>// memory is via OpKernelContext::allocate_temp().  However, Eigen Ops</i></td></tr>
<tr><th id="78">78</th><td><i>// don't have access to OpKernelContext, instead they get access to</i></td></tr>
<tr><th id="79">79</th><td><i>// memory directly through the device allocator.  As an Open Source</i></td></tr>
<tr><th id="80">80</th><td><i>// project, Eigen assumes allocator semantics similar to those of the</i></td></tr>
<tr><th id="81">81</th><td><i>// CUDA memory allocator, and may not work correctly due to race</i></td></tr>
<tr><th id="82">82</th><td><i>// conditions if used with some other allocator.  For safety, we need</i></td></tr>
<tr><th id="83">83</th><td><i>// to delay deallocation calls out of Eigen until all events on the</i></td></tr>
<tr><th id="84">84</th><td><i>// corresponding stream have completed.  The following two classes</i></td></tr>
<tr><th id="85">85</th><td><i>// serve this purpose in two different compilation environments.</i></td></tr>
<tr><th id="86">86</th><td></td></tr>
<tr><th id="87">87</th><td><b>class</b> EigenCudaStreamDevice : <b>public</b> ::Eigen::StreamInterface {</td></tr>
<tr><th id="88">88</th><td> <b>public</b>:</td></tr>
<tr><th id="89">89</th><td>  EigenCudaStreamDevice()</td></tr>
<tr><th id="90">90</th><td>      : scratch_(<b>nullptr</b>), semaphore_(<b>nullptr</b>), context_(<b>nullptr</b>) {</td></tr>
<tr><th id="91">91</th><td>    Eigen::initializeDeviceProp();</td></tr>
<tr><th id="92">92</th><td>  }</td></tr>
<tr><th id="93">93</th><td>  ~EigenCudaStreamDevice() override {}</td></tr>
<tr><th id="94">94</th><td>  <em>void</em> Reinitialize(OpKernelContext* context, <em>const</em> cudaStream_t* cuda_stream,</td></tr>
<tr><th id="95">95</th><td>                    TfGpuId tf_gpu_id, ::tensorflow::Allocator* alloc,</td></tr>
<tr><th id="96">96</th><td>                    <em>char</em>* scratch) {</td></tr>
<tr><th id="97">97</th><td>    <b>if</b> (LogMemory::IsEnabled()) {</td></tr>
<tr><th id="98">98</th><td>      operation_ = context-&gt;op_kernel().name() + <q>"/EigenAllocator"</q>;</td></tr>
<tr><th id="99">99</th><td>      step_id_ = context-&gt;step_id();</td></tr>
<tr><th id="100">100</th><td>    }</td></tr>
<tr><th id="101">101</th><td>    context_ = context;</td></tr>
<tr><th id="102">102</th><td>    scratch_ = scratch;</td></tr>
<tr><th id="103">103</th><td>    semaphore_ =</td></tr>
<tr><th id="104">104</th><td>        <b>reinterpret_cast</b>&lt;<em>unsigned</em> <em>int</em>*&gt;(scratch + Eigen::kCudaScratchSize);</td></tr>
<tr><th id="105">105</th><td>    stream_ = cuda_stream;</td></tr>
<tr><th id="106">106</th><td>    allocator_ = alloc;</td></tr>
<tr><th id="107">107</th><td>    <em>const</em> <em>int</em> cuda_gpu_id = GpuIdManager::TfToCudaGpuId(tf_gpu_id).value();</td></tr>
<tr><th id="108">108</th><td>    device_prop_ = &amp;Eigen::m_deviceProperties[cuda_gpu_id];</td></tr>
<tr><th id="109">109</th><td>  }</td></tr>
<tr><th id="110">110</th><td></td></tr>
<tr><th id="111">111</th><td>  <em>const</em> cudaStream_t&amp; stream() <em>const</em> override { <b>return</b> *stream_; }</td></tr>
<tr><th id="112">112</th><td>  <em>const</em> cudaDeviceProp&amp; deviceProperties() <em>const</em> override {</td></tr>
<tr><th id="113">113</th><td>    <b>return</b> *device_prop_;</td></tr>
<tr><th id="114">114</th><td>  }</td></tr>
<tr><th id="115">115</th><td></td></tr>
<tr><th id="116">116</th><td>  <em>void</em>* allocate(size_t num_bytes) <em>const</em> override {</td></tr>
<tr><th id="117">117</th><td>    <em>void</em>* ret = allocator_-&gt;AllocateRaw(<var>32</var> <i>/* alignment */</i>, num_bytes);</td></tr>
<tr><th id="118">118</th><td>    <b>if</b> (ret == <b>nullptr</b>) {</td></tr>
<tr><th id="119">119</th><td>      <b>if</b> (context_) {</td></tr>
<tr><th id="120">120</th><td>        context_-&gt;SetStatus(errors::ResourceExhausted(</td></tr>
<tr><th id="121">121</th><td>            strings::StrCat(<q>"Ran out of GPU memory when allocating "</q>, num_bytes,</td></tr>
<tr><th id="122">122</th><td>                            <q>" bytes for "</q>, operation_)));</td></tr>
<tr><th id="123">123</th><td>      } <b>else</b> {</td></tr>
<tr><th id="124">124</th><td>        LOG(FATAL)</td></tr>
<tr><th id="125">125</th><td>            &lt;&lt; <q>"EigenAllocator for GPU ran out of memory when allocating "</q></td></tr>
<tr><th id="126">126</th><td>            &lt;&lt; num_bytes &lt;&lt; <q>". See error logs for more detailed info."</q>;</td></tr>
<tr><th id="127">127</th><td>      }</td></tr>
<tr><th id="128">128</th><td>    }</td></tr>
<tr><th id="129">129</th><td>    <b>if</b> (LogMemory::IsEnabled() &amp;&amp; ret != <b>nullptr</b>) {</td></tr>
<tr><th id="130">130</th><td>      LogMemory::RecordRawAllocation(operation_, step_id_, num_bytes, ret,</td></tr>
<tr><th id="131">131</th><td>                                     allocator_);</td></tr>
<tr><th id="132">132</th><td>    }</td></tr>
<tr><th id="133">133</th><td>    <b>return</b> ret;</td></tr>
<tr><th id="134">134</th><td>  }</td></tr>
<tr><th id="135">135</th><td>  <em>void</em> deallocate(<em>void</em>* buffer) <em>const</em> override {</td></tr>
<tr><th id="136">136</th><td>    <b>if</b> (LogMemory::IsEnabled() &amp;&amp; buffer != <b>nullptr</b>) {</td></tr>
<tr><th id="137">137</th><td>      LogMemory::RecordRawDeallocation(operation_, step_id_, buffer, allocator_,</td></tr>
<tr><th id="138">138</th><td>                                       <b>true</b>);</td></tr>
<tr><th id="139">139</th><td>    }</td></tr>
<tr><th id="140">140</th><td>    AsyncFreeData* afData =</td></tr>
<tr><th id="141">141</th><td>        <b>new</b> AsyncFreeData(allocator_, buffer, operation_, step_id_);</td></tr>
<tr><th id="142">142</th><td>    cudaError_t err = cudaStreamAddCallback(*stream_, asyncFree, afData, <var>0</var>);</td></tr>
<tr><th id="143">143</th><td>    CHECK_EQ(err, cudaSuccess);</td></tr>
<tr><th id="144">144</th><td>  }</td></tr>
<tr><th id="145">145</th><td></td></tr>
<tr><th id="146">146</th><td>  <i>// Return a pointer to a per stream scratchpad of 1024 bytes residing</i></td></tr>
<tr><th id="147">147</th><td><i>  // in global memory.</i></td></tr>
<tr><th id="148">148</th><td>  <em>void</em>* scratchpad() <em>const</em> override { <b>return</b> scratch_; }</td></tr>
<tr><th id="149">149</th><td></td></tr>
<tr><th id="150">150</th><td>  <i>// Return a semaphore. The semaphore is initially initialized to 0, and</i></td></tr>
<tr><th id="151">151</th><td><i>  // each kernel using it is responsible for resetting to 0 upon completion</i></td></tr>
<tr><th id="152">152</th><td><i>  // to maintain the invariant that the semaphore is always equal to 0 upon</i></td></tr>
<tr><th id="153">153</th><td><i>  // each kernel start.</i></td></tr>
<tr><th id="154">154</th><td>  <em>unsigned</em> <em>int</em>* semaphore() <em>const</em> override { <b>return</b> semaphore_; }</td></tr>
<tr><th id="155">155</th><td></td></tr>
<tr><th id="156">156</th><td> <b>private</b>:</td></tr>
<tr><th id="157">157</th><td>  <b>struct</b> AsyncFreeData {</td></tr>
<tr><th id="158">158</th><td>    AsyncFreeData(::tensorflow::Allocator* a, <em>void</em>* p, <em>const</em> string&amp; o,</td></tr>
<tr><th id="159">159</th><td>                  <em>const</em> int64 s)</td></tr>
<tr><th id="160">160</th><td>        : allocator_(a), address_(p), operation_(o), step_id_(s) {}</td></tr>
<tr><th id="161">161</th><td>    ::tensorflow::Allocator* allocator_;</td></tr>
<tr><th id="162">162</th><td>    <em>void</em>* address_;</td></tr>
<tr><th id="163">163</th><td>    <em>const</em> string operation_;</td></tr>
<tr><th id="164">164</th><td>    <em>const</em> int64 step_id_;</td></tr>
<tr><th id="165">165</th><td>  };</td></tr>
<tr><th id="166">166</th><td></td></tr>
<tr><th id="167">167</th><td>  <em>static</em> <em>void</em> CUDART_CB asyncFree(cudaStream_t stream, cudaError_t status,</td></tr>
<tr><th id="168">168</th><td>                                  <em>void</em>* userData) {</td></tr>
<tr><th id="169">169</th><td>    AsyncFreeData* data = <b>static_cast</b>&lt;AsyncFreeData*&gt;(userData);</td></tr>
<tr><th id="170">170</th><td>    <b>if</b> (LogMemory::IsEnabled()) {</td></tr>
<tr><th id="171">171</th><td>      LogMemory::RecordRawDeallocation(data-&gt;operation_, data-&gt;step_id_,</td></tr>
<tr><th id="172">172</th><td>                                       data-&gt;address_, data-&gt;allocator_, <b>false</b>);</td></tr>
<tr><th id="173">173</th><td>    }</td></tr>
<tr><th id="174">174</th><td>    data-&gt;allocator_-&gt;DeallocateRaw(data-&gt;address_);</td></tr>
<tr><th id="175">175</th><td>    <b>delete</b> data;</td></tr>
<tr><th id="176">176</th><td>  }</td></tr>
<tr><th id="177">177</th><td></td></tr>
<tr><th id="178">178</th><td>  string operation_;</td></tr>
<tr><th id="179">179</th><td>  int64 step_id_;</td></tr>
<tr><th id="180">180</th><td>  <em>const</em> cudaStream_t* stream_;          <i>// Not owned.</i></td></tr>
<tr><th id="181">181</th><td>  <em>const</em> cudaDeviceProp* device_prop_;   <i>// Not owned.</i></td></tr>
<tr><th id="182">182</th><td>  ::tensorflow::Allocator* allocator_;  <i>// Not owned.</i></td></tr>
<tr><th id="183">183</th><td>  <em>mutable</em> <em>char</em>* scratch_;</td></tr>
<tr><th id="184">184</th><td>  <em>mutable</em> <em>unsigned</em> <em>int</em>* semaphore_;</td></tr>
<tr><th id="185">185</th><td>  OpKernelContext* context_;</td></tr>
<tr><th id="186">186</th><td></td></tr>
<tr><th id="187">187</th><td>  TF_DISALLOW_COPY_AND_ASSIGN(EigenCudaStreamDevice);</td></tr>
<tr><th id="188">188</th><td>};</td></tr>
<tr><th id="189">189</th><td></td></tr>
<tr><th id="190">190</th><td><i>// This factory helps to ensure that different GPU device objects that refer to</i></td></tr>
<tr><th id="191">191</th><td><i>// the same physical device and stream group id use the same stream group</i></td></tr>
<tr><th id="192">192</th><td><i>// object (and therefore the same CUDA streams). This is necessary since there</i></td></tr>
<tr><th id="193">193</th><td><i>// is a single memory allocator per device (see ProcessState::GetGPUAllocator)</i></td></tr>
<tr><th id="194">194</th><td><i>// and allocators must not be shared across streams.</i></td></tr>
<tr><th id="195">195</th><td><b>class</b> BaseGPUDevice::StreamGroupFactory {</td></tr>
<tr><th id="196">196</th><td> <b>public</b>:</td></tr>
<tr><th id="197">197</th><td>  <i>// Returns the unique stream group for use with the stream defined by</i></td></tr>
<tr><th id="198">198</th><td><i>  // {tf_gpu_id, stream_group_within_gpu}, creating it if it does not yet</i></td></tr>
<tr><th id="199">199</th><td><i>  // exist.</i></td></tr>
<tr><th id="200">200</th><td><i>  // This function is thread safe.</i></td></tr>
<tr><th id="201">201</th><td>  BaseGPUDevice::StreamGroup* GetOrCreate(TfGpuId tf_gpu_id,</td></tr>
<tr><th id="202">202</th><td>                                          <em>int</em> stream_group_within_gpu,</td></tr>
<tr><th id="203">203</th><td>                                          gpu::StreamExecutor* executor) {</td></tr>
<tr><th id="204">204</th><td>    mutex_lock guard(lock_);</td></tr>
<tr><th id="205">205</th><td>    StreamGroup* group =</td></tr>
<tr><th id="206">206</th><td>        &amp;streams_[key_type(tf_gpu_id.value(), stream_group_within_gpu)];</td></tr>
<tr><th id="207">207</th><td>    <b>if</b> (!group-&gt;compute) {</td></tr>
<tr><th id="208">208</th><td>      group-&gt;compute = <b>new</b> gpu::Stream(executor);</td></tr>
<tr><th id="209">209</th><td>      group-&gt;compute-&gt;Init();</td></tr>
<tr><th id="210">210</th><td>      VLOG(<var>2</var>) &lt;&lt; <q>"Created stream["</q> &lt;&lt; stream_group_within_gpu</td></tr>
<tr><th id="211">211</th><td>              &lt;&lt; <q>"] = "</q> &lt;&lt; group-&gt;compute;</td></tr>
<tr><th id="212">212</th><td></td></tr>
<tr><th id="213">213</th><td>      group-&gt;host_to_device = <b>new</b> gpu::Stream(executor);</td></tr>
<tr><th id="214">214</th><td>      group-&gt;host_to_device-&gt;Init();</td></tr>
<tr><th id="215">215</th><td>      VLOG(<var>2</var>) &lt;&lt; <q>"Created host_to_device_stream["</q> &lt;&lt; stream_group_within_gpu</td></tr>
<tr><th id="216">216</th><td>              &lt;&lt; <q>"] = "</q> &lt;&lt; group-&gt;host_to_device;</td></tr>
<tr><th id="217">217</th><td></td></tr>
<tr><th id="218">218</th><td>      group-&gt;device_to_host = <b>new</b> gpu::Stream(executor);</td></tr>
<tr><th id="219">219</th><td>      group-&gt;device_to_host-&gt;Init();</td></tr>
<tr><th id="220">220</th><td>      VLOG(<var>2</var>) &lt;&lt; <q>"Created device_to_host_stream["</q> &lt;&lt; stream_group_within_gpu</td></tr>
<tr><th id="221">221</th><td>              &lt;&lt; <q>"] = "</q> &lt;&lt; group-&gt;device_to_host;</td></tr>
<tr><th id="222">222</th><td></td></tr>
<tr><th id="223">223</th><td>      group-&gt;device_to_device = <b>new</b> gpu::Stream(executor);</td></tr>
<tr><th id="224">224</th><td>      group-&gt;device_to_device-&gt;Init();</td></tr>
<tr><th id="225">225</th><td>      VLOG(<var>2</var>) &lt;&lt; <q>"Created device_to_device_stream["</q> &lt;&lt; stream_group_within_gpu</td></tr>
<tr><th id="226">226</th><td>              &lt;&lt; <q>"] = "</q> &lt;&lt; group-&gt;device_to_host;</td></tr>
<tr><th id="227">227</th><td>    }</td></tr>
<tr><th id="228">228</th><td>    <b>return</b> group;</td></tr>
<tr><th id="229">229</th><td>  }</td></tr>
<tr><th id="230">230</th><td></td></tr>
<tr><th id="231">231</th><td>  <i>// Returns a reference to the StreamGroupFactory singleton. Note that this is</i></td></tr>
<tr><th id="232">232</th><td><i>  // never destroyed, so the objects it owns are never deleted.</i></td></tr>
<tr><th id="233">233</th><td>  <em>static</em> StreamGroupFactory&amp; Global() {</td></tr>
<tr><th id="234">234</th><td>    <em>static</em> StreamGroupFactory* instance = <b>new</b> StreamGroupFactory();</td></tr>
<tr><th id="235">235</th><td>    <b>return</b> *instance;</td></tr>
<tr><th id="236">236</th><td>  }</td></tr>
<tr><th id="237">237</th><td></td></tr>
<tr><th id="238">238</th><td> <b>private</b>:</td></tr>
<tr><th id="239">239</th><td>  mutex lock_;</td></tr>
<tr><th id="240">240</th><td>  <b>using</b> key_type = std::tuple&lt;<em>int</em>, <em>int</em>&gt;;</td></tr>
<tr><th id="241">241</th><td>  std::map&lt;key_type, StreamGroup&gt; streams_;</td></tr>
<tr><th id="242">242</th><td></td></tr>
<tr><th id="243">243</th><td>  <i>// StreamGroupFactory cannot be created directly; Call</i></td></tr>
<tr><th id="244">244</th><td><i>  // StreamGroupFactory::Global() to get the global instance.</i></td></tr>
<tr><th id="245">245</th><td>  StreamGroupFactory() = <b>default</b>;</td></tr>
<tr><th id="246">246</th><td>  TF_DISALLOW_COPY_AND_ASSIGN(StreamGroupFactory);</td></tr>
<tr><th id="247">247</th><td>};</td></tr>
<tr><th id="248">248</th><td></td></tr>
<tr><th id="249">249</th><td>BaseGPUDevice::BaseGPUDevice(<em>const</em> SessionOptions&amp; options, <em>const</em> string&amp; name,</td></tr>
<tr><th id="250">250</th><td>                             Bytes memory_limit, <em>const</em> DeviceLocality&amp; locality,</td></tr>
<tr><th id="251">251</th><td>                             TfGpuId tf_gpu_id,</td></tr>
<tr><th id="252">252</th><td>                             <em>const</em> string&amp; physical_device_desc,</td></tr>
<tr><th id="253">253</th><td>                             Allocator* gpu_allocator, Allocator* cpu_allocator,</td></tr>
<tr><th id="254">254</th><td>                             <em>bool</em> sync_every_op, int32 max_streams)</td></tr>
<tr><th id="255">255</th><td>    : LocalDevice(options, Device::BuildDeviceAttributes(name, DEVICE_GPU,</td></tr>
<tr><th id="256">256</th><td>                                                         memory_limit, locality,</td></tr>
<tr><th id="257">257</th><td>                                                         physical_device_desc)),</td></tr>
<tr><th id="258">258</th><td>      gpu_allocator_(gpu_allocator),</td></tr>
<tr><th id="259">259</th><td>      cpu_allocator_(cpu_allocator),</td></tr>
<tr><th id="260">260</th><td>      scoped_allocator_mgr_(<b>new</b> ScopedAllocatorMgr(name)),</td></tr>
<tr><th id="261">261</th><td>      tf_gpu_id_(tf_gpu_id),</td></tr>
<tr><th id="262">262</th><td>      sync_every_op_(sync_every_op),</td></tr>
<tr><th id="263">263</th><td>      max_streams_(max_streams) {</td></tr>
<tr><th id="264">264</th><td>  ProcessState::singleton()-&gt;EnableGPUDevice();</td></tr>
<tr><th id="265">265</th><td>}</td></tr>
<tr><th id="266">266</th><td></td></tr>
<tr><th id="267">267</th><td>BaseGPUDevice::~BaseGPUDevice() {</td></tr>
<tr><th id="268">268</th><td>  <b>delete</b> gpu_device_info_;</td></tr>
<tr><th id="269">269</th><td>  <b>for</b> (<em>auto</em> ctx : device_contexts_) ctx-&gt;Unref();</td></tr>
<tr><th id="270">270</th><td>}</td></tr>
<tr><th id="271">271</th><td></td></tr>
<tr><th id="272">272</th><td>Status BaseGPUDevice::Init(<em>const</em> SessionOptions&amp; options) {</td></tr>
<tr><th id="273">273</th><td>  <em>auto</em> executor_status = GpuIdUtil::ExecutorForTfGpuId(tf_gpu_id_);</td></tr>
<tr><th id="274">274</th><td>  <b>if</b> (!executor_status.status().ok()) {</td></tr>
<tr><th id="275">275</th><td>    <b>return</b> errors::Internal(<q>"Failed to get StreamExecutor for device "</q>,</td></tr>
<tr><th id="276">276</th><td>                            tf_gpu_id_.value());</td></tr>
<tr><th id="277">277</th><td>  }</td></tr>
<tr><th id="278">278</th><td></td></tr>
<tr><th id="279">279</th><td>  executor_ = executor_status.ValueOrDie();</td></tr>
<tr><th id="280">280</th><td>  em_.reset(<b>new</b> EventMgr(executor_, options.config.gpu_options()));</td></tr>
<tr><th id="281">281</th><td></td></tr>
<tr><th id="282">282</th><td>  <b>if</b> (max_streams_ &lt; <var>1</var>) {</td></tr>
<tr><th id="283">283</th><td>    <b>return</b> errors::InvalidArgument(<q>"Invalid value for max_streams."</q>);</td></tr>
<tr><th id="284">284</th><td>  }</td></tr>
<tr><th id="285">285</th><td></td></tr>
<tr><th id="286">286</th><td>  <i>// Create the specified number of GPU streams</i></td></tr>
<tr><th id="287">287</th><td>  <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; max_streams_; i++) {</td></tr>
<tr><th id="288">288</th><td>    streams_.push_back(</td></tr>
<tr><th id="289">289</th><td>        StreamGroupFactory::Global().GetOrCreate(tf_gpu_id_, i, executor_));</td></tr>
<tr><th id="290">290</th><td></td></tr>
<tr><th id="291">291</th><td>    size_t scratch_buffer_size = Eigen::kCudaScratchSize + <b>sizeof</b>(<em>unsigned</em> <em>int</em>);</td></tr>
<tr><th id="292">292</th><td>    <em>void</em>* scratch_buffer = gpu_allocator_-&gt;AllocateRaw(</td></tr>
<tr><th id="293">293</th><td>        Allocator::kAllocatorAlignment, scratch_buffer_size);</td></tr>
<tr><th id="294">294</th><td>    <b>if</b> (scratch_buffer == <b>nullptr</b>) {</td></tr>
<tr><th id="295">295</th><td>      <b>return</b> errors::FailedPrecondition(</td></tr>
<tr><th id="296">296</th><td>          <q>"Failed to allocate scratch buffer for device "</q>, tf_gpu_id_.value());</td></tr>
<tr><th id="297">297</th><td>    }</td></tr>
<tr><th id="298">298</th><td>    scratch_.push_back(<b>static_cast</b>&lt;<em>char</em>*&gt;(scratch_buffer));</td></tr>
<tr><th id="299">299</th><td></td></tr>
<tr><th id="300">300</th><td>    perftools::gputools::DeviceMemory&lt;<em>char</em>&gt; mem(</td></tr>
<tr><th id="301">301</th><td>        perftools::gputools::DeviceMemoryBase(scratch_buffer,</td></tr>
<tr><th id="302">302</th><td>                                              scratch_buffer_size));</td></tr>
<tr><th id="303">303</th><td></td></tr>
<tr><th id="304">304</th><td>    <em>bool</em> ok = executor_-&gt;SynchronousMemZero(</td></tr>
<tr><th id="305">305</th><td>        &amp;mem, Eigen::kCudaScratchSize + <b>sizeof</b>(<em>unsigned</em> <em>int</em>));</td></tr>
<tr><th id="306">306</th><td>    <b>if</b> (!ok) {</td></tr>
<tr><th id="307">307</th><td>      <b>return</b> errors::FailedPrecondition(</td></tr>
<tr><th id="308">308</th><td>          <q>"Failed to memcopy into scratch buffer for device "</q>,</td></tr>
<tr><th id="309">309</th><td>          tf_gpu_id_.value());</td></tr>
<tr><th id="310">310</th><td>    }</td></tr>
<tr><th id="311">311</th><td></td></tr>
<tr><th id="312">312</th><td>    device_contexts_.push_back(<b>new</b> GPUDeviceContext(</td></tr>
<tr><th id="313">313</th><td>        i, streams_.back()-&gt;compute, streams_.back()-&gt;host_to_device,</td></tr>
<tr><th id="314">314</th><td>        streams_.back()-&gt;device_to_host, streams_.back()-&gt;device_to_device));</td></tr>
<tr><th id="315">315</th><td>  }</td></tr>
<tr><th id="316">316</th><td>  gpu_device_info_ = <b>new</b> GpuDeviceInfo;</td></tr>
<tr><th id="317">317</th><td>  gpu_device_info_-&gt;stream = streams_[<var>0</var>]-&gt;compute;</td></tr>
<tr><th id="318">318</th><td>  gpu_device_info_-&gt;default_context = device_contexts_[<var>0</var>];</td></tr>
<tr><th id="319">319</th><td>  gpu_device_info_-&gt;event_mgr = em_.get();</td></tr>
<tr><th id="320">320</th><td>  gpu_device_info_-&gt;gpu_id = GpuIdManager::TfToCudaGpuId(tf_gpu_id_).value();</td></tr>
<tr><th id="321">321</th><td>  set_tensorflow_gpu_device_info(gpu_device_info_);</td></tr>
<tr><th id="322">322</th><td></td></tr>
<tr><th id="323">323</th><td>  <i>// Whether and how the GPU device uses its own threadpool.</i></td></tr>
<tr><th id="324">324</th><td><i>  // This option is experimental. Once we confirm the best setting, we</i></td></tr>
<tr><th id="325">325</th><td><i>  // may change the default behavior and completely remove this flag.</i></td></tr>
<tr><th id="326">326</th><td><i>  // Default values might change in future releases.</i></td></tr>
<tr><th id="327">327</th><td><i>  // Possible values:</i></td></tr>
<tr><th id="328">328</th><td><i>  //   * global: GPU uses threads shared with CPU in the main compute</i></td></tr>
<tr><th id="329">329</th><td><i>  //          thread-pool. This is currently the default.</i></td></tr>
<tr><th id="330">330</th><td><i>  //   * gpu_private: GPU uses threads dedicated to this device.</i></td></tr>
<tr><th id="331">331</th><td><i>  //   * gpu_shared: All GPUs share a dedicated thread pool.</i></td></tr>
<tr><th id="332">332</th><td>  string gpu_thread_mode;</td></tr>
<tr><th id="333">333</th><td>  TF_RETURN_IF_ERROR(</td></tr>
<tr><th id="334">334</th><td>      ReadStringFromEnvVar(<q>"TF_GPU_THREAD_MODE"</q>, <q>"global"</q>, &amp;gpu_thread_mode));</td></tr>
<tr><th id="335">335</th><td>  gpu_thread_mode = str_util::Lowercase(gpu_thread_mode);</td></tr>
<tr><th id="336">336</th><td>  <b>if</b> (gpu_thread_mode != <q>"global"</q>) {</td></tr>
<tr><th id="337">337</th><td>    int64 gpu_thread_count = -<var>1</var>;</td></tr>
<tr><th id="338">338</th><td>    <i>// Default to two threads. One for device compute and another for memory</i></td></tr>
<tr><th id="339">339</th><td><i>    // copies.</i></td></tr>
<tr><th id="340">340</th><td>    TF_RETURN_IF_ERROR(</td></tr>
<tr><th id="341">341</th><td>        ReadInt64FromEnvVar(<q>"TF_GPU_THREAD_COUNT"</q>, <var>2</var>, &amp;gpu_thread_count));</td></tr>
<tr><th id="342">342</th><td>    <b>if</b> (gpu_thread_mode == <q>"gpu_private"</q>) {</td></tr>
<tr><th id="343">343</th><td>      <i>// TODO(zhengxq): since these threads only serve a single GPU device,</i></td></tr>
<tr><th id="344">344</th><td><i>      //   we should set the device context once for each thread, and avoid</i></td></tr>
<tr><th id="345">345</th><td><i>      //   setting them for each kernel.</i></td></tr>
<tr><th id="346">346</th><td><i>      // TODO(zhengxq): pin the thread to the same socket of the target GPU.</i></td></tr>
<tr><th id="347">347</th><td>      thread_pool_.reset(<b>new</b> thread::ThreadPool(</td></tr>
<tr><th id="348">348</th><td>          options.env, strings::StrCat(<q>"gpu_private_"</q>, tf_gpu_id_.value()),</td></tr>
<tr><th id="349">349</th><td>          <b>static_cast</b>&lt;int32&gt;(gpu_thread_count)));</td></tr>
<tr><th id="350">350</th><td>      set_tensorflow_device_thread_pool(thread_pool_.get());</td></tr>
<tr><th id="351">351</th><td>    } <b>else</b> <b>if</b> (gpu_thread_mode == <q>"gpu_shared"</q>) {</td></tr>
<tr><th id="352">352</th><td>      <em>static</em> thread::ThreadPool* thread_pool = <b>new</b> thread::ThreadPool(</td></tr>
<tr><th id="353">353</th><td>          options.env, <q>"gpu_shared"</q>, <b>static_cast</b>&lt;int32&gt;(gpu_thread_count));</td></tr>
<tr><th id="354">354</th><td>      set_tensorflow_device_thread_pool(thread_pool);</td></tr>
<tr><th id="355">355</th><td>    } <b>else</b> {</td></tr>
<tr><th id="356">356</th><td>      string error_message =</td></tr>
<tr><th id="357">357</th><td>          strings::StrCat(<q>"Invalid gpu_thread_mode: "</q>, gpu_thread_mode);</td></tr>
<tr><th id="358">358</th><td>      LOG(WARNING) &lt;&lt; error_message;</td></tr>
<tr><th id="359">359</th><td>      <b>return</b> errors::InvalidArgument(error_message);</td></tr>
<tr><th id="360">360</th><td>    }</td></tr>
<tr><th id="361">361</th><td>  }</td></tr>
<tr><th id="362">362</th><td></td></tr>
<tr><th id="363">363</th><td>  <b>return</b> Status::OK();</td></tr>
<tr><th id="364">364</th><td>}</td></tr>
<tr><th id="365">365</th><td></td></tr>
<tr><th id="366">366</th><td><em>bool</em> BaseGPUDevice::RequiresRecordingAccessedTensors() <em>const</em> {</td></tr>
<tr><th id="367">367</th><td>  <i>// When there is no more than one stream, we release the tensor reference</i></td></tr>
<tr><th id="368">368</th><td><i>  // at the end of the kernel launch, instead of at the end of the kernel</i></td></tr>
<tr><th id="369">369</th><td><i>  // execution.</i></td></tr>
<tr><th id="370">370</th><td>  <b>return</b> streams_.size() &gt; <var>1</var>;</td></tr>
<tr><th id="371">371</th><td>}</td></tr>
<tr><th id="372">372</th><td></td></tr>
<tr><th id="373">373</th><td>Status BaseGPUDevice::FillContextMap(<em>const</em> Graph* graph,</td></tr>
<tr><th id="374">374</th><td>                                     DeviceContextMap* device_context_map) {</td></tr>
<tr><th id="375">375</th><td>  VLOG(<var>2</var>) &lt;&lt; <q>"FillContextMap"</q>;</td></tr>
<tr><th id="376">376</th><td></td></tr>
<tr><th id="377">377</th><td>  <em>const</em> size_t num_streams = streams_.size();</td></tr>
<tr><th id="378">378</th><td>  <i>// Special case for single stream.</i></td></tr>
<tr><th id="379">379</th><td>  <b>if</b> (num_streams == <var>1</var>) {</td></tr>
<tr><th id="380">380</th><td>    <b>return</b> Status::OK();</td></tr>
<tr><th id="381">381</th><td>  }</td></tr>
<tr><th id="382">382</th><td>  <em>const</em> int64 before = Env::Default()-&gt;NowMicros();</td></tr>
<tr><th id="383">383</th><td>  gpu_stream_util::AssignStreamsOpts opts;</td></tr>
<tr><th id="384">384</th><td>  opts.max_streams = <b>static_cast</b>&lt;int32&gt;(num_streams);</td></tr>
<tr><th id="385">385</th><td>  std::unordered_map&lt;<em>int</em>, <em>int</em>&gt; node_to_stream_id;</td></tr>
<tr><th id="386">386</th><td>  TF_RETURN_IF_ERROR(</td></tr>
<tr><th id="387">387</th><td>      gpu_stream_util::AssignStreams(graph, opts, &amp;node_to_stream_id));</td></tr>
<tr><th id="388">388</th><td>  int64 elapsed = Env::Default()-&gt;NowMicros() - before;</td></tr>
<tr><th id="389">389</th><td>  VLOG(<var>3</var>) &lt;&lt; <q>"AssignStreams took "</q> &lt;&lt; elapsed &lt;&lt; <q>"us"</q>;</td></tr>
<tr><th id="390">390</th><td></td></tr>
<tr><th id="391">391</th><td>  <i>// Fill in the context map.  It is OK for this map to contain</i></td></tr>
<tr><th id="392">392</th><td><i>  // duplicate DeviceContexts so long as we increment the refcount.</i></td></tr>
<tr><th id="393">393</th><td>  device_context_map-&gt;resize(graph-&gt;num_node_ids());</td></tr>
<tr><th id="394">394</th><td>  <b>for</b> (Node* n : graph-&gt;nodes()) {</td></tr>
<tr><th id="395">395</th><td>    <em>auto</em> mapped_stream = node_to_stream_id[n-&gt;id()];</td></tr>
<tr><th id="396">396</th><td>    CHECK_LE(mapped_stream, num_streams);</td></tr>
<tr><th id="397">397</th><td>    <em>auto</em> ctx = device_contexts_[mapped_stream];</td></tr>
<tr><th id="398">398</th><td>    VLOG(<var>3</var>) &lt;&lt; <q>"Assigned stream "</q> &lt;&lt; node_to_stream_id[n-&gt;id()]</td></tr>
<tr><th id="399">399</th><td>            &lt;&lt; <q>" ==&gt; stream["</q> &lt;&lt; ctx-&gt;stream_id() &lt;&lt; <q>"] for node id "</q> &lt;&lt; n-&gt;id()</td></tr>
<tr><th id="400">400</th><td>            &lt;&lt; <q>" "</q> &lt;&lt; n-&gt;type_string() &lt;&lt; <q>" "</q> &lt;&lt; n-&gt;name();</td></tr>
<tr><th id="401">401</th><td>    ctx-&gt;Ref();</td></tr>
<tr><th id="402">402</th><td>    (*device_context_map)[n-&gt;id()] = ctx;</td></tr>
<tr><th id="403">403</th><td>  }</td></tr>
<tr><th id="404">404</th><td></td></tr>
<tr><th id="405">405</th><td>  <b>return</b> Status::OK();</td></tr>
<tr><th id="406">406</th><td>}</td></tr>
<tr><th id="407">407</th><td></td></tr>
<tr><th id="408">408</th><td><em>void</em> BaseGPUDevice::Compute(OpKernel* op_kernel, OpKernelContext* context) {</td></tr>
<tr><th id="409">409</th><td>  <i>// ScopedActivity is cheap when tracing is not active, but we</i></td></tr>
<tr><th id="410">410</th><td><i>  // can avoid computing the Hash64.</i></td></tr>
<tr><th id="411">411</th><td><i>  // TODO(pbar) This would no longer be needed if Ops have a unique id.</i></td></tr>
<tr><th id="412">412</th><td>  <em>const</em> uint64 id = port::Tracing::IsActive() ? Hash64(op_kernel-&gt;name()) : <var>0</var>;</td></tr>
<tr><th id="413">413</th><td>  port::Tracing::ScopedActivity region(port::Tracing::EventCategory::kCompute,</td></tr>
<tr><th id="414">414</th><td>                                       id);</td></tr>
<tr><th id="415">415</th><td></td></tr>
<tr><th id="416">416</th><td>  <i>// NOTE(tucker): We need to discriminate between Eigen GPU</i></td></tr>
<tr><th id="417">417</th><td><i>  // operations and all others.  If an operation is Eigen</i></td></tr>
<tr><th id="418">418</th><td><i>  // implemented (or otherwise tries to launch a cuda kernel</i></td></tr>
<tr><th id="419">419</th><td><i>  // directly), we need to establish a stacked-scoped environment</i></td></tr>
<tr><th id="420">420</th><td><i>  // that directs it to execute on the proper device.  Otherwise we</i></td></tr>
<tr><th id="421">421</th><td><i>  // expect the Op to use StreamExecutor directly and correctly.  The</i></td></tr>
<tr><th id="422">422</th><td><i>  // way we make this discrimination is quite hacky: At the moment</i></td></tr>
<tr><th id="423">423</th><td><i>  // the only non-Eigen GPU Op is the recv-op, which is known to be</i></td></tr>
<tr><th id="424">424</th><td><i>  // asynchronous.</i></td></tr>
<tr><th id="425">425</th><td>  <b>if</b> (op_kernel-&gt;is_internal() &amp;&amp; op_kernel-&gt;type_string() == <q>"_Recv"</q>) {</td></tr>
<tr><th id="426">426</th><td>    context-&gt;SetStatus(errors::Internal(</td></tr>
<tr><th id="427">427</th><td>        <q>"Invalid synchronous 'Compute' on GPU for '_Recv' op"</q>));</td></tr>
<tr><th id="428">428</th><td>  } <b>else</b> <b>if</b> (port::Tracing::ScopedAnnotation::Enabled()) {</td></tr>
<tr><th id="429">429</th><td>    port::Tracing::ScopedAnnotation annotation(op_kernel-&gt;name(),</td></tr>
<tr><th id="430">430</th><td>                                               op_kernel-&gt;type_string());</td></tr>
<tr><th id="431">431</th><td>    ComputeHelper(op_kernel, context);</td></tr>
<tr><th id="432">432</th><td>  } <b>else</b> {</td></tr>
<tr><th id="433">433</th><td>    ComputeHelper(op_kernel, context);</td></tr>
<tr><th id="434">434</th><td>  }</td></tr>
<tr><th id="435">435</th><td>}</td></tr>
<tr><th id="436">436</th><td></td></tr>
<tr><th id="437">437</th><td><em>void</em> BaseGPUDevice::ComputeHelper(OpKernel* op_kernel,</td></tr>
<tr><th id="438">438</th><td>                                  OpKernelContext* context) {</td></tr>
<tr><th id="439">439</th><td>  GPUDeviceContext* gpu_device_context = device_contexts_[<var>0</var>];</td></tr>
<tr><th id="440">440</th><td>  <b>if</b> (context-&gt;op_device_context() != <b>nullptr</b>) {</td></tr>
<tr><th id="441">441</th><td>    gpu_device_context =</td></tr>
<tr><th id="442">442</th><td>        <b>static_cast</b>&lt;GPUDeviceContext*&gt;(context-&gt;op_device_context());</td></tr>
<tr><th id="443">443</th><td>  }</td></tr>
<tr><th id="444">444</th><td>  gpu::Stream* stream = gpu_device_context-&gt;stream();</td></tr>
<tr><th id="445">445</th><td>  <em>const</em> <em>auto</em> stream_id = gpu_device_context-&gt;stream_id();</td></tr>
<tr><th id="446">446</th><td></td></tr>
<tr><th id="447">447</th><td>  <em>const</em> <em>bool</em> vlog_1 = VLOG_IS_ON(<var>1</var>);</td></tr>
<tr><th id="448">448</th><td>  <em>const</em> <em>bool</em> vlog_2 = vlog_1 &amp;&amp; VLOG_IS_ON(<var>2</var>);</td></tr>
<tr><th id="449">449</th><td></td></tr>
<tr><th id="450">450</th><td>  <b>if</b> (vlog_1) {</td></tr>
<tr><th id="451">451</th><td>    VLOG(<var>1</var>) &lt;&lt; <q>"GpuDevice::Compute "</q> &lt;&lt; op_kernel-&gt;name() &lt;&lt; <q>" op "</q></td></tr>
<tr><th id="452">452</th><td>            &lt;&lt; op_kernel-&gt;type_string() &lt;&lt; <q>" on GPU"</q> &lt;&lt; tf_gpu_id_ &lt;&lt; <q>" stream["</q></td></tr>
<tr><th id="453">453</th><td>            &lt;&lt; stream_id &lt;&lt; <q>"]"</q>;</td></tr>
<tr><th id="454">454</th><td>  }</td></tr>
<tr><th id="455">455</th><td></td></tr>
<tr><th id="456">456</th><td>  <em>const</em> <em>auto</em> num_streams = streams_.size();</td></tr>
<tr><th id="457">457</th><td>  <b>if</b> (num_streams &gt; <var>1</var>) {</td></tr>
<tr><th id="458">458</th><td>    <i>// If this op's device context is different from the other contexts,</i></td></tr>
<tr><th id="459">459</th><td><i>    // we must wait on the stream.</i></td></tr>
<tr><th id="460">460</th><td>    <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; context-&gt;num_inputs(); ++i) {</td></tr>
<tr><th id="461">461</th><td>      <em>const</em> GPUDeviceContext* idc =</td></tr>
<tr><th id="462">462</th><td>          <b>static_cast</b>&lt;GPUDeviceContext*&gt;(context-&gt;input_device_context(i));</td></tr>
<tr><th id="463">463</th><td>      OP_REQUIRES(context, idc != <b>nullptr</b>,</td></tr>
<tr><th id="464">464</th><td>                  errors::Internal(<q>"Input device context "</q>, i,</td></tr>
<tr><th id="465">465</th><td>                                   <q>" was not set properly."</q>));</td></tr>
<tr><th id="466">466</th><td>      <b>if</b> (vlog_2) {</td></tr>
<tr><th id="467">467</th><td>        <em>const</em> <em>void</em>* base;</td></tr>
<tr><th id="468">468</th><td>        size_t len;</td></tr>
<tr><th id="469">469</th><td>        <b>if</b> (context-&gt;has_input(i)) {</td></tr>
<tr><th id="470">470</th><td>          <b>if</b> (IsRefType(context-&gt;input_dtype(i))) {</td></tr>
<tr><th id="471">471</th><td>            Tensor tensor = context-&gt;mutable_input(i, <b>false</b>);</td></tr>
<tr><th id="472">472</th><td>            base = DMAHelper::base(&amp;tensor);</td></tr>
<tr><th id="473">473</th><td>            len = tensor.TotalBytes();</td></tr>
<tr><th id="474">474</th><td>          } <b>else</b> {</td></tr>
<tr><th id="475">475</th><td>            <em>const</em> Tensor&amp; tensor = context-&gt;input(i);</td></tr>
<tr><th id="476">476</th><td>            base = DMAHelper::base(&amp;tensor);</td></tr>
<tr><th id="477">477</th><td>            len = tensor.TotalBytes();</td></tr>
<tr><th id="478">478</th><td>          }</td></tr>
<tr><th id="479">479</th><td>          LOG(INFO) &lt;&lt; <q>"Input "</q> &lt;&lt; i &lt;&lt; <q>" "</q> &lt;&lt; base &lt;&lt; <q>"  "</q> &lt;&lt; len;</td></tr>
<tr><th id="480">480</th><td>          LOG(INFO) &lt;&lt; <q>"  stream["</q> &lt;&lt; stream_id &lt;&lt; <q>"].ThenWaitFor(stream["</q></td></tr>
<tr><th id="481">481</th><td>                    &lt;&lt; idc-&gt;stream_id() &lt;&lt; <q>"])"</q></td></tr>
<tr><th id="482">482</th><td>                    &lt;&lt; ((idc-&gt;stream() == stream) ? <q>" not needed"</q> : <q>""</q>);</td></tr>
<tr><th id="483">483</th><td>        }</td></tr>
<tr><th id="484">484</th><td>      }</td></tr>
<tr><th id="485">485</th><td>      <b>if</b> (idc-&gt;stream() != stream) stream-&gt;ThenWaitFor(idc-&gt;stream());</td></tr>
<tr><th id="486">486</th><td>    }</td></tr>
<tr><th id="487">487</th><td>  }</td></tr>
<tr><th id="488">488</th><td>  gpu::cuda::ScopedActivateExecutorContext scoped_activation{stream-&gt;parent()};</td></tr>
<tr><th id="489">489</th><td>  op_kernel-&gt;Compute(context);</td></tr>
<tr><th id="490">490</th><td>  <b>if</b> (context-&gt;status().ok()) {</td></tr>
<tr><th id="491">491</th><td>    <b>if</b> (sync_every_op_) {</td></tr>
<tr><th id="492">492</th><td>      <i>// Note: GPUUtil::Sync() only syncs the default stream.</i></td></tr>
<tr><th id="493">493</th><td><i>      // We need to either sync the stream used by this op, or</i></td></tr>
<tr><th id="494">494</th><td><i>      // all streams.  Given that this flag is typically used for</i></td></tr>
<tr><th id="495">495</th><td><i>      // debugging it makes more sense to sync all GPU activity.</i></td></tr>
<tr><th id="496">496</th><td>      context-&gt;SetStatus(GPUUtil::SyncAll(<b>this</b>));</td></tr>
<tr><th id="497">497</th><td>    }</td></tr>
<tr><th id="498">498</th><td>  }</td></tr>
<tr><th id="499">499</th><td>}</td></tr>
<tr><th id="500">500</th><td></td></tr>
<tr><th id="501">501</th><td><em>void</em> BaseGPUDevice::ConsumeListOfAccessedTensors(</td></tr>
<tr><th id="502">502</th><td>    DeviceContext* device_context, <em>const</em> TensorReferenceVector&amp; tensor_refs) {</td></tr>
<tr><th id="503">503</th><td>  GPUDeviceContext* gpu_device_context = device_contexts_[<var>0</var>];</td></tr>
<tr><th id="504">504</th><td>  <b>if</b> (device_context != <b>nullptr</b>) {</td></tr>
<tr><th id="505">505</th><td>    gpu_device_context = <b>static_cast</b>&lt;GPUDeviceContext*&gt;(device_context);</td></tr>
<tr><th id="506">506</th><td>  }</td></tr>
<tr><th id="507">507</th><td>  gpu::Stream* stream = gpu_device_context-&gt;stream();</td></tr>
<tr><th id="508">508</th><td>  em_-&gt;ThenDeleteTensors(stream, tensor_refs);</td></tr>
<tr><th id="509">509</th><td>}</td></tr>
<tr><th id="510">510</th><td></td></tr>
<tr><th id="511">511</th><td><i>// Based on the semantics of Device::Sync this call should wait for</i></td></tr>
<tr><th id="512">512</th><td><i>// all streams not just the current one.</i></td></tr>
<tr><th id="513">513</th><td>Status BaseGPUDevice::Sync() { <b>return</b> GPUUtil::SyncAll(<b>this</b>); }</td></tr>
<tr><th id="514">514</th><td></td></tr>
<tr><th id="515">515</th><td><em>void</em> BaseGPUDevice::ComputeAsync(AsyncOpKernel* op_kernel,</td></tr>
<tr><th id="516">516</th><td>                                 OpKernelContext* context,</td></tr>
<tr><th id="517">517</th><td>                                 AsyncOpKernel::DoneCallback done) {</td></tr>
<tr><th id="518">518</th><td>  GPUDeviceContext* gpu_device_context = device_contexts_[<var>0</var>];</td></tr>
<tr><th id="519">519</th><td>  <b>if</b> (context-&gt;op_device_context() != <b>nullptr</b>) {</td></tr>
<tr><th id="520">520</th><td>    gpu_device_context =</td></tr>
<tr><th id="521">521</th><td>        <b>static_cast</b>&lt;GPUDeviceContext*&gt;(context-&gt;op_device_context());</td></tr>
<tr><th id="522">522</th><td>  }</td></tr>
<tr><th id="523">523</th><td>  gpu::Stream* stream = gpu_device_context-&gt;stream();</td></tr>
<tr><th id="524">524</th><td>  <em>const</em> <em>auto</em> stream_id = gpu_device_context-&gt;stream_id();</td></tr>
<tr><th id="525">525</th><td></td></tr>
<tr><th id="526">526</th><td>  VLOG(<var>1</var>) &lt;&lt; <q>"GpuDevice::ComputeAsync "</q> &lt;&lt; op_kernel-&gt;name() &lt;&lt; <q>" op "</q></td></tr>
<tr><th id="527">527</th><td>          &lt;&lt; op_kernel-&gt;type_string() &lt;&lt; <q>" on GPU"</q> &lt;&lt; tf_gpu_id_ &lt;&lt; <q>" stream["</q></td></tr>
<tr><th id="528">528</th><td>          &lt;&lt; stream_id &lt;&lt; <q>"]"</q>;</td></tr>
<tr><th id="529">529</th><td></td></tr>
<tr><th id="530">530</th><td>  <i>// When TraceMe profiling is off (which is the default), the</i></td></tr>
<tr><th id="531">531</th><td><i>  // following TraceMe constructor is simply a conditional test of</i></td></tr>
<tr><th id="532">532</th><td><i>  // false value. Measurements show that its overhead is negligible.</i></td></tr>
<tr><th id="533">533</th><td>  port::Tracing::TraceMe activity(op_kernel-&gt;name(), op_kernel-&gt;type_string(),</td></tr>
<tr><th id="534">534</th><td>                                  op_kernel-&gt;IsExpensive());</td></tr>
<tr><th id="535">535</th><td>  gpu::cuda::ScopedActivateExecutorContext scoped_activation{stream-&gt;parent()};</td></tr>
<tr><th id="536">536</th><td>  op_kernel-&gt;ComputeAsync(context, done);</td></tr>
<tr><th id="537">537</th><td>}</td></tr>
<tr><th id="538">538</th><td></td></tr>
<tr><th id="539">539</th><td>Status BaseGPUDevice::MaybeCopyTensorToGPU(</td></tr>
<tr><th id="540">540</th><td>    <em>const</em> AllocatorAttributes&amp; alloc_attrs, <em>const</em> Tensor&amp; from, Tensor* to,</td></tr>
<tr><th id="541">541</th><td>    StatusCallback done) {</td></tr>
<tr><th id="542">542</th><td>  <b>if</b> (alloc_attrs.on_host()) {</td></tr>
<tr><th id="543">543</th><td>    *to = from;</td></tr>
<tr><th id="544">544</th><td>    done(Status::OK());</td></tr>
<tr><th id="545">545</th><td>    <b>return</b> Status::OK();</td></tr>
<tr><th id="546">546</th><td>  } <b>else</b> {</td></tr>
<tr><th id="547">547</th><td>    <b>if</b> (!DMAHelper::CanUseDMA(&amp;from)) {</td></tr>
<tr><th id="548">548</th><td>      Status err = errors::Internal(<q>"GPU copy from non-DMA "</q>,</td></tr>
<tr><th id="549">549</th><td>                                    DataTypeString(from.dtype()), <q>" tensor"</q>);</td></tr>
<tr><th id="550">550</th><td>      done(err);</td></tr>
<tr><th id="551">551</th><td>      <b>return</b> err;</td></tr>
<tr><th id="552">552</th><td>    }</td></tr>
<tr><th id="553">553</th><td>    <em>auto</em>* copy =</td></tr>
<tr><th id="554">554</th><td>        <b>new</b> Tensor(GetAllocator(alloc_attrs), from.dtype(), from.shape());</td></tr>
<tr><th id="555">555</th><td></td></tr>
<tr><th id="556">556</th><td>    <i>// If the tensor is not initialized, we likely ran out of memory.</i></td></tr>
<tr><th id="557">557</th><td>    <b>if</b> (!copy-&gt;IsInitialized()) {</td></tr>
<tr><th id="558">558</th><td>      <b>delete</b> copy;</td></tr>
<tr><th id="559">559</th><td>      Status err = errors::ResourceExhausted(</td></tr>
<tr><th id="560">560</th><td>          <q>"OOM when allocating tensor of shape "</q>, from.shape().DebugString(),</td></tr>
<tr><th id="561">561</th><td>          <q>" and type "</q>, DataTypeString(from.dtype()));</td></tr>
<tr><th id="562">562</th><td>      done(err);</td></tr>
<tr><th id="563">563</th><td>      <b>return</b> err;</td></tr>
<tr><th id="564">564</th><td>    }</td></tr>
<tr><th id="565">565</th><td></td></tr>
<tr><th id="566">566</th><td>    StatusCallback wrapped_done = std::bind(</td></tr>
<tr><th id="567">567</th><td>        [to, copy](StatusCallback done_,</td></tr>
<tr><th id="568">568</th><td>                   <i>// Begin unbound arguments.</i></td></tr>
<tr><th id="569">569</th><td>                   <em>const</em> Status&amp; s) {</td></tr>
<tr><th id="570">570</th><td>          *to = std::move(*copy);</td></tr>
<tr><th id="571">571</th><td>          <b>delete</b> copy;</td></tr>
<tr><th id="572">572</th><td>          done_(s);</td></tr>
<tr><th id="573">573</th><td>        },</td></tr>
<tr><th id="574">574</th><td>        std::move(done), std::placeholders::_1);</td></tr>
<tr><th id="575">575</th><td></td></tr>
<tr><th id="576">576</th><td>    port::Tracing::ScopedAnnotation annotation(<q>"MakeTensorFromProto"</q>);</td></tr>
<tr><th id="577">577</th><td>    device_contexts_[<var>0</var>]-&gt;CopyCPUTensorToDevice(&amp;from, <b>this</b>, copy,</td></tr>
<tr><th id="578">578</th><td>                                               std::move(wrapped_done));</td></tr>
<tr><th id="579">579</th><td>    <b>return</b> Status::OK();</td></tr>
<tr><th id="580">580</th><td>  }</td></tr>
<tr><th id="581">581</th><td>}</td></tr>
<tr><th id="582">582</th><td></td></tr>
<tr><th id="583">583</th><td>Status BaseGPUDevice::MakeTensorFromProto(<em>const</em> TensorProto&amp; tensor_proto,</td></tr>
<tr><th id="584">584</th><td>                                          <em>const</em> AllocatorAttributes alloc_attrs,</td></tr>
<tr><th id="585">585</th><td>                                          Tensor* tensor) {</td></tr>
<tr><th id="586">586</th><td>  AllocatorAttributes attr;</td></tr>
<tr><th id="587">587</th><td>  attr.set_on_host(<b>true</b>);</td></tr>
<tr><th id="588">588</th><td>  attr.set_gpu_compatible(<b>true</b>);</td></tr>
<tr><th id="589">589</th><td>  Allocator* host_alloc = GetAllocator(attr);</td></tr>
<tr><th id="590">590</th><td>  Tensor parsed(tensor_proto.dtype());</td></tr>
<tr><th id="591">591</th><td>  <b>if</b> (!parsed.FromProto(host_alloc, tensor_proto)) {</td></tr>
<tr><th id="592">592</th><td>    <b>return</b> errors::InvalidArgument(<q>"Cannot parse tensor from proto: "</q>,</td></tr>
<tr><th id="593">593</th><td>                                   tensor_proto.DebugString());</td></tr>
<tr><th id="594">594</th><td>  }</td></tr>
<tr><th id="595">595</th><td></td></tr>
<tr><th id="596">596</th><td>  <b>if</b> (parsed.dtype() == DT_VARIANT) {</td></tr>
<tr><th id="597">597</th><td>    <em>const</em> Variant* from = parsed.flat&lt;Variant&gt;().data();</td></tr>
<tr><th id="598">598</th><td>    Tensor copy(cpu_allocator(), DT_VARIANT, parsed.shape());</td></tr>
<tr><th id="599">599</th><td>    Variant* copy_variant = copy.flat&lt;Variant&gt;().data();</td></tr>
<tr><th id="600">600</th><td></td></tr>
<tr><th id="601">601</th><td>    std::list&lt;Notification&gt; notifications;</td></tr>
<tr><th id="602">602</th><td>    Status copy_status;</td></tr>
<tr><th id="603">603</th><td>    <em>auto</em> copier = [<b>this</b>, &amp;alloc_attrs, &amp;notifications, &amp;copy_status](</td></tr>
<tr><th id="604">604</th><td>                      <em>const</em> Tensor&amp; from, Tensor* to) {</td></tr>
<tr><th id="605">605</th><td>      <i>// Copier isn't run in a multithreaded environment, so we don't</i></td></tr>
<tr><th id="606">606</th><td><i>      // have to worry about the notifications list being modified in parallel.</i></td></tr>
<tr><th id="607">607</th><td>      notifications.emplace_back();</td></tr>
<tr><th id="608">608</th><td>      Notification&amp; n = *notifications.rbegin();</td></tr>
<tr><th id="609">609</th><td>      <b>return</b> MaybeCopyTensorToGPU(alloc_attrs, from, to,</td></tr>
<tr><th id="610">610</th><td>                                  [&amp;n, &amp;copy_status](<em>const</em> Status&amp; s) {</td></tr>
<tr><th id="611">611</th><td>                                    <b>if</b> (copy_status.ok()) {</td></tr>
<tr><th id="612">612</th><td>                                      copy_status.Update(s);</td></tr>
<tr><th id="613">613</th><td>                                    }</td></tr>
<tr><th id="614">614</th><td>                                    n.Notify();</td></tr>
<tr><th id="615">615</th><td>                                  });</td></tr>
<tr><th id="616">616</th><td>    };</td></tr>
<tr><th id="617">617</th><td>    Status s;</td></tr>
<tr><th id="618">618</th><td>    <b>for</b> (int64 ix = <var>0</var>; ix &lt; parsed.NumElements(); ++ix) {</td></tr>
<tr><th id="619">619</th><td>      s = VariantDeviceCopy(VariantDeviceCopyDirection::HOST_TO_DEVICE,</td></tr>
<tr><th id="620">620</th><td>                            from[ix], &amp;copy_variant[ix], copier);</td></tr>
<tr><th id="621">621</th><td>      <b>if</b> (!s.ok()) {</td></tr>
<tr><th id="622">622</th><td>        <b>break</b>;</td></tr>
<tr><th id="623">623</th><td>      }</td></tr>
<tr><th id="624">624</th><td>    }</td></tr>
<tr><th id="625">625</th><td>    <b>for</b> (<em>auto</em>&amp; n : notifications) {</td></tr>
<tr><th id="626">626</th><td>      n.WaitForNotification();</td></tr>
<tr><th id="627">627</th><td>    }</td></tr>
<tr><th id="628">628</th><td>    <b>if</b> (!s.ok()) {</td></tr>
<tr><th id="629">629</th><td>      <b>return</b> s;</td></tr>
<tr><th id="630">630</th><td>    }</td></tr>
<tr><th id="631">631</th><td>    *tensor = std::move(copy);</td></tr>
<tr><th id="632">632</th><td>    <b>return</b> copy_status;</td></tr>
<tr><th id="633">633</th><td>  } <b>else</b> {</td></tr>
<tr><th id="634">634</th><td>    Notification n;</td></tr>
<tr><th id="635">635</th><td>    Status status;</td></tr>
<tr><th id="636">636</th><td>    TF_RETURN_IF_ERROR(MaybeCopyTensorToGPU(alloc_attrs, parsed, tensor,</td></tr>
<tr><th id="637">637</th><td>                                            [&amp;n, &amp;status](<em>const</em> Status&amp; s) {</td></tr>
<tr><th id="638">638</th><td>                                              status = s;</td></tr>
<tr><th id="639">639</th><td>                                              n.Notify();</td></tr>
<tr><th id="640">640</th><td>                                            }));</td></tr>
<tr><th id="641">641</th><td>    n.WaitForNotification();</td></tr>
<tr><th id="642">642</th><td>    <b>return</b> status;</td></tr>
<tr><th id="643">643</th><td>  }</td></tr>
<tr><th id="644">644</th><td>}</td></tr>
<tr><th id="645">645</th><td></td></tr>
<tr><th id="646">646</th><td><b>namespace</b> {</td></tr>
<tr><th id="647">647</th><td><b>class</b> ConcretePerOpGpuDevice : <b>public</b> PerOpGpuDevice {</td></tr>
<tr><th id="648">648</th><td> <b>public</b>:</td></tr>
<tr><th id="649">649</th><td>  ConcretePerOpGpuDevice() : device_(&amp;stream_device_) {}</td></tr>
<tr><th id="650">650</th><td></td></tr>
<tr><th id="651">651</th><td>  <em>void</em> Reinitialize(OpKernelContext* context, <em>const</em> cudaStream_t* cuda_stream,</td></tr>
<tr><th id="652">652</th><td>                    TfGpuId tf_gpu_id, Allocator* base_allocator,</td></tr>
<tr><th id="653">653</th><td>                    <em>char</em>* scratch) {</td></tr>
<tr><th id="654">654</th><td>    stream_device_.Reinitialize(context, cuda_stream, tf_gpu_id, base_allocator,</td></tr>
<tr><th id="655">655</th><td>                                scratch);</td></tr>
<tr><th id="656">656</th><td>  }</td></tr>
<tr><th id="657">657</th><td></td></tr>
<tr><th id="658">658</th><td>  <em>const</em> Eigen::GpuDevice&amp; device() <em>const</em> override { <b>return</b> device_; }</td></tr>
<tr><th id="659">659</th><td></td></tr>
<tr><th id="660">660</th><td> <b>private</b>:</td></tr>
<tr><th id="661">661</th><td>  EigenCudaStreamDevice stream_device_;</td></tr>
<tr><th id="662">662</th><td>  Eigen::GpuDevice device_;</td></tr>
<tr><th id="663">663</th><td>};</td></tr>
<tr><th id="664">664</th><td></td></tr>
<tr><th id="665">665</th><td><i>// Parse 'visible_device_list' into a list of CUDA GPU ids.</i></td></tr>
<tr><th id="666">666</th><td>Status ParseVisibleDeviceList(<em>const</em> string&amp; visible_device_list,</td></tr>
<tr><th id="667">667</th><td>                              std::vector&lt;CudaGpuId&gt;* visible_gpu_order) {</td></tr>
<tr><th id="668">668</th><td>  visible_gpu_order-&gt;clear();</td></tr>
<tr><th id="669">669</th><td>  gpu::Platform* gpu_manager = GPUMachineManager();</td></tr>
<tr><th id="670">670</th><td></td></tr>
<tr><th id="671">671</th><td>  <i>// If the user wants to remap the visible to virtual GPU mapping,</i></td></tr>
<tr><th id="672">672</th><td><i>  // check for that here.</i></td></tr>
<tr><th id="673">673</th><td>  <b>if</b> (visible_device_list.empty()) {</td></tr>
<tr><th id="674">674</th><td>    visible_gpu_order-&gt;resize(gpu_manager-&gt;VisibleDeviceCount());</td></tr>
<tr><th id="675">675</th><td>    <i>// By default, visible to virtual mapping is unchanged.</i></td></tr>
<tr><th id="676">676</th><td>    <em>int</em> deviceNo = <var>0</var>;</td></tr>
<tr><th id="677">677</th><td>    std::generate(visible_gpu_order-&gt;begin(), visible_gpu_order-&gt;end(),</td></tr>
<tr><th id="678">678</th><td>                  [&amp;deviceNo] { <b>return</b> deviceNo++; });</td></tr>
<tr><th id="679">679</th><td>  } <b>else</b> {</td></tr>
<tr><th id="680">680</th><td>    <em>const</em> std::vector&lt;string&gt; order_str =</td></tr>
<tr><th id="681">681</th><td>        str_util::Split(visible_device_list, <kbd>','</kbd>);</td></tr>
<tr><th id="682">682</th><td>    <b>for</b> (<em>const</em> string&amp; cuda_gpu_id_str : order_str) {</td></tr>
<tr><th id="683">683</th><td>      int32 cuda_gpu_id;</td></tr>
<tr><th id="684">684</th><td>      <b>if</b> (!strings::safe_strto32(cuda_gpu_id_str, &amp;cuda_gpu_id)) {</td></tr>
<tr><th id="685">685</th><td>        <b>return</b> errors::InvalidArgument(</td></tr>
<tr><th id="686">686</th><td>            <q>"Could not parse entry in 'visible_device_list': '"</q>,</td></tr>
<tr><th id="687">687</th><td>            cuda_gpu_id_str, <q>"'. visible_device_list = "</q>, visible_device_list);</td></tr>
<tr><th id="688">688</th><td>      }</td></tr>
<tr><th id="689">689</th><td>      <b>if</b> (cuda_gpu_id &lt; <var>0</var> || cuda_gpu_id &gt;= gpu_manager-&gt;VisibleDeviceCount()) {</td></tr>
<tr><th id="690">690</th><td>        <b>return</b> errors::InvalidArgument(</td></tr>
<tr><th id="691">691</th><td>            <q>"'visible_device_list' listed an invalid GPU id '"</q>, cuda_gpu_id,</td></tr>
<tr><th id="692">692</th><td>            <q>"' but visible device count is "</q>,</td></tr>
<tr><th id="693">693</th><td>            gpu_manager-&gt;VisibleDeviceCount());</td></tr>
<tr><th id="694">694</th><td>      }</td></tr>
<tr><th id="695">695</th><td>      visible_gpu_order-&gt;push_back(CudaGpuId(cuda_gpu_id));</td></tr>
<tr><th id="696">696</th><td>    }</td></tr>
<tr><th id="697">697</th><td>  }</td></tr>
<tr><th id="698">698</th><td></td></tr>
<tr><th id="699">699</th><td>  <i>// Validate no repeats.</i></td></tr>
<tr><th id="700">700</th><td>  std::set&lt;CudaGpuId&gt; visible_device_set(visible_gpu_order-&gt;begin(),</td></tr>
<tr><th id="701">701</th><td>                                         visible_gpu_order-&gt;end());</td></tr>
<tr><th id="702">702</th><td>  <b>if</b> (visible_device_set.size() != visible_gpu_order-&gt;size()) {</td></tr>
<tr><th id="703">703</th><td>    <b>return</b> errors::InvalidArgument(</td></tr>
<tr><th id="704">704</th><td>        <q>"visible_device_list contained a duplicate entry: "</q>,</td></tr>
<tr><th id="705">705</th><td>        visible_device_list);</td></tr>
<tr><th id="706">706</th><td>  }</td></tr>
<tr><th id="707">707</th><td>  <b>return</b> Status::OK();</td></tr>
<tr><th id="708">708</th><td>}</td></tr>
<tr><th id="709">709</th><td></td></tr>
<tr><th id="710">710</th><td>Status VerifyVirtualDeviceSettings(</td></tr>
<tr><th id="711">711</th><td>    <em>const</em> size_t num_gpus_to_use, <em>const</em> GPUOptions&amp; gpu_options,</td></tr>
<tr><th id="712">712</th><td>    <em>const</em> std::vector&lt;CudaGpuId&gt;&amp; visible_gpu_order,</td></tr>
<tr><th id="713">713</th><td>    <em>const</em> std::vector&lt;CudaGpuId&gt;&amp; valid_cuda_gpu_ids) {</td></tr>
<tr><th id="714">714</th><td>  <em>const</em> <em>auto</em>&amp; virtual_devices = gpu_options.experimental().virtual_devices();</td></tr>
<tr><th id="715">715</th><td>  CHECK(!virtual_devices.empty());</td></tr>
<tr><th id="716">716</th><td>  <b>if</b> (gpu_options.per_process_gpu_memory_fraction() &gt; <var>0</var>) {</td></tr>
<tr><th id="717">717</th><td>    <b>return</b> errors::InvalidArgument(</td></tr>
<tr><th id="718">718</th><td>        <q>"It's invalid to set per_process_gpu_memory_fraction when "</q></td></tr>
<tr><th id="719">719</th><td>        <q>"virtual_devices is set."</q>);</td></tr>
<tr><th id="720">720</th><td>  }</td></tr>
<tr><th id="721">721</th><td>  <b>if</b> (num_gpus_to_use &lt; virtual_devices.size()) {</td></tr>
<tr><th id="722">722</th><td>    <b>return</b> errors::Unknown(</td></tr>
<tr><th id="723">723</th><td>        <q>"Not enough GPUs to create virtual devices."</q></td></tr>
<tr><th id="724">724</th><td>        <q>" num_gpus_to_use: "</q>,</td></tr>
<tr><th id="725">725</th><td>        num_gpus_to_use, <q>" #virtual_devices: "</q>, virtual_devices.size());</td></tr>
<tr><th id="726">726</th><td>  }</td></tr>
<tr><th id="727">727</th><td>  <b>if</b> (!gpu_options.visible_device_list().empty() &amp;&amp;</td></tr>
<tr><th id="728">728</th><td>      visible_gpu_order.size() != virtual_devices.size()) {</td></tr>
<tr><th id="729">729</th><td>    <b>return</b> errors::InvalidArgument(</td></tr>
<tr><th id="730">730</th><td>        <q>"The number of GPUs in visible_device_list doesn't match the number "</q></td></tr>
<tr><th id="731">731</th><td>        <q>"of elements in the virtual_devices list."</q>,</td></tr>
<tr><th id="732">732</th><td>        <q>" #GPUs in visible_device_list: "</q>, visible_gpu_order.size(),</td></tr>
<tr><th id="733">733</th><td>        <q>" virtual_devices.size(): "</q>, virtual_devices.size());</td></tr>
<tr><th id="734">734</th><td>  }</td></tr>
<tr><th id="735">735</th><td>  <b>if</b> (valid_cuda_gpu_ids.size() != virtual_devices.size()) {</td></tr>
<tr><th id="736">736</th><td>    <b>return</b> errors::Unknown(</td></tr>
<tr><th id="737">737</th><td>        <q>"The number of valid GPUs doesn't match the number of elements in "</q></td></tr>
<tr><th id="738">738</th><td>        <q>"the virtual_devices list."</q>,</td></tr>
<tr><th id="739">739</th><td>        <q>" #valid GPUs: "</q>, valid_cuda_gpu_ids.size(),</td></tr>
<tr><th id="740">740</th><td>        <q>" virtual_devices.size(): "</q>, virtual_devices.size());</td></tr>
<tr><th id="741">741</th><td>  }</td></tr>
<tr><th id="742">742</th><td>  <b>return</b> Status::OK();</td></tr>
<tr><th id="743">743</th><td>}</td></tr>
<tr><th id="744">744</th><td></td></tr>
<tr><th id="745">745</th><td>int64 MinSystemMemory(int64 available_memory) {</td></tr>
<tr><th id="746">746</th><td>  <i>// We use the following heuristic for now:</i></td></tr>
<tr><th id="747">747</th><td><i>  //</i></td></tr>
<tr><th id="748">748</th><td><i>  // If the available_memory is &lt; 2GiB, we allocate 225MiB to system memory.</i></td></tr>
<tr><th id="749">749</th><td><i>  // Otherwise, allocate max(300MiB, 0.05 * available_memory) to system memory.</i></td></tr>
<tr><th id="750">750</th><td><i>  //</i></td></tr>
<tr><th id="751">751</th><td><i>  // In the future we could be more sophisticated by using a table of devices.</i></td></tr>
<tr><th id="752">752</th><td>  int64 min_system_memory;</td></tr>
<tr><th id="753">753</th><td>  <b>if</b> (available_memory &lt; (<var>1LL</var> &lt;&lt; <var>31</var>)) {</td></tr>
<tr><th id="754">754</th><td>    <i>// 225MiB</i></td></tr>
<tr><th id="755">755</th><td>    min_system_memory = <var>225</var> * <var>1024</var> * <var>1024</var>;</td></tr>
<tr><th id="756">756</th><td>  } <b>else</b> {</td></tr>
<tr><th id="757">757</th><td>    <i>// max(300 MiB, 0.05 * available_memory)</i></td></tr>
<tr><th id="758">758</th><td>    min_system_memory =</td></tr>
<tr><th id="759">759</th><td>        std::max(<var>314572800LL</var>, <b>static_cast</b>&lt;int64&gt;(available_memory * <var>0.05</var>));</td></tr>
<tr><th id="760">760</th><td>  }</td></tr>
<tr><th id="761">761</th><td><u>#if defined(__GNUC__) &amp;&amp; defined(__OPTIMIZE__)</u></td></tr>
<tr><th id="762">762</th><td><i>// Do nothing</i></td></tr>
<tr><th id="763">763</th><td><u>#elif !defined(__GNUC__) &amp;&amp; defined(NDEBUG)</u></td></tr>
<tr><th id="764">764</th><td><i>// Do nothing</i></td></tr>
<tr><th id="765">765</th><td><u>#else</u></td></tr>
<tr><th id="766">766</th><td>  <i>// Double the amount of available GPU memory in non-opt builds (debug</i></td></tr>
<tr><th id="767">767</th><td><i>  // builds in windows); because in non-opt builds more system memory</i></td></tr>
<tr><th id="768">768</th><td><i>  // is necessary.</i></td></tr>
<tr><th id="769">769</th><td>  min_system_memory *= <var>2</var>;</td></tr>
<tr><th id="770">770</th><td><u>#endif</u></td></tr>
<tr><th id="771">771</th><td></td></tr>
<tr><th id="772">772</th><td><u>#if defined(ANDROID_TEGRA)</u></td></tr>
<tr><th id="773">773</th><td>  <i>// 1GB system mem for NVIDIA Tegra devices since they use the same mem for RAM</i></td></tr>
<tr><th id="774">774</th><td><i>  // and Video RAM</i></td></tr>
<tr><th id="775">775</th><td>  min_system_memory = <var>1</var> &lt;&lt; <var>30</var>;</td></tr>
<tr><th id="776">776</th><td><u>#endif</u></td></tr>
<tr><th id="777">777</th><td>  <b>return</b> min_system_memory;</td></tr>
<tr><th id="778">778</th><td>}</td></tr>
<tr><th id="779">779</th><td></td></tr>
<tr><th id="780">780</th><td><i>// Get the memory limit for the virtual device being created on GPU with</i></td></tr>
<tr><th id="781">781</th><td><i>// 'cuda_gpu_id', when that virtual device is the only virtual device being</i></td></tr>
<tr><th id="782">782</th><td><i>// created on that GPU.</i></td></tr>
<tr><th id="783">783</th><td>Status SingleVirtualDeviceMemoryLimit(<em>const</em> GPUOptions&amp; gpu_options,</td></tr>
<tr><th id="784">784</th><td>                                      CudaGpuId cuda_gpu_id,</td></tr>
<tr><th id="785">785</th><td>                                      int64* memory_limit) {</td></tr>
<tr><th id="786">786</th><td>  int64 total_memory = <var>0</var>;</td></tr>
<tr><th id="787">787</th><td>  int64 available_memory = <var>0</var>;</td></tr>
<tr><th id="788">788</th><td>  gpu::StreamExecutor* se =</td></tr>
<tr><th id="789">789</th><td>      GpuIdUtil::ExecutorForCudaGpuId(cuda_gpu_id).ValueOrDie();</td></tr>
<tr><th id="790">790</th><td>  <b>if</b> (!se-&gt;DeviceMemoryUsage(&amp;available_memory, &amp;total_memory)) {</td></tr>
<tr><th id="791">791</th><td>    <b>return</b> errors::Unknown(<q>"Failed to query available memory for GPU "</q>,</td></tr>
<tr><th id="792">792</th><td>                           cuda_gpu_id.value());</td></tr>
<tr><th id="793">793</th><td>  }</td></tr>
<tr><th id="794">794</th><td></td></tr>
<tr><th id="795">795</th><td>  int64 allocated_memory = <var>0</var>;</td></tr>
<tr><th id="796">796</th><td>  <em>const</em> <em>double</em> per_process_gpu_memory_fraction =</td></tr>
<tr><th id="797">797</th><td>      gpu_options.per_process_gpu_memory_fraction();</td></tr>
<tr><th id="798">798</th><td>  <b>if</b> (per_process_gpu_memory_fraction == <var>0</var>) {</td></tr>
<tr><th id="799">799</th><td>    allocated_memory = available_memory;</td></tr>
<tr><th id="800">800</th><td>    <em>const</em> int64 min_system_memory = MinSystemMemory(available_memory);</td></tr>
<tr><th id="801">801</th><td>    <b>if</b> (min_system_memory &lt; allocated_memory) {</td></tr>
<tr><th id="802">802</th><td>      allocated_memory -= min_system_memory;</td></tr>
<tr><th id="803">803</th><td>    }</td></tr>
<tr><th id="804">804</th><td>  } <b>else</b> {</td></tr>
<tr><th id="805">805</th><td>    allocated_memory = total_memory * per_process_gpu_memory_fraction;</td></tr>
<tr><th id="806">806</th><td>  }</td></tr>
<tr><th id="807">807</th><td>  *memory_limit = allocated_memory;</td></tr>
<tr><th id="808">808</th><td>  <b>return</b> Status::OK();</td></tr>
<tr><th id="809">809</th><td>}</td></tr>
<tr><th id="810">810</th><td>}  <i>// namespace</i></td></tr>
<tr><th id="811">811</th><td></td></tr>
<tr><th id="812">812</th><td><em>void</em> BaseGPUDevice::ReinitializeDevice(OpKernelContext* context,</td></tr>
<tr><th id="813">813</th><td>                                       PerOpGpuDevice* device, <em>int</em> stream_id,</td></tr>
<tr><th id="814">814</th><td>                                       Allocator* allocator) {</td></tr>
<tr><th id="815">815</th><td>  ConcretePerOpGpuDevice* concrete_device =</td></tr>
<tr><th id="816">816</th><td>      <b>static_cast</b>&lt;ConcretePerOpGpuDevice*&gt;(device);</td></tr>
<tr><th id="817">817</th><td>  DCHECK(concrete_device);</td></tr>
<tr><th id="818">818</th><td>  <em>const</em> cudaStream_t* cuda_stream = <b>reinterpret_cast</b>&lt;<em>const</em> cudaStream_t*&gt;(</td></tr>
<tr><th id="819">819</th><td>      streams_[stream_id]-&gt;compute-&gt;implementation()-&gt;CudaStreamMemberHack());</td></tr>
<tr><th id="820">820</th><td>  concrete_device-&gt;Reinitialize(context, cuda_stream, tf_gpu_id_, allocator,</td></tr>
<tr><th id="821">821</th><td>                                scratch_[stream_id]);</td></tr>
<tr><th id="822">822</th><td>}</td></tr>
<tr><th id="823">823</th><td></td></tr>
<tr><th id="824">824</th><td>PerOpGpuDevice* BaseGPUDevice::MakeGpuDevice() {</td></tr>
<tr><th id="825">825</th><td>  <b>return</b> <b>new</b> ConcretePerOpGpuDevice();</td></tr>
<tr><th id="826">826</th><td>}</td></tr>
<tr><th id="827">827</th><td></td></tr>
<tr><th id="828">828</th><td><em>void</em> BaseGPUDevice::ReinitializeGpuDevice(OpKernelContext* context,</td></tr>
<tr><th id="829">829</th><td>                                          PerOpGpuDevice* device,</td></tr>
<tr><th id="830">830</th><td>                                          DeviceContext* dc,</td></tr>
<tr><th id="831">831</th><td>                                          Allocator* allocator) {</td></tr>
<tr><th id="832">832</th><td>  <b>if</b> (dc) {</td></tr>
<tr><th id="833">833</th><td>    <em>const</em> GPUDeviceContext* gpu_dc = <b>static_cast</b>&lt;GPUDeviceContext*&gt;(dc);</td></tr>
<tr><th id="834">834</th><td>    <em>const</em> <em>int</em> stream_id = gpu_dc-&gt;stream_id();</td></tr>
<tr><th id="835">835</th><td>    VLOG(<var>1</var>) &lt;&lt; <q>"  eigen_gpu_device("</q> &lt;&lt; dc &lt;&lt; <q>") =&gt; stream["</q> &lt;&lt; stream_id</td></tr>
<tr><th id="836">836</th><td>            &lt;&lt; <q>"]"</q>;</td></tr>
<tr><th id="837">837</th><td>    CHECK_LT(stream_id, streams_.size());</td></tr>
<tr><th id="838">838</th><td>    ReinitializeDevice(context, device, stream_id, allocator);</td></tr>
<tr><th id="839">839</th><td>  } <b>else</b> {</td></tr>
<tr><th id="840">840</th><td>    ReinitializeDevice(context, device, <var>0</var>, allocator);</td></tr>
<tr><th id="841">841</th><td>  }</td></tr>
<tr><th id="842">842</th><td>}</td></tr>
<tr><th id="843">843</th><td></td></tr>
<tr><th id="844">844</th><td>Allocator* BaseGPUDevice::GetScopedAllocator(AllocatorAttributes attr,</td></tr>
<tr><th id="845">845</th><td>                                             int64 step_id) {</td></tr>
<tr><th id="846">846</th><td>  <b>if</b> (attr.scope_id &gt; <var>0</var>) {</td></tr>
<tr><th id="847">847</th><td>    <b>return</b> scoped_allocator_mgr_-&gt;GetContainer(step_id)-&gt;GetInstance(</td></tr>
<tr><th id="848">848</th><td>        attr.scope_id);</td></tr>
<tr><th id="849">849</th><td>  }</td></tr>
<tr><th id="850">850</th><td>  LOG(FATAL) &lt;&lt; <q>"Unexpected call to BaseGPUDevice::GetScopedAllocator "</q></td></tr>
<tr><th id="851">851</th><td>             &lt;&lt; <q>"attr.scope_id = "</q> &lt;&lt; attr.scope_id;</td></tr>
<tr><th id="852">852</th><td>  <b>return</b> gpu_allocator_;</td></tr>
<tr><th id="853">853</th><td>}</td></tr>
<tr><th id="854">854</th><td></td></tr>
<tr><th id="855">855</th><td><em>const</em> <em>int</em> BaseGPUDeviceFactory::InterconnectMap::kSameDeviceStrength = <var>1000</var>;</td></tr>
<tr><th id="856">856</th><td><em>const</em> <em>int</em> BaseGPUDeviceFactory::InterconnectMap::kStreamExecutorStrength = <var>1</var>;</td></tr>
<tr><th id="857">857</th><td></td></tr>
<tr><th id="858">858</th><td>Status BaseGPUDeviceFactory::CreateDevices(<em>const</em> SessionOptions&amp; options,</td></tr>
<tr><th id="859">859</th><td>                                           <em>const</em> string&amp; name_prefix,</td></tr>
<tr><th id="860">860</th><td>                                           std::vector&lt;Device*&gt;* devices) {</td></tr>
<tr><th id="861">861</th><td>  TF_RETURN_IF_ERROR(ValidateGPUMachineManager());</td></tr>
<tr><th id="862">862</th><td>  gpu::Platform* gpu_manager = GPUMachineManager();</td></tr>
<tr><th id="863">863</th><td>  <b>if</b> (gpu_manager == <b>nullptr</b>) {</td></tr>
<tr><th id="864">864</th><td>    <b>return</b> Status::OK();</td></tr>
<tr><th id="865">865</th><td>  }</td></tr>
<tr><th id="866">866</th><td>  <i>// If there are no GPUs visible, do nothing.</i></td></tr>
<tr><th id="867">867</th><td>  <b>if</b> (gpu_manager-&gt;VisibleDeviceCount() &lt;= <var>0</var>) {</td></tr>
<tr><th id="868">868</th><td>    <b>return</b> Status::OK();</td></tr>
<tr><th id="869">869</th><td>  }</td></tr>
<tr><th id="870">870</th><td></td></tr>
<tr><th id="871">871</th><td>  size_t num_gpus_to_use = INT_MAX;</td></tr>
<tr><th id="872">872</th><td>  <em>auto</em> iter = options.config.device_count().find(<q>"GPU"</q>);</td></tr>
<tr><th id="873">873</th><td>  <b>if</b> (iter != options.config.device_count().end()) {</td></tr>
<tr><th id="874">874</th><td>    num_gpus_to_use = iter-&gt;second;</td></tr>
<tr><th id="875">875</th><td>  }</td></tr>
<tr><th id="876">876</th><td>  <em>const</em> <em>auto</em>&amp; gpu_options = options.config.gpu_options();</td></tr>
<tr><th id="877">877</th><td>  std::vector&lt;CudaGpuId&gt; visible_gpu_order;</td></tr>
<tr><th id="878">878</th><td>  TF_RETURN_IF_ERROR(ParseVisibleDeviceList(gpu_options.visible_device_list(),</td></tr>
<tr><th id="879">879</th><td>                                            &amp;visible_gpu_order));</td></tr>
<tr><th id="880">880</th><td></td></tr>
<tr><th id="881">881</th><td>  std::vector&lt;CudaGpuId&gt; valid_cuda_gpu_ids;</td></tr>
<tr><th id="882">882</th><td>  TF_RETURN_IF_ERROR(GetValidDeviceIds(visible_gpu_order, &amp;valid_cuda_gpu_ids));</td></tr>
<tr><th id="883">883</th><td>  <b>if</b> (num_gpus_to_use &gt; valid_cuda_gpu_ids.size()) {</td></tr>
<tr><th id="884">884</th><td>    num_gpus_to_use = valid_cuda_gpu_ids.size();</td></tr>
<tr><th id="885">885</th><td>  }</td></tr>
<tr><th id="886">886</th><td>  <b>if</b> (!valid_cuda_gpu_ids.empty()) {</td></tr>
<tr><th id="887">887</th><td>    <i>// Save the original device.</i></td></tr>
<tr><th id="888">888</th><td>    <em>int</em> original_device = <var>0</var>;</td></tr>
<tr><th id="889">889</th><td>    cudaError_t err = cudaGetDevice(&amp;original_device);</td></tr>
<tr><th id="890">890</th><td>    <b>if</b> (err != cudaSuccess) {</td></tr>
<tr><th id="891">891</th><td>      <b>return</b> errors::Internal(<q>"cudaGetDevice() failed. Status: "</q>,</td></tr>
<tr><th id="892">892</th><td>                              cudaGetErrorString(err));</td></tr>
<tr><th id="893">893</th><td>    }</td></tr>
<tr><th id="894">894</th><td>    <i>// Force to implicitly initialize CUDA runtime on each valid GPU before</i></td></tr>
<tr><th id="895">895</th><td><i>    // CreateGPUDevice().</i></td></tr>
<tr><th id="896">896</th><td>    <b>for</b> (CudaGpuId cuda_gpu_id : valid_cuda_gpu_ids) {</td></tr>
<tr><th id="897">897</th><td>      err = cudaSetDevice(cuda_gpu_id.value());</td></tr>
<tr><th id="898">898</th><td>      <b>if</b> (err != cudaSuccess) {</td></tr>
<tr><th id="899">899</th><td>        <b>return</b> errors::Internal(<q>"cudaSetDevice() on GPU:"</q>, cuda_gpu_id.value(),</td></tr>
<tr><th id="900">900</th><td>                                <q>" failed. Status: "</q>, cudaGetErrorString(err));</td></tr>
<tr><th id="901">901</th><td>      }</td></tr>
<tr><th id="902">902</th><td>      err = cudaFree(<b>nullptr</b>);</td></tr>
<tr><th id="903">903</th><td>      <b>if</b> (err != cudaSuccess) {</td></tr>
<tr><th id="904">904</th><td>        <b>return</b> errors::Internal(</td></tr>
<tr><th id="905">905</th><td>            <q>"CUDA runtime implicit initialization on GPU:"</q>, cuda_gpu_id.value(),</td></tr>
<tr><th id="906">906</th><td>            <q>" failed. Status: "</q>, cudaGetErrorString(err));</td></tr>
<tr><th id="907">907</th><td>      }</td></tr>
<tr><th id="908">908</th><td>    }</td></tr>
<tr><th id="909">909</th><td>    <i>// Reset to the original device.</i></td></tr>
<tr><th id="910">910</th><td>    err = cudaSetDevice(original_device);</td></tr>
<tr><th id="911">911</th><td>    <b>if</b> (err != cudaSuccess) {</td></tr>
<tr><th id="912">912</th><td>      <b>return</b> errors::Internal(<q>"cudaSetDevice() on GPU:"</q>, original_device,</td></tr>
<tr><th id="913">913</th><td>                              <q>" failed. Status: "</q>, cudaGetErrorString(err));</td></tr>
<tr><th id="914">914</th><td>    }</td></tr>
<tr><th id="915">915</th><td>  }</td></tr>
<tr><th id="916">916</th><td></td></tr>
<tr><th id="917">917</th><td>  std::vector&lt;InterconnectMap&gt; interconnect_maps;</td></tr>
<tr><th id="918">918</th><td>  TF_RETURN_IF_ERROR(</td></tr>
<tr><th id="919">919</th><td>      GetInterconnectMaps(visible_gpu_order, gpu_manager, &amp;interconnect_maps));</td></tr>
<tr><th id="920">920</th><td></td></tr>
<tr><th id="921">921</th><td>  <i>// Print each interconnect map to the log.</i></td></tr>
<tr><th id="922">922</th><td>  <b>for</b> (<em>const</em> InterconnectMap&amp; im : interconnect_maps) {</td></tr>
<tr><th id="923">923</th><td>    LOG(INFO) &lt;&lt; <q>"Device interconnect "</q> &lt;&lt; im.name &lt;&lt; <q>" with strength "</q></td></tr>
<tr><th id="924">924</th><td>              &lt;&lt; im.strength &lt;&lt; <q>" edge matrix:"</q>;</td></tr>
<tr><th id="925">925</th><td>    string line_buf = <q>"     "</q>;</td></tr>
<tr><th id="926">926</th><td>    <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; visible_gpu_order.size(); ++i) {</td></tr>
<tr><th id="927">927</th><td>      strings::StrAppend(&amp;line_buf, visible_gpu_order[i].value(), <q>" "</q>);</td></tr>
<tr><th id="928">928</th><td>    }</td></tr>
<tr><th id="929">929</th><td>    LOG(INFO) &lt;&lt; line_buf;</td></tr>
<tr><th id="930">930</th><td>    <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; visible_gpu_order.size(); ++i) {</td></tr>
<tr><th id="931">931</th><td>      line_buf = strings::StrCat(visible_gpu_order[i].value(), <q>":   "</q>);</td></tr>
<tr><th id="932">932</th><td>      CudaGpuId cuda_id_i = visible_gpu_order[i];</td></tr>
<tr><th id="933">933</th><td>      <b>for</b> (<em>int</em> j = <var>0</var>; j &lt; visible_gpu_order.size(); ++j) {</td></tr>
<tr><th id="934">934</th><td>        CudaGpuId cuda_id_j = visible_gpu_order[j];</td></tr>
<tr><th id="935">935</th><td>        <b>if</b> (im.directed_links.find({cuda_id_i, cuda_id_j}) !=</td></tr>
<tr><th id="936">936</th><td>            im.directed_links.end()) {</td></tr>
<tr><th id="937">937</th><td>          line_buf.append(<q>"Y "</q>);</td></tr>
<tr><th id="938">938</th><td>        } <b>else</b> {</td></tr>
<tr><th id="939">939</th><td>          line_buf.append(<q>"N "</q>);</td></tr>
<tr><th id="940">940</th><td>        }</td></tr>
<tr><th id="941">941</th><td>      }</td></tr>
<tr><th id="942">942</th><td>      LOG(INFO) &lt;&lt; line_buf;</td></tr>
<tr><th id="943">943</th><td>    }</td></tr>
<tr><th id="944">944</th><td>  }</td></tr>
<tr><th id="945">945</th><td></td></tr>
<tr><th id="946">946</th><td>  <em>const</em> <em>auto</em>&amp; virtual_devices = gpu_options.experimental().virtual_devices();</td></tr>
<tr><th id="947">947</th><td>  <b>if</b> (!virtual_devices.empty()) {</td></tr>
<tr><th id="948">948</th><td>    TF_RETURN_IF_ERROR(VerifyVirtualDeviceSettings(</td></tr>
<tr><th id="949">949</th><td>        num_gpus_to_use, gpu_options, visible_gpu_order, valid_cuda_gpu_ids));</td></tr>
<tr><th id="950">950</th><td>    <i>// We've verified that num_gpus_to_use &gt;= virtual_devices.size().</i></td></tr>
<tr><th id="951">951</th><td>    num_gpus_to_use = virtual_devices.size();</td></tr>
<tr><th id="952">952</th><td>    CHECK(gpu_options.visible_device_list().empty() ||</td></tr>
<tr><th id="953">953</th><td>          valid_cuda_gpu_ids == visible_gpu_order);</td></tr>
<tr><th id="954">954</th><td>  }</td></tr>
<tr><th id="955">955</th><td>  <em>int</em> next_tf_gpu_id = <var>0</var>;</td></tr>
<tr><th id="956">956</th><td>  std::vector&lt;int64&gt; memory_limit_bytes;</td></tr>
<tr><th id="957">957</th><td>  <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; num_gpus_to_use; ++i) {</td></tr>
<tr><th id="958">958</th><td>    <em>const</em> CudaGpuId cuda_gpu_id = valid_cuda_gpu_ids[i];</td></tr>
<tr><th id="959">959</th><td>    <b>if</b> (virtual_devices.empty() ||</td></tr>
<tr><th id="960">960</th><td>        virtual_devices.Get(i).memory_limit_mb_size() == <var>0</var>) {</td></tr>
<tr><th id="961">961</th><td>      int64 single_virtual_device_memory_limit = <var>0</var>;</td></tr>
<tr><th id="962">962</th><td>      TF_RETURN_IF_ERROR(SingleVirtualDeviceMemoryLimit(</td></tr>
<tr><th id="963">963</th><td>          gpu_options, cuda_gpu_id, &amp;single_virtual_device_memory_limit));</td></tr>
<tr><th id="964">964</th><td>      memory_limit_bytes.push_back(single_virtual_device_memory_limit);</td></tr>
<tr><th id="965">965</th><td>    } <b>else</b> {</td></tr>
<tr><th id="966">966</th><td>      <em>const</em> <em>auto</em>&amp; memory_limit_mb = virtual_devices.Get(i).memory_limit_mb();</td></tr>
<tr><th id="967">967</th><td>      std::transform(memory_limit_mb.begin(), memory_limit_mb.end(),</td></tr>
<tr><th id="968">968</th><td>                     std::back_inserter(memory_limit_bytes), [](<em>float</em> mb) {</td></tr>
<tr><th id="969">969</th><td>                       <b>return</b> <b>static_cast</b>&lt;int64&gt;(mb) * (<var>1ll</var> &lt;&lt; <var>20</var>);</td></tr>
<tr><th id="970">970</th><td>                     });</td></tr>
<tr><th id="971">971</th><td>    }</td></tr>
<tr><th id="972">972</th><td>    <b>while</b> (next_tf_gpu_id &lt; memory_limit_bytes.size()) {</td></tr>
<tr><th id="973">973</th><td>      TfGpuId tf_gpu_id(next_tf_gpu_id);</td></tr>
<tr><th id="974">974</th><td>      ++next_tf_gpu_id;</td></tr>
<tr><th id="975">975</th><td>      GpuIdManager::InsertTfCudaGpuIdPair(tf_gpu_id, cuda_gpu_id);</td></tr>
<tr><th id="976">976</th><td>    }</td></tr>
<tr><th id="977">977</th><td>  }</td></tr>
<tr><th id="978">978</th><td>  <em>const</em> <em>int</em> num_tf_gpus = next_tf_gpu_id;</td></tr>
<tr><th id="979">979</th><td></td></tr>
<tr><th id="980">980</th><td>  LocalityMap device_localities;</td></tr>
<tr><th id="981">981</th><td>  TF_RETURN_IF_ERROR(</td></tr>
<tr><th id="982">982</th><td>      GetDeviceLocalities(num_tf_gpus, interconnect_maps, &amp;device_localities));</td></tr>
<tr><th id="983">983</th><td></td></tr>
<tr><th id="984">984</th><td>  <i>// Build the GPUDevices</i></td></tr>
<tr><th id="985">985</th><td>  CHECK_EQ(next_tf_gpu_id, memory_limit_bytes.size());</td></tr>
<tr><th id="986">986</th><td>  <b>for</b> (<em>int</em> di = <var>0</var>; di &lt; num_tf_gpus; ++di) {</td></tr>
<tr><th id="987">987</th><td>    TfGpuId tf_gpu_id(di);</td></tr>
<tr><th id="988">988</th><td>    int64 bytes = memory_limit_bytes[di];</td></tr>
<tr><th id="989">989</th><td>    <em>auto</em> it = device_localities.find(tf_gpu_id);</td></tr>
<tr><th id="990">990</th><td>    <b>if</b> (it == device_localities.end()) {</td></tr>
<tr><th id="991">991</th><td>      <b>return</b> errors::Internal(<q>"Failed to find DeviceLocality for GPU device "</q>,</td></tr>
<tr><th id="992">992</th><td>                              tf_gpu_id.value());</td></tr>
<tr><th id="993">993</th><td>    }</td></tr>
<tr><th id="994">994</th><td>    TF_RETURN_IF_ERROR(CreateGPUDevice(options, name_prefix, tf_gpu_id, bytes,</td></tr>
<tr><th id="995">995</th><td>                                       it-&gt;second, devices));</td></tr>
<tr><th id="996">996</th><td>  }</td></tr>
<tr><th id="997">997</th><td>  <b>return</b> Status::OK();</td></tr>
<tr><th id="998">998</th><td>}</td></tr>
<tr><th id="999">999</th><td></td></tr>
<tr><th id="1000">1000</th><td><em>static</em> string GetShortDeviceDescription(CudaGpuId cuda_gpu_id,</td></tr>
<tr><th id="1001">1001</th><td>                                        <em>const</em> gpu::DeviceDescription&amp; desc) {</td></tr>
<tr><th id="1002">1002</th><td>  <em>int</em> cc_major;</td></tr>
<tr><th id="1003">1003</th><td>  <em>int</em> cc_minor;</td></tr>
<tr><th id="1004">1004</th><td>  <b>if</b> (!desc.cuda_compute_capability(&amp;cc_major, &amp;cc_minor)) {</td></tr>
<tr><th id="1005">1005</th><td>    cc_major = <var>0</var>;</td></tr>
<tr><th id="1006">1006</th><td>    cc_minor = <var>0</var>;</td></tr>
<tr><th id="1007">1007</th><td>  }</td></tr>
<tr><th id="1008">1008</th><td>  <i>// LINT.IfChange</i></td></tr>
<tr><th id="1009">1009</th><td>  <b>return</b> strings::StrCat(<q>"device: "</q>, cuda_gpu_id.value(),</td></tr>
<tr><th id="1010">1010</th><td>                         <q>", name: "</q>, desc.name(),</td></tr>
<tr><th id="1011">1011</th><td>                         <q>", pci bus id: "</q>, desc.pci_bus_id(),</td></tr>
<tr><th id="1012">1012</th><td>                         <q>", compute capability: "</q>, cc_major, <q>"."</q>, cc_minor);</td></tr>
<tr><th id="1013">1013</th><td>  <i>// LINT.ThenChange(//tensorflow/python/platform/test.py)</i></td></tr>
<tr><th id="1014">1014</th><td>}</td></tr>
<tr><th id="1015">1015</th><td></td></tr>
<tr><th id="1016">1016</th><td>Status BaseGPUDeviceFactory::CreateGPUDevice(<em>const</em> SessionOptions&amp; options,</td></tr>
<tr><th id="1017">1017</th><td>                                             <em>const</em> string&amp; name_prefix,</td></tr>
<tr><th id="1018">1018</th><td>                                             TfGpuId tf_gpu_id,</td></tr>
<tr><th id="1019">1019</th><td>                                             int64 memory_limit,</td></tr>
<tr><th id="1020">1020</th><td>                                             <em>const</em> DeviceLocality&amp; dev_locality,</td></tr>
<tr><th id="1021">1021</th><td>                                             std::vector&lt;Device*&gt;* devices) {</td></tr>
<tr><th id="1022">1022</th><td>  CHECK_GE(tf_gpu_id.value(), <var>0</var>);</td></tr>
<tr><th id="1023">1023</th><td>  <em>const</em> string device_name =</td></tr>
<tr><th id="1024">1024</th><td>      strings::StrCat(name_prefix, <q>"/device:GPU:"</q>, tf_gpu_id.value());</td></tr>
<tr><th id="1025">1025</th><td>  GpuIdUtil::CheckValidTfGpuId(tf_gpu_id);</td></tr>
<tr><th id="1026">1026</th><td>  CudaGpuId cuda_gpu_id = GpuIdManager::TfToCudaGpuId(tf_gpu_id);</td></tr>
<tr><th id="1027">1027</th><td>  <em>int</em> numa_node = dev_locality.numa_node();</td></tr>
<tr><th id="1028">1028</th><td></td></tr>
<tr><th id="1029">1029</th><td>  gpu::StreamExecutor* se =</td></tr>
<tr><th id="1030">1030</th><td>      GpuIdUtil::ExecutorForCudaGpuId(cuda_gpu_id).ValueOrDie();</td></tr>
<tr><th id="1031">1031</th><td>  <em>const</em> gpu::DeviceDescription&amp; desc = se-&gt;GetDeviceDescription();</td></tr>
<tr><th id="1032">1032</th><td>  ProcessState* process_state = ProcessState::singleton();</td></tr>
<tr><th id="1033">1033</th><td>  Allocator* gpu_allocator = process_state-&gt;GetGPUAllocator(</td></tr>
<tr><th id="1034">1034</th><td>      options.config.gpu_options(), tf_gpu_id, memory_limit);</td></tr>
<tr><th id="1035">1035</th><td>  <b>if</b> (gpu_allocator == <b>nullptr</b>) {</td></tr>
<tr><th id="1036">1036</th><td>    <b>return</b> errors::Internal(<q>"Failed to get memory allocator for TF GPU "</q>,</td></tr>
<tr><th id="1037">1037</th><td>                            tf_gpu_id.value(), <q>" with "</q>, memory_limit,</td></tr>
<tr><th id="1038">1038</th><td>                            <q>" bytes of memory."</q>);</td></tr>
<tr><th id="1039">1039</th><td>  }</td></tr>
<tr><th id="1040">1040</th><td>  AllocatorStats stats;</td></tr>
<tr><th id="1041">1041</th><td>  gpu_allocator-&gt;GetStats(&amp;stats);</td></tr>
<tr><th id="1042">1042</th><td>  <i>// 'memory_limit' is the required memory size, but if the allocator with given</i></td></tr>
<tr><th id="1043">1043</th><td><i>  // tf_gpu_id was created before, we'll use it instead of creating a new one</i></td></tr>
<tr><th id="1044">1044</th><td><i>  // (as TF gpu device is a shared resource), in which case the actual memory</i></td></tr>
<tr><th id="1045">1045</th><td><i>  // limit represented by 'stats.bytes_limit' used by that allocator may be</i></td></tr>
<tr><th id="1046">1046</th><td><i>  // different (which should be an error).</i></td></tr>
<tr><th id="1047">1047</th><td><i>  //</i></td></tr>
<tr><th id="1048">1048</th><td><i>  // TODO(laigd): report error if memory_limit doesn't match stats.bytes_limit.</i></td></tr>
<tr><th id="1049">1049</th><td>  BaseGPUDevice* gpu_device = CreateGPUDevice(</td></tr>
<tr><th id="1050">1050</th><td>      options, device_name, <b>static_cast</b>&lt;Bytes&gt;(stats.bytes_limit), dev_locality,</td></tr>
<tr><th id="1051">1051</th><td>      tf_gpu_id, GetShortDeviceDescription(cuda_gpu_id, desc), gpu_allocator,</td></tr>
<tr><th id="1052">1052</th><td>      process_state-&gt;GetCPUAllocator(numa_node));</td></tr>
<tr><th id="1053">1053</th><td>  LOG(INFO) &lt;&lt; <q>"Created TensorFlow device ("</q> &lt;&lt; device_name &lt;&lt; <q>" with "</q></td></tr>
<tr><th id="1054">1054</th><td>            &lt;&lt; (stats.bytes_limit &gt;&gt; <var>20</var>) &lt;&lt; <q>" MB memory) -&gt; physical GPU ("</q></td></tr>
<tr><th id="1055">1055</th><td>            &lt;&lt; GetShortDeviceDescription(cuda_gpu_id, desc) &lt;&lt; <q>")"</q>;</td></tr>
<tr><th id="1056">1056</th><td>  TF_RETURN_IF_ERROR(gpu_device-&gt;Init(options));</td></tr>
<tr><th id="1057">1057</th><td>  devices-&gt;push_back(gpu_device);</td></tr>
<tr><th id="1058">1058</th><td></td></tr>
<tr><th id="1059">1059</th><td>  <b>return</b> Status::OK();</td></tr>
<tr><th id="1060">1060</th><td>}</td></tr>
<tr><th id="1061">1061</th><td></td></tr>
<tr><th id="1062">1062</th><td><b>namespace</b> {</td></tr>
<tr><th id="1063">1063</th><td>std::unique_ptr&lt;std::map&lt;std::pair&lt;CudaGpuId, CudaGpuId&gt;, <em>bool</em>&gt;&gt;</td></tr>
<tr><th id="1064">1064</th><td>GetPeerAccessMap(gpu::Platform* platform,</td></tr>
<tr><th id="1065">1065</th><td>                 <em>const</em> std::vector&lt;CudaGpuId&gt;&amp; visible_gpu_order) {</td></tr>
<tr><th id="1066">1066</th><td>  std::unique_ptr&lt;std::map&lt;std::pair&lt;CudaGpuId, CudaGpuId&gt;, <em>bool</em>&gt;&gt; map(</td></tr>
<tr><th id="1067">1067</th><td>      <b>new</b> std::map&lt;std::pair&lt;CudaGpuId, CudaGpuId&gt;, <em>bool</em>&gt;);</td></tr>
<tr><th id="1068">1068</th><td>  <b>for</b> (CudaGpuId cuda_gpu_i : visible_gpu_order) {</td></tr>
<tr><th id="1069">1069</th><td>    <b>for</b> (CudaGpuId cuda_gpu_j : visible_gpu_order) {</td></tr>
<tr><th id="1070">1070</th><td>      gpu::StreamExecutor* from =</td></tr>
<tr><th id="1071">1071</th><td>          GpuIdUtil::ExecutorForCudaGpuId(platform, cuda_gpu_i).ValueOrDie();</td></tr>
<tr><th id="1072">1072</th><td>      gpu::StreamExecutor* to =</td></tr>
<tr><th id="1073">1073</th><td>          GpuIdUtil::ExecutorForCudaGpuId(platform, cuda_gpu_j).ValueOrDie();</td></tr>
<tr><th id="1074">1074</th><td>      (*map)[{cuda_gpu_i, cuda_gpu_j}] = from-&gt;CanEnablePeerAccessTo(to);</td></tr>
<tr><th id="1075">1075</th><td>    }</td></tr>
<tr><th id="1076">1076</th><td>  }</td></tr>
<tr><th id="1077">1077</th><td></td></tr>
<tr><th id="1078">1078</th><td>  <b>return</b> map;</td></tr>
<tr><th id="1079">1079</th><td>}</td></tr>
<tr><th id="1080">1080</th><td></td></tr>
<tr><th id="1081">1081</th><td>}  <i>// namespace</i></td></tr>
<tr><th id="1082">1082</th><td></td></tr>
<tr><th id="1083">1083</th><td>Status BaseGPUDeviceFactory::GetInterconnectMaps(</td></tr>
<tr><th id="1084">1084</th><td>    <em>const</em> std::vector&lt;CudaGpuId&gt;&amp; visible_gpu_order, gpu::Platform* gpu_manager,</td></tr>
<tr><th id="1085">1085</th><td>    std::vector&lt;InterconnectMap&gt;* maps) {</td></tr>
<tr><th id="1086">1086</th><td>  <i>// The default interconnect map is obtained from the StreamExecutor.</i></td></tr>
<tr><th id="1087">1087</th><td>  <em>auto</em> access_map = GetPeerAccessMap(gpu_manager, visible_gpu_order);</td></tr>
<tr><th id="1088">1088</th><td>  maps-&gt;resize(<var>1</var>);</td></tr>
<tr><th id="1089">1089</th><td>  InterconnectMap&amp; imap = maps-&gt;at(<var>0</var>);</td></tr>
<tr><th id="1090">1090</th><td>  imap.name = <q>"StreamExecutor"</q>;</td></tr>
<tr><th id="1091">1091</th><td>  imap.strength = InterconnectMap::kStreamExecutorStrength;</td></tr>
<tr><th id="1092">1092</th><td>  <b>for</b> (CudaGpuId cuda_id_i : visible_gpu_order) {</td></tr>
<tr><th id="1093">1093</th><td>    <b>for</b> (CudaGpuId cuda_id_j : visible_gpu_order) {</td></tr>
<tr><th id="1094">1094</th><td>      <b>if</b> (cuda_id_i == cuda_id_j) <b>continue</b>;</td></tr>
<tr><th id="1095">1095</th><td>      <b>if</b> ((*access_map)[{cuda_id_i, cuda_id_j}]) {</td></tr>
<tr><th id="1096">1096</th><td>        imap.directed_links.insert({cuda_id_i, cuda_id_j});</td></tr>
<tr><th id="1097">1097</th><td>      }</td></tr>
<tr><th id="1098">1098</th><td>    }</td></tr>
<tr><th id="1099">1099</th><td>  }</td></tr>
<tr><th id="1100">1100</th><td>  <b>return</b> Status::OK();</td></tr>
<tr><th id="1101">1101</th><td>}</td></tr>
<tr><th id="1102">1102</th><td></td></tr>
<tr><th id="1103">1103</th><td>Status BaseGPUDeviceFactory::GetDeviceLocalities(</td></tr>
<tr><th id="1104">1104</th><td>    <em>int</em> num_tf_gpus, <em>const</em> std::vector&lt;InterconnectMap&gt;&amp; interconnects,</td></tr>
<tr><th id="1105">1105</th><td>    LocalityMap* localities) {</td></tr>
<tr><th id="1106">1106</th><td>  std::vector&lt;TfGpuId&gt; all_tf_gpu_ids;</td></tr>
<tr><th id="1107">1107</th><td>  <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; num_tf_gpus; ++i) {</td></tr>
<tr><th id="1108">1108</th><td>    all_tf_gpu_ids.push_back(TfGpuId(i));</td></tr>
<tr><th id="1109">1109</th><td>  }</td></tr>
<tr><th id="1110">1110</th><td>  <b>for</b> (TfGpuId tf_gpu_id : all_tf_gpu_ids) {</td></tr>
<tr><th id="1111">1111</th><td>    CudaGpuId cuda_gpu_id = GpuIdManager::TfToCudaGpuId(tf_gpu_id);</td></tr>
<tr><th id="1112">1112</th><td>    <i>// Get GPU bus_id from its reported NUMA affinity.  Because GPUs are</i></td></tr>
<tr><th id="1113">1113</th><td><i>    // virtualized in some environments, we can't just use the GPU id.</i></td></tr>
<tr><th id="1114">1114</th><td><i>    // NUMA locales are indexed from 0, buses are indexed from 1.</i></td></tr>
<tr><th id="1115">1115</th><td>    gpu::StreamExecutor* se =</td></tr>
<tr><th id="1116">1116</th><td>        GpuIdUtil::ExecutorForCudaGpuId(cuda_gpu_id).ValueOrDie();</td></tr>
<tr><th id="1117">1117</th><td>    <em>const</em> gpu::DeviceDescription&amp; desc = se-&gt;GetDeviceDescription();</td></tr>
<tr><th id="1118">1118</th><td>    <em>int</em> numa_node = desc.numa_node();</td></tr>
<tr><th id="1119">1119</th><td>    <b>if</b> (numa_node &lt; <var>0</var>) {</td></tr>
<tr><th id="1120">1120</th><td>      <i>// For some reason the StreamExecutor couldn't get the NUMA</i></td></tr>
<tr><th id="1121">1121</th><td><i>      // affinity of the GPU.  If this is not a multi-socket mobo with</i></td></tr>
<tr><th id="1122">1122</th><td><i>      // GPUs local to different buses, it doesn't matter.  If it is, we</i></td></tr>
<tr><th id="1123">1123</th><td><i>      // may run into trouble later with data transfer operations.  The</i></td></tr>
<tr><th id="1124">1124</th><td><i>      // trouble may manifest as slower than expected performance, or</i></td></tr>
<tr><th id="1125">1125</th><td><i>      // outright failures.</i></td></tr>
<tr><th id="1126">1126</th><td>      LOG(INFO) &lt;&lt; <q>"Could not identify NUMA node of CUDA gpu id "</q> &lt;&lt; cuda_gpu_id</td></tr>
<tr><th id="1127">1127</th><td>                &lt;&lt; <q>", defaulting to 0.  Your kernel may not have been built "</q></td></tr>
<tr><th id="1128">1128</th><td>                &lt;&lt; <q>"with NUMA support."</q>;</td></tr>
<tr><th id="1129">1129</th><td>      numa_node = <var>0</var>;</td></tr>
<tr><th id="1130">1130</th><td>    }</td></tr>
<tr><th id="1131">1131</th><td>    DeviceLocality dev_locality;</td></tr>
<tr><th id="1132">1132</th><td>    dev_locality.set_numa_node(numa_node);</td></tr>
<tr><th id="1133">1133</th><td>    dev_locality.set_bus_id(numa_node + <var>1</var>);</td></tr>
<tr><th id="1134">1134</th><td></td></tr>
<tr><th id="1135">1135</th><td>    <i>// Set LocalLinks from InterconnectMaps.</i></td></tr>
<tr><th id="1136">1136</th><td>    LocalLinks* links = dev_locality.mutable_links();</td></tr>
<tr><th id="1137">1137</th><td>    <b>for</b> (<em>const</em> InterconnectMap&amp; imap : interconnects) {</td></tr>
<tr><th id="1138">1138</th><td>      <b>for</b> (TfGpuId tf_gpu_dst : all_tf_gpu_ids) {</td></tr>
<tr><th id="1139">1139</th><td>        CudaGpuId cuda_gpu_dst = GpuIdManager::TfToCudaGpuId(tf_gpu_dst);</td></tr>
<tr><th id="1140">1140</th><td>        <b>if</b> (imap.directed_links.find({cuda_gpu_id, cuda_gpu_dst}) !=</td></tr>
<tr><th id="1141">1141</th><td>            imap.directed_links.end()) {</td></tr>
<tr><th id="1142">1142</th><td>          InterconnectLink* ilink = links-&gt;add_link();</td></tr>
<tr><th id="1143">1143</th><td>          ilink-&gt;set_device_id(tf_gpu_dst.value());</td></tr>
<tr><th id="1144">1144</th><td>          ilink-&gt;set_type(imap.name);</td></tr>
<tr><th id="1145">1145</th><td>          ilink-&gt;set_strength(imap.strength);</td></tr>
<tr><th id="1146">1146</th><td>        }</td></tr>
<tr><th id="1147">1147</th><td>      }</td></tr>
<tr><th id="1148">1148</th><td>    }</td></tr>
<tr><th id="1149">1149</th><td></td></tr>
<tr><th id="1150">1150</th><td>    <i>// If this is one of multiple virtual GPUs on the same physical GPU</i></td></tr>
<tr><th id="1151">1151</th><td><i>    // add high strength links to the others.</i></td></tr>
<tr><th id="1152">1152</th><td>    <b>for</b> (TfGpuId tf_gpu_dst : all_tf_gpu_ids) {</td></tr>
<tr><th id="1153">1153</th><td>      <b>if</b> (tf_gpu_id == tf_gpu_dst) <b>continue</b>;</td></tr>
<tr><th id="1154">1154</th><td>      CudaGpuId cuda_gpu_dst = GpuIdManager::TfToCudaGpuId(tf_gpu_dst);</td></tr>
<tr><th id="1155">1155</th><td>      <b>if</b> (cuda_gpu_id == cuda_gpu_dst) {</td></tr>
<tr><th id="1156">1156</th><td>        InterconnectLink* ilink = links-&gt;add_link();</td></tr>
<tr><th id="1157">1157</th><td>        ilink-&gt;set_device_id(tf_gpu_dst.value());</td></tr>
<tr><th id="1158">1158</th><td>        ilink-&gt;set_type(<q>"SAME_DEVICE"</q>);</td></tr>
<tr><th id="1159">1159</th><td>        ilink-&gt;set_strength(InterconnectMap::kSameDeviceStrength);</td></tr>
<tr><th id="1160">1160</th><td>      }</td></tr>
<tr><th id="1161">1161</th><td>    }</td></tr>
<tr><th id="1162">1162</th><td></td></tr>
<tr><th id="1163">1163</th><td>    (*localities)[tf_gpu_id] = dev_locality;</td></tr>
<tr><th id="1164">1164</th><td>    VLOG(<var>1</var>) &lt;&lt; <q>"GPUDevice CudaGpuId "</q> &lt;&lt; cuda_gpu_id &lt;&lt; <q>" TfGpuId "</q> &lt;&lt; tf_gpu_id</td></tr>
<tr><th id="1165">1165</th><td>            &lt;&lt; <q>" on bus "</q> &lt;&lt; dev_locality.bus_id() &lt;&lt; <q>" numa: "</q> &lt;&lt; numa_node</td></tr>
<tr><th id="1166">1166</th><td>            &lt;&lt; <q>" pci: "</q> &lt;&lt; desc.pci_bus_id()</td></tr>
<tr><th id="1167">1167</th><td>            &lt;&lt; <q>" DeviceLocality: "</q> &lt;&lt; dev_locality.DebugString();</td></tr>
<tr><th id="1168">1168</th><td>  }</td></tr>
<tr><th id="1169">1169</th><td>  <b>return</b> Status::OK();</td></tr>
<tr><th id="1170">1170</th><td>}</td></tr>
<tr><th id="1171">1171</th><td></td></tr>
<tr><th id="1172">1172</th><td><em>static</em> <em>int</em> GetDefaultMinGPUMultiprocessorCount(</td></tr>
<tr><th id="1173">1173</th><td>    gpu::Platform* gpu_manager,</td></tr>
<tr><th id="1174">1174</th><td>    <em>const</em> std::vector&lt;CudaGpuId&gt;&amp; visible_gpu_order) {</td></tr>
<tr><th id="1175">1175</th><td>  <em>static</em> <em>const</em> <em>int</em> kDefaultMinGPUMultiprocessorCount = <var>8</var>;</td></tr>
<tr><th id="1176">1176</th><td></td></tr>
<tr><th id="1177">1177</th><td>  <i>// Find the highest multi-processor count across all visible GPUs.</i></td></tr>
<tr><th id="1178">1178</th><td>  <em>int</em> max_count = -<var>1</var>;</td></tr>
<tr><th id="1179">1179</th><td>  <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; visible_gpu_order.size(); ++i) {</td></tr>
<tr><th id="1180">1180</th><td>    <em>auto</em> exec_status =</td></tr>
<tr><th id="1181">1181</th><td>        GpuIdUtil::ExecutorForCudaGpuId(gpu_manager, visible_gpu_order[i]);</td></tr>
<tr><th id="1182">1182</th><td>    <b>if</b> (!exec_status.ok()) {</td></tr>
<tr><th id="1183">1183</th><td>      <b>continue</b>;</td></tr>
<tr><th id="1184">1184</th><td>    }</td></tr>
<tr><th id="1185">1185</th><td></td></tr>
<tr><th id="1186">1186</th><td>    gpu::StreamExecutor* se = exec_status.ValueOrDie();</td></tr>
<tr><th id="1187">1187</th><td>    <em>const</em> gpu::DeviceDescription&amp; desc = se-&gt;GetDeviceDescription();</td></tr>
<tr><th id="1188">1188</th><td>    max_count = std::max(max_count, desc.core_count());</td></tr>
<tr><th id="1189">1189</th><td>  }</td></tr>
<tr><th id="1190">1190</th><td></td></tr>
<tr><th id="1191">1191</th><td>  <b>if</b> (max_count &lt; <var>0</var> || kDefaultMinGPUMultiprocessorCount &lt; max_count) {</td></tr>
<tr><th id="1192">1192</th><td>    <b>return</b> kDefaultMinGPUMultiprocessorCount;</td></tr>
<tr><th id="1193">1193</th><td>  } <b>else</b> {</td></tr>
<tr><th id="1194">1194</th><td>    <b>return</b> max_count;</td></tr>
<tr><th id="1195">1195</th><td>  }</td></tr>
<tr><th id="1196">1196</th><td>}</td></tr>
<tr><th id="1197">1197</th><td></td></tr>
<tr><th id="1198">1198</th><td><em>static</em> <em>int</em> GetMinGPUMultiprocessorCount(</td></tr>
<tr><th id="1199">1199</th><td>    gpu::Platform* gpu_manager,</td></tr>
<tr><th id="1200">1200</th><td>    <em>const</em> std::vector&lt;CudaGpuId&gt;&amp; visible_gpu_order) {</td></tr>
<tr><th id="1201">1201</th><td>  <em>const</em> <em>char</em>* tf_min_gpu_core_count = getenv(<q>"TF_MIN_GPU_MULTIPROCESSOR_COUNT"</q>);</td></tr>
<tr><th id="1202">1202</th><td></td></tr>
<tr><th id="1203">1203</th><td>  <b>if</b> (tf_min_gpu_core_count == <b>nullptr</b> ||</td></tr>
<tr><th id="1204">1204</th><td>      strcmp(tf_min_gpu_core_count, <q>""</q>) == <var>0</var>) {</td></tr>
<tr><th id="1205">1205</th><td>    <b>return</b> GetDefaultMinGPUMultiprocessorCount(gpu_manager, visible_gpu_order);</td></tr>
<tr><th id="1206">1206</th><td>  }</td></tr>
<tr><th id="1207">1207</th><td></td></tr>
<tr><th id="1208">1208</th><td>  <em>int</em> min_gpu_core_count = -<var>1</var>;</td></tr>
<tr><th id="1209">1209</th><td>  <b>if</b> (strings::safe_strto32(tf_min_gpu_core_count, &amp;min_gpu_core_count)) {</td></tr>
<tr><th id="1210">1210</th><td>    <b>if</b> (min_gpu_core_count &gt;= <var>0</var>) {</td></tr>
<tr><th id="1211">1211</th><td>      <b>return</b> min_gpu_core_count;</td></tr>
<tr><th id="1212">1212</th><td>    }</td></tr>
<tr><th id="1213">1213</th><td>  }</td></tr>
<tr><th id="1214">1214</th><td></td></tr>
<tr><th id="1215">1215</th><td>  <em>int</em> count =</td></tr>
<tr><th id="1216">1216</th><td>      GetDefaultMinGPUMultiprocessorCount(gpu_manager, visible_gpu_order);</td></tr>
<tr><th id="1217">1217</th><td>  LOG(ERROR) &lt;&lt; <q>"Invalid minimum GPU multiprocessor count: ["</q></td></tr>
<tr><th id="1218">1218</th><td>             &lt;&lt; tf_min_gpu_core_count &lt;&lt; <q>"]. "</q></td></tr>
<tr><th id="1219">1219</th><td>             &lt;&lt; <q>"Using the default value: "</q> &lt;&lt; count;</td></tr>
<tr><th id="1220">1220</th><td>  <b>return</b> count;</td></tr>
<tr><th id="1221">1221</th><td>}</td></tr>
<tr><th id="1222">1222</th><td></td></tr>
<tr><th id="1223">1223</th><td><b>namespace</b> {</td></tr>
<tr><th id="1224">1224</th><td></td></tr>
<tr><th id="1225">1225</th><td><b>struct</b> CudaVersion {</td></tr>
<tr><th id="1226">1226</th><td>  <i>// Initialize from version_name in the form of "3.5"</i></td></tr>
<tr><th id="1227">1227</th><td>  <b>explicit</b> CudaVersion(<em>const</em> std::string&amp; version_name) {</td></tr>
<tr><th id="1228">1228</th><td>    size_t dot_pos = version_name.find(<kbd>'.'</kbd>);</td></tr>
<tr><th id="1229">1229</th><td>    CHECK(dot_pos != string::npos)</td></tr>
<tr><th id="1230">1230</th><td>        &lt;&lt; <q>"Illegal version name: ["</q> &lt;&lt; version_name &lt;&lt; <q>"]"</q>;</td></tr>
<tr><th id="1231">1231</th><td>    string major_str = version_name.substr(<var>0</var>, dot_pos);</td></tr>
<tr><th id="1232">1232</th><td>    CHECK(strings::safe_strto32(major_str, &amp;major_part))</td></tr>
<tr><th id="1233">1233</th><td>        &lt;&lt; <q>"Illegal version name: ["</q> &lt;&lt; version_name &lt;&lt; <q>"]"</q>;</td></tr>
<tr><th id="1234">1234</th><td>    string minor_str = version_name.substr(dot_pos + <var>1</var>);</td></tr>
<tr><th id="1235">1235</th><td>    CHECK(strings::safe_strto32(minor_str, &amp;minor_part))</td></tr>
<tr><th id="1236">1236</th><td>        &lt;&lt; <q>"Illegal version name: ["</q> &lt;&lt; version_name &lt;&lt; <q>"]"</q>;</td></tr>
<tr><th id="1237">1237</th><td>  }</td></tr>
<tr><th id="1238">1238</th><td>  CudaVersion() {}</td></tr>
<tr><th id="1239">1239</th><td>  <em>bool</em> <b>operator</b>&lt;(<em>const</em> CudaVersion&amp; other) <em>const</em> {</td></tr>
<tr><th id="1240">1240</th><td>    <b>if</b> (<b>this</b>-&gt;major_part != other.major_part) {</td></tr>
<tr><th id="1241">1241</th><td>      <b>return</b> <b>this</b>-&gt;major_part &lt; other.major_part;</td></tr>
<tr><th id="1242">1242</th><td>    }</td></tr>
<tr><th id="1243">1243</th><td>    <b>return</b> <b>this</b>-&gt;minor_part &lt; other.minor_part;</td></tr>
<tr><th id="1244">1244</th><td>  }</td></tr>
<tr><th id="1245">1245</th><td>  <b>friend</b> std::ostream&amp; <b>operator</b>&lt;&lt;(std::ostream&amp; os,</td></tr>
<tr><th id="1246">1246</th><td>                                  <em>const</em> CudaVersion&amp; version) {</td></tr>
<tr><th id="1247">1247</th><td>    os &lt;&lt; version.major_part &lt;&lt; <q>"."</q> &lt;&lt; version.minor_part;</td></tr>
<tr><th id="1248">1248</th><td>    <b>return</b> os;</td></tr>
<tr><th id="1249">1249</th><td>  }</td></tr>
<tr><th id="1250">1250</th><td>  <em>int</em> major_part = -<var>1</var>;</td></tr>
<tr><th id="1251">1251</th><td>  <em>int</em> minor_part = -<var>1</var>;</td></tr>
<tr><th id="1252">1252</th><td>};</td></tr>
<tr><th id="1253">1253</th><td></td></tr>
<tr><th id="1254">1254</th><td>std::vector&lt;CudaVersion&gt; supported_cuda_compute_capabilities = {</td></tr>
<tr><th id="1255">1255</th><td>    TF_CUDA_CAPABILITIES,};</td></tr>
<tr><th id="1256">1256</th><td></td></tr>
<tr><th id="1257">1257</th><td>std::vector&lt;CudaVersion&gt; GetSupportedCudaComputeCapabilities() {</td></tr>
<tr><th id="1258">1258</th><td>  <em>auto</em> cuda_caps = supported_cuda_compute_capabilities;</td></tr>
<tr><th id="1259">1259</th><td><u>#ifdef TF_EXTRA_CUDA_CAPABILITIES</u></td></tr>
<tr><th id="1260">1260</th><td><i>// TF_EXTRA_CUDA_CAPABILITIES should be defined a sequence separated by commas,</i></td></tr>
<tr><th id="1261">1261</th><td><i>// for example:</i></td></tr>
<tr><th id="1262">1262</th><td><i>//   TF_EXTRA_CUDA_CAPABILITIES=3.0,4.0,5.0</i></td></tr>
<tr><th id="1263">1263</th><td><i>// Use two-level macro expansion for stringification.</i></td></tr>
<tr><th id="1264">1264</th><td><u>#define TF_XSTRING(...) #__VA_ARGS__</u></td></tr>
<tr><th id="1265">1265</th><td><u>#define TF_STRING(s) TF_XSTRING(s)</u></td></tr>
<tr><th id="1266">1266</th><td>  string extra_cuda_caps = TF_STRING(TF_EXTRA_CUDA_CAPABILITIES);</td></tr>
<tr><th id="1267">1267</th><td><u>#undef TF_STRING</u></td></tr>
<tr><th id="1268">1268</th><td><u>#undef TF_XSTRING</u></td></tr>
<tr><th id="1269">1269</th><td>  <em>auto</em> extra_capabilities = str_util::Split(extra_cuda_caps, <kbd>','</kbd>);</td></tr>
<tr><th id="1270">1270</th><td>  <b>for</b> (<em>const</em> <em>auto</em>&amp; capability : extra_capabilities) {</td></tr>
<tr><th id="1271">1271</th><td>    cuda_caps.push_back(CudaVersion(capability));</td></tr>
<tr><th id="1272">1272</th><td>  }</td></tr>
<tr><th id="1273">1273</th><td><u>#endif</u></td></tr>
<tr><th id="1274">1274</th><td>  <b>return</b> cuda_caps;</td></tr>
<tr><th id="1275">1275</th><td>}</td></tr>
<tr><th id="1276">1276</th><td></td></tr>
<tr><th id="1277">1277</th><td>Status EnablePeerAccess(gpu::Platform* platform,</td></tr>
<tr><th id="1278">1278</th><td>                        <em>const</em> std::vector&lt;CudaGpuId&gt;&amp; visible_gpu_order) {</td></tr>
<tr><th id="1279">1279</th><td>  <em>int</em> possible_peer_count = <var>0</var>;</td></tr>
<tr><th id="1280">1280</th><td>  <em>int</em> enabled_peer_count = <var>0</var>;</td></tr>
<tr><th id="1281">1281</th><td>  <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; visible_gpu_order.size(); ++i) {</td></tr>
<tr><th id="1282">1282</th><td>    <em>const</em> CudaGpuId cuda_gpu_i = visible_gpu_order[i];</td></tr>
<tr><th id="1283">1283</th><td>    <b>for</b> (<em>int</em> j = <var>0</var>; j &lt; visible_gpu_order.size(); ++j) {</td></tr>
<tr><th id="1284">1284</th><td>      <em>const</em> CudaGpuId cuda_gpu_j = visible_gpu_order[j];</td></tr>
<tr><th id="1285">1285</th><td>      <i>// We have already validated that ExecutorForDevice() calls return OK.</i></td></tr>
<tr><th id="1286">1286</th><td>      gpu::StreamExecutor* from =</td></tr>
<tr><th id="1287">1287</th><td>          GpuIdUtil::ExecutorForCudaGpuId(platform, cuda_gpu_i).ValueOrDie();</td></tr>
<tr><th id="1288">1288</th><td>      gpu::StreamExecutor* to =</td></tr>
<tr><th id="1289">1289</th><td>          GpuIdUtil::ExecutorForCudaGpuId(platform, cuda_gpu_j).ValueOrDie();</td></tr>
<tr><th id="1290">1290</th><td></td></tr>
<tr><th id="1291">1291</th><td>      <b>if</b> (from-&gt;CanEnablePeerAccessTo(to)) {</td></tr>
<tr><th id="1292">1292</th><td>        ++possible_peer_count;</td></tr>
<tr><th id="1293">1293</th><td>        <em>auto</em> status = from-&gt;EnablePeerAccessTo(to);</td></tr>
<tr><th id="1294">1294</th><td>        <b>if</b> (!status.ok()) {</td></tr>
<tr><th id="1295">1295</th><td>          LOG(WARNING)</td></tr>
<tr><th id="1296">1296</th><td>              &lt;&lt; <q>"Unable to enable peer access between device ordinals "</q></td></tr>
<tr><th id="1297">1297</th><td>              &lt;&lt; cuda_gpu_i &lt;&lt; <q>" and "</q> &lt;&lt; cuda_gpu_j &lt;&lt; <q>", status: "</q> &lt;&lt; status;</td></tr>
<tr><th id="1298">1298</th><td>        } <b>else</b> {</td></tr>
<tr><th id="1299">1299</th><td>          ++enabled_peer_count;</td></tr>
<tr><th id="1300">1300</th><td>        }</td></tr>
<tr><th id="1301">1301</th><td>      }</td></tr>
<tr><th id="1302">1302</th><td>    }</td></tr>
<tr><th id="1303">1303</th><td>  }</td></tr>
<tr><th id="1304">1304</th><td></td></tr>
<tr><th id="1305">1305</th><td>  <i>// Return an error in the extreme failure case where the driver</i></td></tr>
<tr><th id="1306">1306</th><td><i>  // reported that peering was possible but not a single peering was</i></td></tr>
<tr><th id="1307">1307</th><td><i>  // successful.  This is to catch possible system misconfigurations</i></td></tr>
<tr><th id="1308">1308</th><td><i>  // or more fundamental issues.</i></td></tr>
<tr><th id="1309">1309</th><td>  <b>if</b> (possible_peer_count &gt; <var>0</var> &amp;&amp; enabled_peer_count == <var>0</var>) {</td></tr>
<tr><th id="1310">1310</th><td>    <b>return</b> errors::Internal(possible_peer_count,</td></tr>
<tr><th id="1311">1311</th><td>                            <q>" potential peer access pairs were reported by the "</q></td></tr>
<tr><th id="1312">1312</th><td>                            <q>"driver, but no peering could be enabled."</q>);</td></tr>
<tr><th id="1313">1313</th><td>  }</td></tr>
<tr><th id="1314">1314</th><td>  <b>return</b> Status::OK();</td></tr>
<tr><th id="1315">1315</th><td>}</td></tr>
<tr><th id="1316">1316</th><td></td></tr>
<tr><th id="1317">1317</th><td>}  <i>// namespace</i></td></tr>
<tr><th id="1318">1318</th><td></td></tr>
<tr><th id="1319">1319</th><td>Status BaseGPUDeviceFactory::GetValidDeviceIds(</td></tr>
<tr><th id="1320">1320</th><td>    <em>const</em> std::vector&lt;CudaGpuId&gt;&amp; visible_gpu_order,</td></tr>
<tr><th id="1321">1321</th><td>    std::vector&lt;CudaGpuId&gt;* ids) {</td></tr>
<tr><th id="1322">1322</th><td>  gpu::Platform* gpu_manager = GPUMachineManager();</td></tr>
<tr><th id="1323">1323</th><td>  <em>bool</em> new_gpu_found = <b>false</b>;</td></tr>
<tr><th id="1324">1324</th><td>  <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; visible_gpu_order.size(); ++i) {</td></tr>
<tr><th id="1325">1325</th><td>    <em>const</em> CudaGpuId cuda_gpu_id = visible_gpu_order[i];</td></tr>
<tr><th id="1326">1326</th><td></td></tr>
<tr><th id="1327">1327</th><td>    <i>// Only perform this once per visible cuda gpu id.</i></td></tr>
<tr><th id="1328">1328</th><td>    <b>if</b> (visible_gpu_initialized_[cuda_gpu_id.value()]) {</td></tr>
<tr><th id="1329">1329</th><td>      <b>continue</b>;</td></tr>
<tr><th id="1330">1330</th><td>    }</td></tr>
<tr><th id="1331">1331</th><td></td></tr>
<tr><th id="1332">1332</th><td>    visible_gpu_initialized_[cuda_gpu_id.value()] = <b>true</b>;</td></tr>
<tr><th id="1333">1333</th><td>    new_gpu_found = <b>true</b>;</td></tr>
<tr><th id="1334">1334</th><td></td></tr>
<tr><th id="1335">1335</th><td>    <em>auto</em> executor = GpuIdUtil::ExecutorForCudaGpuId(gpu_manager, cuda_gpu_id);</td></tr>
<tr><th id="1336">1336</th><td>    <b>if</b> (!executor.ok()) {</td></tr>
<tr><th id="1337">1337</th><td>      <b>return</b> StreamExecutorUtil::ConvertStatus(executor.status());</td></tr>
<tr><th id="1338">1338</th><td>    }</td></tr>
<tr><th id="1339">1339</th><td></td></tr>
<tr><th id="1340">1340</th><td>    <em>auto</em> stream_exec = executor.ValueOrDie();</td></tr>
<tr><th id="1341">1341</th><td>    int64 free_bytes;</td></tr>
<tr><th id="1342">1342</th><td>    int64 total_bytes;</td></tr>
<tr><th id="1343">1343</th><td>    <b>if</b> (!stream_exec-&gt;DeviceMemoryUsage(&amp;free_bytes, &amp;total_bytes)) {</td></tr>
<tr><th id="1344">1344</th><td>      <i>// Logs internally on failure.</i></td></tr>
<tr><th id="1345">1345</th><td>      free_bytes = <var>0</var>;</td></tr>
<tr><th id="1346">1346</th><td>      total_bytes = <var>0</var>;</td></tr>
<tr><th id="1347">1347</th><td>    }</td></tr>
<tr><th id="1348">1348</th><td>    <em>const</em> <em>auto</em>&amp; description = stream_exec-&gt;GetDeviceDescription();</td></tr>
<tr><th id="1349">1349</th><td>    <em>int</em> cc_major;</td></tr>
<tr><th id="1350">1350</th><td>    <em>int</em> cc_minor;</td></tr>
<tr><th id="1351">1351</th><td>    <b>if</b> (!description.cuda_compute_capability(&amp;cc_major, &amp;cc_minor)) {</td></tr>
<tr><th id="1352">1352</th><td>      <i>// Logs internally on failure.</i></td></tr>
<tr><th id="1353">1353</th><td>      cc_major = <var>0</var>;</td></tr>
<tr><th id="1354">1354</th><td>      cc_minor = <var>0</var>;</td></tr>
<tr><th id="1355">1355</th><td>    }</td></tr>
<tr><th id="1356">1356</th><td>    LOG(INFO) &lt;&lt; <q>"Found device "</q> &lt;&lt; i &lt;&lt; <q>" with properties: "</q></td></tr>
<tr><th id="1357">1357</th><td>              &lt;&lt; <q>"\nname: "</q> &lt;&lt; description.name() &lt;&lt; <q>" major: "</q> &lt;&lt; cc_major</td></tr>
<tr><th id="1358">1358</th><td>              &lt;&lt; <q>" minor: "</q> &lt;&lt; cc_minor</td></tr>
<tr><th id="1359">1359</th><td>              &lt;&lt; <q>" memoryClockRate(GHz): "</q> &lt;&lt; description.clock_rate_ghz()</td></tr>
<tr><th id="1360">1360</th><td>              &lt;&lt; <q>"\npciBusID: "</q> &lt;&lt; description.pci_bus_id() &lt;&lt; <q>"\ntotalMemory: "</q></td></tr>
<tr><th id="1361">1361</th><td>              &lt;&lt; strings::HumanReadableNumBytes(total_bytes)</td></tr>
<tr><th id="1362">1362</th><td>              &lt;&lt; <q>" freeMemory: "</q> &lt;&lt; strings::HumanReadableNumBytes(free_bytes);</td></tr>
<tr><th id="1363">1363</th><td>  }</td></tr>
<tr><th id="1364">1364</th><td>  <i>// Checking peering and shows matrix if more than one gpu found.</i></td></tr>
<tr><th id="1365">1365</th><td>  <b>if</b> (new_gpu_found &amp;&amp; visible_gpu_order.size() &gt; <var>1</var>) {</td></tr>
<tr><th id="1366">1366</th><td>    <i>// Enable peer access</i></td></tr>
<tr><th id="1367">1367</th><td>    TF_RETURN_IF_ERROR(EnablePeerAccess(gpu_manager, visible_gpu_order));</td></tr>
<tr><th id="1368">1368</th><td>  }</td></tr>
<tr><th id="1369">1369</th><td></td></tr>
<tr><th id="1370">1370</th><td>  <em>auto</em> cuda_supported_capabilities = GetSupportedCudaComputeCapabilities();</td></tr>
<tr><th id="1371">1371</th><td>  <b>if</b> (cuda_supported_capabilities.empty()) {</td></tr>
<tr><th id="1372">1372</th><td>    <b>return</b> errors::FailedPrecondition(</td></tr>
<tr><th id="1373">1373</th><td>        <q>"No supported cuda capabilities in binary."</q>);</td></tr>
<tr><th id="1374">1374</th><td>  }</td></tr>
<tr><th id="1375">1375</th><td>  CudaVersion min_supported_capability = *std::min_element(</td></tr>
<tr><th id="1376">1376</th><td>      cuda_supported_capabilities.begin(), cuda_supported_capabilities.end());</td></tr>
<tr><th id="1377">1377</th><td></td></tr>
<tr><th id="1378">1378</th><td>  <em>int</em> min_gpu_core_count =</td></tr>
<tr><th id="1379">1379</th><td>      GetMinGPUMultiprocessorCount(gpu_manager, visible_gpu_order);</td></tr>
<tr><th id="1380">1380</th><td></td></tr>
<tr><th id="1381">1381</th><td>  <i>// Filter out devices that don't have the right capability or power.</i></td></tr>
<tr><th id="1382">1382</th><td>  <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; visible_gpu_order.size(); ++i) {</td></tr>
<tr><th id="1383">1383</th><td>    <em>const</em> CudaGpuId visible_gpu_id = visible_gpu_order[i];</td></tr>
<tr><th id="1384">1384</th><td>    <em>auto</em> exec_status =</td></tr>
<tr><th id="1385">1385</th><td>        GpuIdUtil::ExecutorForCudaGpuId(gpu_manager, visible_gpu_id);</td></tr>
<tr><th id="1386">1386</th><td>    <b>if</b> (!exec_status.ok()) {</td></tr>
<tr><th id="1387">1387</th><td>      LOG(INFO) &lt;&lt; <q>"Ignoring visible gpu device "</q> &lt;&lt; visible_gpu_id</td></tr>
<tr><th id="1388">1388</th><td>                &lt;&lt; <q>" whose executor is in invalid state: "</q></td></tr>
<tr><th id="1389">1389</th><td>                &lt;&lt; exec_status.status().ToString();</td></tr>
<tr><th id="1390">1390</th><td>      <b>continue</b>;</td></tr>
<tr><th id="1391">1391</th><td>    }</td></tr>
<tr><th id="1392">1392</th><td>    gpu::StreamExecutor* se = exec_status.ValueOrDie();</td></tr>
<tr><th id="1393">1393</th><td>    <em>const</em> gpu::DeviceDescription&amp; desc = se-&gt;GetDeviceDescription();</td></tr>
<tr><th id="1394">1394</th><td>    CudaVersion device_capability;</td></tr>
<tr><th id="1395">1395</th><td>    <b>if</b> (!desc.cuda_compute_capability(&amp;device_capability.major_part,</td></tr>
<tr><th id="1396">1396</th><td>                                      &amp;device_capability.minor_part)) {</td></tr>
<tr><th id="1397">1397</th><td>      LOG(INFO) &lt;&lt; <q>"Ignoring visible gpu device "</q></td></tr>
<tr><th id="1398">1398</th><td>                &lt;&lt; <q>"("</q> &lt;&lt; GetShortDeviceDescription(visible_gpu_id, desc)</td></tr>
<tr><th id="1399">1399</th><td>                &lt;&lt; <q>") "</q></td></tr>
<tr><th id="1400">1400</th><td>                &lt;&lt; <q>"whose CUDA compute capability is not available."</q>;</td></tr>
<tr><th id="1401">1401</th><td>      <b>continue</b>;</td></tr>
<tr><th id="1402">1402</th><td>    }</td></tr>
<tr><th id="1403">1403</th><td>    <i>// Only GPUs with no less than the minimum supported compute capability is</i></td></tr>
<tr><th id="1404">1404</th><td><i>    // accepted.</i></td></tr>
<tr><th id="1405">1405</th><td>    <b>if</b> (device_capability &lt; min_supported_capability) {</td></tr>
<tr><th id="1406">1406</th><td>      LOG(INFO) &lt;&lt; <q>"Ignoring visible gpu device "</q></td></tr>
<tr><th id="1407">1407</th><td>                &lt;&lt; <q>"("</q> &lt;&lt; GetShortDeviceDescription(visible_gpu_id, desc)</td></tr>
<tr><th id="1408">1408</th><td>                &lt;&lt; <q>") "</q></td></tr>
<tr><th id="1409">1409</th><td>                &lt;&lt; <q>"with Cuda compute capability "</q> &lt;&lt; device_capability</td></tr>
<tr><th id="1410">1410</th><td>                &lt;&lt; <q>". The minimum required Cuda capability is "</q></td></tr>
<tr><th id="1411">1411</th><td>                &lt;&lt; min_supported_capability &lt;&lt; <q>"."</q>;</td></tr>
<tr><th id="1412">1412</th><td>      <b>continue</b>;</td></tr>
<tr><th id="1413">1413</th><td>    }</td></tr>
<tr><th id="1414">1414</th><td></td></tr>
<tr><th id="1415">1415</th><td>    <i>// Filter out slow GPUs. By default, GPUs with a lower multiprocessor</i></td></tr>
<tr><th id="1416">1416</th><td><i>    // count than the fastest GPU are filtered out, unless they have 8 or more</i></td></tr>
<tr><th id="1417">1417</th><td><i>    // multiprocessors. If the TF_MIN_GPU_MULTIPROCESSOR_COUNT environment</i></td></tr>
<tr><th id="1418">1418</th><td><i>    // variable is set, its value will be used to filter out GPUs.</i></td></tr>
<tr><th id="1419">1419</th><td>    <b>if</b> (desc.core_count() &lt; min_gpu_core_count) {</td></tr>
<tr><th id="1420">1420</th><td>      LOG(INFO) &lt;&lt; <q>"Ignoring visible gpu device "</q></td></tr>
<tr><th id="1421">1421</th><td>                &lt;&lt; <q>"("</q> &lt;&lt; GetShortDeviceDescription(visible_gpu_id, desc)</td></tr>
<tr><th id="1422">1422</th><td>                &lt;&lt; <q>") "</q></td></tr>
<tr><th id="1423">1423</th><td>                &lt;&lt; <q>"with Cuda multiprocessor count: "</q> &lt;&lt; desc.core_count()</td></tr>
<tr><th id="1424">1424</th><td>                &lt;&lt; <q>". The minimum required count is "</q> &lt;&lt; min_gpu_core_count</td></tr>
<tr><th id="1425">1425</th><td>                &lt;&lt; <q>". You can adjust this requirement with the env var "</q></td></tr>
<tr><th id="1426">1426</th><td>                   <q>"TF_MIN_GPU_MULTIPROCESSOR_COUNT."</q>;</td></tr>
<tr><th id="1427">1427</th><td>      <b>continue</b>;</td></tr>
<tr><th id="1428">1428</th><td>    }</td></tr>
<tr><th id="1429">1429</th><td>    ids-&gt;push_back(visible_gpu_id);</td></tr>
<tr><th id="1430">1430</th><td>  }</td></tr>
<tr><th id="1431">1431</th><td>  <b>if</b> (!ids-&gt;empty()) {</td></tr>
<tr><th id="1432">1432</th><td>    std::vector&lt;<em>int</em>&gt; raw_ids(ids-&gt;size());</td></tr>
<tr><th id="1433">1433</th><td>    std::transform(ids-&gt;begin(), ids-&gt;end(), raw_ids.begin(),</td></tr>
<tr><th id="1434">1434</th><td>                   [](CudaGpuId id) -&gt; <em>int</em> { <b>return</b> id.value(); });</td></tr>
<tr><th id="1435">1435</th><td>    LOG(INFO) &lt;&lt; <q>"Adding visible gpu devices: "</q></td></tr>
<tr><th id="1436">1436</th><td>              &lt;&lt; str_util::Join(raw_ids, <q>", "</q>);</td></tr>
<tr><th id="1437">1437</th><td>  }</td></tr>
<tr><th id="1438">1438</th><td></td></tr>
<tr><th id="1439">1439</th><td>  <b>return</b> Status::OK();</td></tr>
<tr><th id="1440">1440</th><td>}</td></tr>
<tr><th id="1441">1441</th><td></td></tr>
<tr><th id="1442">1442</th><td>}  <i>// namespace tensorflow</i></td></tr>
<tr><th id="1443">1443</th><td></td></tr>
<tr><th id="1444">1444</th><td><u>#<span data-ppcond="18">endif</span>  // GOOGLE_CUDA</u></td></tr>
<tr><th id="1445">1445</th><td></td></tr>
</table><hr/><p id='footer'>
Generated on <em>2018-Aug-20</em> from project tensorflow revision <em>v1.8</em><br />Powered by <a href='https://woboq.com'><img alt='Woboq' src='https://code.woboq.org/woboq-16.png' width='41' height='16' /></a> <a href='https://code.woboq.org'>Code Browser</a> 2.1
<br/>Generator usage only permitted with license.</p>
</div></body></html>
