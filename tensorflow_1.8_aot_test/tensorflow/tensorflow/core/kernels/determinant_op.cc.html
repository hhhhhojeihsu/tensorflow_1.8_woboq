<!doctype html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><title>determinant_op.cc source code [tensorflow/tensorflow/core/kernels/determinant_op.cc] - Woboq Code Browser</title>
<meta name="woboq:interestingDefinitions" content="tensorflow::DeterminantOp,tensorflow::LogDeterminantOp "/>
<link rel="stylesheet" href="https://code.woboq.org/data/qtcreator.css" title="QtCreator"/>
<link rel="alternate stylesheet" href="https://code.woboq.org/data/kdevelop.css" title="KDevelop"/>
<script type="text/javascript" src="https://code.woboq.org/data/jquery/jquery.min.js"></script>
<script type="text/javascript" src="https://code.woboq.org/data/jquery/jquery-ui.min.js"></script>
<script>var file = 'tensorflow/tensorflow/core/kernels/determinant_op.cc'; var root_path = '../../../..'; var data_path = 'https://code.woboq.org/data';</script>
<script src='https://code.woboq.org/data/codebrowser.js'></script>
</head>
<body><div id='header'><h1 id='breadcrumb'><span>Browse the source code of </span><a href='../../..'>tensorflow</a>/<a href='../..'>tensorflow</a>/<a href='..'>core</a>/<a href='./'>kernels</a>/<a href='determinant_op.cc.html'>determinant_op.cc</a></h1></div>
<hr/><div id='content'><table class="code">
<tr><th id="1">1</th><td><i>/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.</i></td></tr>
<tr><th id="2">2</th><td><i></i></td></tr>
<tr><th id="3">3</th><td><i>Licensed under the Apache License, Version 2.0 (the "License");</i></td></tr>
<tr><th id="4">4</th><td><i>you may not use this file except in compliance with the License.</i></td></tr>
<tr><th id="5">5</th><td><i>You may obtain a copy of the License at</i></td></tr>
<tr><th id="6">6</th><td><i></i></td></tr>
<tr><th id="7">7</th><td><i>    <a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></i></td></tr>
<tr><th id="8">8</th><td><i></i></td></tr>
<tr><th id="9">9</th><td><i>Unless required by applicable law or agreed to in writing, software</i></td></tr>
<tr><th id="10">10</th><td><i>distributed under the License is distributed on an "AS IS" BASIS,</i></td></tr>
<tr><th id="11">11</th><td><i>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</i></td></tr>
<tr><th id="12">12</th><td><i>See the License for the specific language governing permissions and</i></td></tr>
<tr><th id="13">13</th><td><i>limitations under the License.</i></td></tr>
<tr><th id="14">14</th><td><i>==============================================================================*/</i></td></tr>
<tr><th id="15">15</th><td><i></i></td></tr>
<tr><th id="16">16</th><td><i>// See docs in ../ops/linalg_ops.cc.</i></td></tr>
<tr><th id="17">17</th><td></td></tr>
<tr><th id="18">18</th><td><u>#include <a href="../../../../include/c++/5/cmath.html">&lt;cmath&gt;</a></u></td></tr>
<tr><th id="19">19</th><td></td></tr>
<tr><th id="20">20</th><td><u>#<span data-ppcond="20">if</span> GOOGLE_CUDA</u></td></tr>
<tr><th id="21">21</th><td><u>#define EIGEN_USE_GPU</u></td></tr>
<tr><th id="22">22</th><td><u>#include "third_party/eigen3/unsupported/Eigen/CXX11/Tensor"</u></td></tr>
<tr><th id="23">23</th><td><u>#include "tensorflow/core/kernels/determinant_op.h"</u></td></tr>
<tr><th id="24">24</th><td><u>#<span data-ppcond="20">endif</span></u></td></tr>
<tr><th id="25">25</th><td></td></tr>
<tr><th id="26">26</th><td><u>#include <a href="../../../third_party/eigen3/Eigen/LU.html">"third_party/eigen3/Eigen/LU"</a></u></td></tr>
<tr><th id="27">27</th><td><u>#include <a href="../framework/kernel_def_builder.h.html">"tensorflow/core/framework/kernel_def_builder.h"</a></u></td></tr>
<tr><th id="28">28</th><td><u>#include <a href="../framework/numeric_types.h.html">"tensorflow/core/framework/numeric_types.h"</a></u></td></tr>
<tr><th id="29">29</th><td><u>#include <a href="../framework/op_kernel.h.html">"tensorflow/core/framework/op_kernel.h"</a></u></td></tr>
<tr><th id="30">30</th><td><u>#include <a href="../framework/tensor_shape.h.html">"tensorflow/core/framework/tensor_shape.h"</a></u></td></tr>
<tr><th id="31">31</th><td><u>#include <a href="linalg_ops_common.h.html">"tensorflow/core/kernels/linalg_ops_common.h"</a></u></td></tr>
<tr><th id="32">32</th><td><u>#include <a href="../lib/core/errors.h.html">"tensorflow/core/lib/core/errors.h"</a></u></td></tr>
<tr><th id="33">33</th><td><u>#include <a href="../platform/logging.h.html">"tensorflow/core/platform/logging.h"</a></u></td></tr>
<tr><th id="34">34</th><td><u>#include <a href="../platform/types.h.html">"tensorflow/core/platform/types.h"</a></u></td></tr>
<tr><th id="35">35</th><td></td></tr>
<tr><th id="36">36</th><td><u>#<span data-ppcond="36">if</span> GOOGLE_CUDA</u></td></tr>
<tr><th id="37">37</th><td><u>#include "tensorflow/core/kernels/cuda_solvers.h"</u></td></tr>
<tr><th id="38">38</th><td><u>#include "tensorflow/core/kernels/fill_functor.h"</u></td></tr>
<tr><th id="39">39</th><td><u>#<span data-ppcond="36">endif</span></u></td></tr>
<tr><th id="40">40</th><td></td></tr>
<tr><th id="41">41</th><td><b>namespace</b> <span class="namespace">tensorflow</span> {</td></tr>
<tr><th id="42">42</th><td></td></tr>
<tr><th id="43">43</th><td><i>// A helper function to compute the sign and absolute value of the log of the</i></td></tr>
<tr><th id="44">44</th><td><i>// determinant of inputs via a partially pivoted LU</i></td></tr>
<tr><th id="45">45</th><td><i>// factorization.</i></td></tr>
<tr><th id="46">46</th><td><i>//</i></td></tr>
<tr><th id="47">47</th><td><i>// Returns the log of the absolute value of the determinant, and its sign in</i></td></tr>
<tr><th id="48">48</th><td><i>// 'sign'.</i></td></tr>
<tr><th id="49">49</th><td><b>template</b> &lt;<b>class</b> Scalar&gt;</td></tr>
<tr><th id="50">50</th><td><em>static</em> <b>typename</b> <span class="namespace">Eigen::</span><span class='type' title='Eigen::NumTraits' data-ref="Eigen::NumTraits">NumTraits</span>&lt;Scalar&gt;::Real <dfn class="tu decl def" id="_ZN10tensorflowL7SLogDetERKN5Eigen6MatrixIT_Lin1ELin1ELi0ELin1ELin1EEEPS2_" title='tensorflow::SLogDet' data-type='typename Eigen::NumTraits&lt;Scalar&gt;::Real tensorflow::SLogDet(const Eigen::Matrix&lt;Scalar, Eigen::Dynamic, Eigen::Dynamic&gt; &amp; inputs, Scalar * sign)' data-ref="_ZN10tensorflowL7SLogDetERKN5Eigen6MatrixIT_Lin1ELin1ELi0ELin1ELin1EEEPS2_">SLogDet</dfn>(</td></tr>
<tr><th id="51">51</th><td>    <em>const</em> <span class="namespace">Eigen::</span><span class='type' title='Eigen::Matrix' data-ref="Eigen::Matrix">Matrix</span>&lt;Scalar, <span class="namespace">Eigen::</span><span class='ref' title='Eigen::Dynamic' data-ref="Eigen::Dynamic">Dynamic</span>, <span class="namespace">Eigen::</span><span class='ref' title='Eigen::Dynamic' data-ref="Eigen::Dynamic">Dynamic</span>&gt;&amp; <dfn class="local col1 decl" id="1inputs" title='inputs' data-type='const Eigen::Matrix&lt;Scalar, Eigen::Dynamic, Eigen::Dynamic&gt; &amp;' data-ref="1inputs">inputs</dfn>,</td></tr>
<tr><th id="52">52</th><td>    Scalar* <dfn class="local col2 decl" id="2sign" title='sign' data-type='Scalar *' data-ref="2sign">sign</dfn>) {</td></tr>
<tr><th id="53">53</th><td>  <b>using</b> <dfn class="local col3 typedef" id="3RealScalar" title='RealScalar' data-type='typename Eigen::NumTraits&lt;Scalar&gt;::Real' data-ref="3RealScalar">RealScalar</dfn> = <b>typename</b> <span class="namespace">Eigen::</span><span class='type' title='Eigen::NumTraits' data-ref="Eigen::NumTraits">NumTraits</span>&lt;Scalar&gt;::Real;</td></tr>
<tr><th id="54">54</th><td>  <a class="local col3 typedef" href="#3RealScalar" title='RealScalar' data-type='typename Eigen::NumTraits&lt;Scalar&gt;::Real' data-ref="3RealScalar">RealScalar</a> <dfn class="local col4 decl" id="4log_abs_det" title='log_abs_det' data-type='RealScalar' data-ref="4log_abs_det">log_abs_det</dfn> = <var>0</var>;</td></tr>
<tr><th id="55">55</th><td>  *<a class="local col2 ref" href="#2sign" title='sign' data-ref="2sign">sign</a> = <var>1</var>;</td></tr>
<tr><th id="56">56</th><td>  <i>// An empty matrix' determinant is defined to be 1.</i></td></tr>
<tr><th id="57">57</th><td><i>  // (<a href="https://en.wikipedia.org/wiki/Determinant">https://en.wikipedia.org/wiki/Determinant</a>)</i></td></tr>
<tr><th id="58">58</th><td>  <b>if</b> (<a class="local col1 ref" href="#1inputs" title='inputs' data-ref="1inputs">inputs</a>.size() &gt; <var>0</var>) {</td></tr>
<tr><th id="59">59</th><td>    <i>// Compute the log determinant through a Partially Pivoted LU decomposition</i></td></tr>
<tr><th id="60">60</th><td>    <b>using</b> <span class="namespace">Eigen::</span>Dynamic;</td></tr>
<tr><th id="61">61</th><td>    <span class="namespace">Eigen::</span><span class='type' title='Eigen::PartialPivLU' data-ref="Eigen::PartialPivLU">PartialPivLU</span>&lt;<span class="namespace">Eigen::</span><span class='type' title='Eigen::Matrix' data-ref="Eigen::Matrix">Matrix</span>&lt;Scalar, <span class='ref' title='Eigen::Dynamic' data-ref="Eigen::Dynamic">Dynamic</span>, <span class='ref' title='Eigen::Dynamic' data-ref="Eigen::Dynamic">Dynamic</span>&gt;&gt; <dfn class="local col5 decl" id="5lu" title='lu' data-type='Eigen::PartialPivLU&lt;Eigen::Matrix&lt;Scalar, Dynamic, Dynamic&gt; &gt;' data-ref="5lu">lu</dfn>(<a class="local col1 ref" href="#1inputs" title='inputs' data-ref="1inputs">inputs</a>);</td></tr>
<tr><th id="62">62</th><td>    <span class="namespace">Eigen::</span><span class='type' title='Eigen::Matrix' data-ref="Eigen::Matrix">Matrix</span>&lt;Scalar, <span class='ref' title='Eigen::Dynamic' data-ref="Eigen::Dynamic">Dynamic</span>, <span class='ref' title='Eigen::Dynamic' data-ref="Eigen::Dynamic">Dynamic</span>&gt; <dfn class="local col6 decl" id="6LU" title='LU' data-type='Eigen::Matrix&lt;Scalar, Dynamic, Dynamic&gt;' data-ref="6LU">LU</dfn> = <a class="local col5 ref" href="#5lu" title='lu' data-ref="5lu">lu</a>.matrixLU();</td></tr>
<tr><th id="63">63</th><td>    *<a class="local col2 ref" href="#2sign" title='sign' data-ref="2sign">sign</a> = <a class="local col5 ref" href="#5lu" title='lu' data-ref="5lu">lu</a>.permutationP().determinant();</td></tr>
<tr><th id="64">64</th><td>    <em>auto</em> <dfn class="local col7 decl" id="7diag" title='diag' data-type='auto' data-ref="7diag">diag</dfn> = <a class="local col6 ref" href="#6LU" title='LU' data-ref="6LU">LU</a>.diagonal().array().eval();</td></tr>
<tr><th id="65">65</th><td>    <em>auto</em> <dfn class="local col8 decl" id="8abs_diag" title='abs_diag' data-type='auto' data-ref="8abs_diag">abs_diag</dfn> = <a class="local col7 ref" href="#7diag" title='diag' data-ref="7diag">diag</a>.cwiseAbs().eval();</td></tr>
<tr><th id="66">66</th><td>    <a class="local col4 ref" href="#4log_abs_det" title='log_abs_det' data-ref="4log_abs_det">log_abs_det</a> += <a class="local col8 ref" href="#8abs_diag" title='abs_diag' data-ref="8abs_diag">abs_diag</a>.log().sum();</td></tr>
<tr><th id="67">67</th><td>    *<a class="local col2 ref" href="#2sign" title='sign' data-ref="2sign">sign</a> *= (<a class="local col7 ref" href="#7diag" title='diag' data-ref="7diag">diag</a> / <a class="local col8 ref" href="#8abs_diag" title='abs_diag' data-ref="8abs_diag">abs_diag</a>).prod();</td></tr>
<tr><th id="68">68</th><td>  }</td></tr>
<tr><th id="69">69</th><td>  <b>if</b> (!<span class="namespace">Eigen::numext::</span>isfinite(<a class="local col4 ref" href="#4log_abs_det" title='log_abs_det' data-ref="4log_abs_det">log_abs_det</a>)) {</td></tr>
<tr><th id="70">70</th><td>    *<a class="local col2 ref" href="#2sign" title='sign' data-ref="2sign">sign</a> = <var>0</var>;</td></tr>
<tr><th id="71">71</th><td>    <a class="local col4 ref" href="#4log_abs_det" title='log_abs_det' data-ref="4log_abs_det">log_abs_det</a> =</td></tr>
<tr><th id="72">72</th><td>        <a class="local col4 ref" href="#4log_abs_det" title='log_abs_det' data-ref="4log_abs_det">log_abs_det</a> &gt; <var>0</var> ? -<span class="namespace">std::</span>log(<a class="local col3 typedef" href="#3RealScalar" title='RealScalar' data-type='typename Eigen::NumTraits&lt;Scalar&gt;::Real' data-ref="3RealScalar">RealScalar</a>(<var>0</var>)) : <span class="namespace">std::</span>log(<a class="local col3 typedef" href="#3RealScalar" title='RealScalar' data-type='typename Eigen::NumTraits&lt;Scalar&gt;::Real' data-ref="3RealScalar">RealScalar</a>(<var>0</var>));</td></tr>
<tr><th id="73">73</th><td>  }</td></tr>
<tr><th id="74">74</th><td>  <b>return</b> <a class="local col4 ref" href="#4log_abs_det" title='log_abs_det' data-ref="4log_abs_det">log_abs_det</a>;</td></tr>
<tr><th id="75">75</th><td>}</td></tr>
<tr><th id="76">76</th><td></td></tr>
<tr><th id="77">77</th><td><b>template</b> &lt;<b>class</b> Scalar&gt;</td></tr>
<tr><th id="78">78</th><td><b>class</b> <dfn class="type def" id="tensorflow::LogDeterminantOp" title='tensorflow::LogDeterminantOp' data-ref="tensorflow::LogDeterminantOp">LogDeterminantOp</dfn> : <b>public</b> <a class="type" href="linalg_ops_common.h.html#tensorflow::LinearAlgebraOp" title='tensorflow::LinearAlgebraOp' data-ref="tensorflow::LinearAlgebraOp">LinearAlgebraOp</a>&lt;Scalar&gt; {</td></tr>
<tr><th id="79">79</th><td> <b>public</b>:</td></tr>
<tr><th id="80">80</th><td>  <a class="macro" href="linalg_ops_common.h.html#175" title="typedef LinearAlgebraOp&lt;Scalar&gt; Base; using RealScalar = typename Eigen::NumTraits&lt;Scalar&gt;::Real; using Matrix = typename Base::Matrix; using MatrixMap = typename Base::MatrixMap; using MatrixMaps = typename Base::MatrixMaps; using ConstMatrixMap = typename Base::ConstMatrixMap; using ConstMatrixMaps = typename Base::ConstMatrixMaps; using TensorShapes = typename Base::TensorShapes;" data-ref="_M/INHERIT_LINALG_TYPEDEFS">INHERIT_LINALG_TYPEDEFS</a>(Scalar);</td></tr>
<tr><th id="81">81</th><td></td></tr>
<tr><th id="82">82</th><td>  <b>explicit</b> <dfn class="tu decl def" id="_ZN10tensorflow16LogDeterminantOpC1EPNS_20OpKernelConstructionE" title='tensorflow::LogDeterminantOp::LogDeterminantOp&lt;Scalar&gt;' data-type='void tensorflow::LogDeterminantOp::LogDeterminantOp&lt;Scalar&gt;(tensorflow::OpKernelConstruction * context)' data-ref="_ZN10tensorflow16LogDeterminantOpC1EPNS_20OpKernelConstructionE">LogDeterminantOp</dfn>(<a class="type" href="../framework/op_kernel.h.html#tensorflow::OpKernelConstruction" title='tensorflow::OpKernelConstruction' data-ref="tensorflow::OpKernelConstruction">OpKernelConstruction</a>* <dfn class="local col9 decl" id="9context" title='context' data-type='tensorflow::OpKernelConstruction *' data-ref="9context">context</dfn>) : <a class="typedef" href="#80" title='tensorflow::LogDeterminantOp::Base' data-type='LinearAlgebraOp&lt;Scalar&gt;' data-ref="tensorflow::LogDeterminantOp::Base">Base</a>(<a class="local col9 ref" href="#9context" title='context' data-ref="9context">context</a>) {}</td></tr>
<tr><th id="83">83</th><td></td></tr>
<tr><th id="84">84</th><td>  <a class="typedef" href="#80" title='tensorflow::LogDeterminantOp::TensorShapes' data-type='typename Base::TensorShapes' data-ref="tensorflow::LogDeterminantOp::TensorShapes">TensorShapes</a> <dfn class="tu decl def" id="_ZNK10tensorflow16LogDeterminantOp21GetOutputMatrixShapesERKNS_15LinearAlgebraOpIT_E12TensorShapesE" title='tensorflow::LogDeterminantOp::GetOutputMatrixShapes' data-type='TensorShapes tensorflow::LogDeterminantOp::GetOutputMatrixShapes(const TensorShapes &amp; input_matrix_shapes) const' data-ref="_ZNK10tensorflow16LogDeterminantOp21GetOutputMatrixShapesERKNS_15LinearAlgebraOpIT_E12TensorShapesE">GetOutputMatrixShapes</dfn>(</td></tr>
<tr><th id="85">85</th><td>      <em>const</em> <a class="typedef" href="#80" title='tensorflow::LogDeterminantOp::TensorShapes' data-type='typename Base::TensorShapes' data-ref="tensorflow::LogDeterminantOp::TensorShapes">TensorShapes</a>&amp; <dfn class="local col0 decl" id="10input_matrix_shapes" title='input_matrix_shapes' data-type='const TensorShapes &amp;' data-ref="10input_matrix_shapes">input_matrix_shapes</dfn>) <em>const</em> final {</td></tr>
<tr><th id="86">86</th><td>    <b>return</b> <a class="typedef" href="#80" title='tensorflow::LogDeterminantOp::TensorShapes' data-type='typename Base::TensorShapes' data-ref="tensorflow::LogDeterminantOp::TensorShapes">TensorShapes</a>({<a class="type" href="../framework/tensor_shape.h.html#tensorflow::TensorShape" title='tensorflow::TensorShape' data-ref="tensorflow::TensorShape">TensorShape</a><a class="ref" href="../framework/tensor_shape.h.html#291" title='tensorflow::TensorShape::TensorShape' data-ref="_ZN10tensorflow11TensorShapeC1ESt16initializer_listIxE">(</a><a class="ref" href="../../../../include/c++/5/initializer_list.html#_ZNSt16initializer_listC1Ev" title='std::initializer_list::initializer_list&lt;_E&gt;' data-ref="_ZNSt16initializer_listC1Ev">{</a>}), <a class="type" href="../framework/tensor_shape.h.html#tensorflow::TensorShape" title='tensorflow::TensorShape' data-ref="tensorflow::TensorShape">TensorShape</a><a class="ref" href="../framework/tensor_shape.h.html#291" title='tensorflow::TensorShape::TensorShape' data-ref="_ZN10tensorflow11TensorShapeC1ESt16initializer_listIxE">(</a><a class="ref" href="../../../../include/c++/5/initializer_list.html#_ZNSt16initializer_listC1Ev" title='std::initializer_list::initializer_list&lt;_E&gt;' data-ref="_ZNSt16initializer_listC1Ev">{</a>})});</td></tr>
<tr><th id="87">87</th><td>  }</td></tr>
<tr><th id="88">88</th><td></td></tr>
<tr><th id="89">89</th><td>  <em>void</em> <dfn class="tu decl def" id="_ZN10tensorflow16LogDeterminantOp13ComputeMatrixEPNS_15OpKernelContextERKNS_15LinearAlgebraOpIT_E15ConstMatrixMapsEPNS5_10MatrixMapsE" title='tensorflow::LogDeterminantOp::ComputeMatrix' data-type='void tensorflow::LogDeterminantOp::ComputeMatrix(tensorflow::OpKernelContext * context, const ConstMatrixMaps &amp; inputs, MatrixMaps * outputs)' data-ref="_ZN10tensorflow16LogDeterminantOp13ComputeMatrixEPNS_15OpKernelContextERKNS_15LinearAlgebraOpIT_E15ConstMatrixMapsEPNS5_10MatrixMapsE">ComputeMatrix</dfn>(<a class="type" href="../framework/op_kernel.h.html#tensorflow::OpKernelContext" title='tensorflow::OpKernelContext' data-ref="tensorflow::OpKernelContext">OpKernelContext</a>* <dfn class="local col1 decl" id="11context" title='context' data-type='tensorflow::OpKernelContext *' data-ref="11context">context</dfn>, <em>const</em> <a class="typedef" href="#80" title='tensorflow::LogDeterminantOp::ConstMatrixMaps' data-type='typename Base::ConstMatrixMaps' data-ref="tensorflow::LogDeterminantOp::ConstMatrixMaps">ConstMatrixMaps</a>&amp; <dfn class="local col2 decl" id="12inputs" title='inputs' data-type='const ConstMatrixMaps &amp;' data-ref="12inputs">inputs</dfn>,</td></tr>
<tr><th id="90">90</th><td>                     <a class="typedef" href="#80" title='tensorflow::LogDeterminantOp::MatrixMaps' data-type='typename Base::MatrixMaps' data-ref="tensorflow::LogDeterminantOp::MatrixMaps">MatrixMaps</a>* <dfn class="local col3 decl" id="13outputs" title='outputs' data-type='MatrixMaps *' data-ref="13outputs">outputs</dfn>) final {</td></tr>
<tr><th id="91">91</th><td>    Scalar <dfn class="local col4 decl" id="14sign" title='sign' data-type='Scalar' data-ref="14sign">sign</dfn>;</td></tr>
<tr><th id="92">92</th><td>    <em>const</em> <a class="typedef" href="#80" title='tensorflow::LogDeterminantOp::RealScalar' data-type='typename Eigen::NumTraits&lt;Scalar&gt;::Real' data-ref="tensorflow::LogDeterminantOp::RealScalar">RealScalar</a> <dfn class="local col5 decl" id="15log_abs_det" title='log_abs_det' data-type='const RealScalar' data-ref="15log_abs_det">log_abs_det</dfn> = SLogDet(</td></tr>
<tr><th id="93">93</th><td>        <span class="namespace">Eigen::</span><span class='type' title='Eigen::Matrix' data-ref="Eigen::Matrix">Matrix</span>&lt;Scalar, <span class="namespace">Eigen::</span><span class='ref' title='Eigen::Dynamic' data-ref="Eigen::Dynamic">Dynamic</span>, <span class="namespace">Eigen::</span><span class='ref' title='Eigen::Dynamic' data-ref="Eigen::Dynamic">Dynamic</span>&gt;(<a class="local col2 ref" href="#12inputs" title='inputs' data-ref="12inputs">inputs</a>[<var>0</var>]),</td></tr>
<tr><th id="94">94</th><td>        &amp;<a class="local col4 ref" href="#14sign" title='sign' data-ref="14sign">sign</a>);</td></tr>
<tr><th id="95">95</th><td></td></tr>
<tr><th id="96">96</th><td>    <a class="local col3 ref" href="#13outputs" title='outputs' data-ref="13outputs">outputs</a>-&gt;at(<var>0</var>)(<var>0</var>, <var>0</var>) = <a class="local col4 ref" href="#14sign" title='sign' data-ref="14sign">sign</a>;</td></tr>
<tr><th id="97">97</th><td>    <a class="local col3 ref" href="#13outputs" title='outputs' data-ref="13outputs">outputs</a>-&gt;at(<var>1</var>)(<var>0</var>, <var>0</var>) = <a class="local col5 ref" href="#15log_abs_det" title='log_abs_det' data-ref="15log_abs_det">log_abs_det</a>;</td></tr>
<tr><th id="98">98</th><td>  }</td></tr>
<tr><th id="99">99</th><td>};</td></tr>
<tr><th id="100">100</th><td></td></tr>
<tr><th id="101">101</th><td><b>template</b> &lt;<b>class</b> Scalar&gt;</td></tr>
<tr><th id="102">102</th><td><b>class</b> <dfn class="type def" id="tensorflow::DeterminantOp" title='tensorflow::DeterminantOp' data-ref="tensorflow::DeterminantOp">DeterminantOp</dfn> : <b>public</b> <a class="type" href="linalg_ops_common.h.html#tensorflow::LinearAlgebraOp" title='tensorflow::LinearAlgebraOp' data-ref="tensorflow::LinearAlgebraOp">LinearAlgebraOp</a>&lt;Scalar&gt; {</td></tr>
<tr><th id="103">103</th><td> <b>public</b>:</td></tr>
<tr><th id="104">104</th><td>  <a class="macro" href="linalg_ops_common.h.html#175" title="typedef LinearAlgebraOp&lt;Scalar&gt; Base; using RealScalar = typename Eigen::NumTraits&lt;Scalar&gt;::Real; using Matrix = typename Base::Matrix; using MatrixMap = typename Base::MatrixMap; using MatrixMaps = typename Base::MatrixMaps; using ConstMatrixMap = typename Base::ConstMatrixMap; using ConstMatrixMaps = typename Base::ConstMatrixMaps; using TensorShapes = typename Base::TensorShapes;" data-ref="_M/INHERIT_LINALG_TYPEDEFS">INHERIT_LINALG_TYPEDEFS</a>(Scalar);</td></tr>
<tr><th id="105">105</th><td></td></tr>
<tr><th id="106">106</th><td>  <b>explicit</b> <dfn class="tu decl def" id="_ZN10tensorflow13DeterminantOpC1EPNS_20OpKernelConstructionE" title='tensorflow::DeterminantOp::DeterminantOp&lt;Scalar&gt;' data-type='void tensorflow::DeterminantOp::DeterminantOp&lt;Scalar&gt;(tensorflow::OpKernelConstruction * context)' data-ref="_ZN10tensorflow13DeterminantOpC1EPNS_20OpKernelConstructionE">DeterminantOp</dfn>(<a class="type" href="../framework/op_kernel.h.html#tensorflow::OpKernelConstruction" title='tensorflow::OpKernelConstruction' data-ref="tensorflow::OpKernelConstruction">OpKernelConstruction</a>* <dfn class="local col6 decl" id="16context" title='context' data-type='tensorflow::OpKernelConstruction *' data-ref="16context">context</dfn>) : <a class="typedef" href="#104" title='tensorflow::DeterminantOp::Base' data-type='LinearAlgebraOp&lt;Scalar&gt;' data-ref="tensorflow::DeterminantOp::Base">Base</a>(<a class="local col6 ref" href="#16context" title='context' data-ref="16context">context</a>) {}</td></tr>
<tr><th id="107">107</th><td></td></tr>
<tr><th id="108">108</th><td>  <a class="typedef" href="#104" title='tensorflow::DeterminantOp::TensorShapes' data-type='typename Base::TensorShapes' data-ref="tensorflow::DeterminantOp::TensorShapes">TensorShapes</a> <dfn class="tu decl def" id="_ZNK10tensorflow13DeterminantOp21GetOutputMatrixShapesERKNS_15LinearAlgebraOpIT_E12TensorShapesE" title='tensorflow::DeterminantOp::GetOutputMatrixShapes' data-type='TensorShapes tensorflow::DeterminantOp::GetOutputMatrixShapes(const TensorShapes &amp; input_matrix_shape) const' data-ref="_ZNK10tensorflow13DeterminantOp21GetOutputMatrixShapesERKNS_15LinearAlgebraOpIT_E12TensorShapesE">GetOutputMatrixShapes</dfn>(</td></tr>
<tr><th id="109">109</th><td>      <em>const</em> <a class="typedef" href="#104" title='tensorflow::DeterminantOp::TensorShapes' data-type='typename Base::TensorShapes' data-ref="tensorflow::DeterminantOp::TensorShapes">TensorShapes</a>&amp; <dfn class="local col7 decl" id="17input_matrix_shape" title='input_matrix_shape' data-type='const TensorShapes &amp;' data-ref="17input_matrix_shape">input_matrix_shape</dfn>) <em>const</em> final {</td></tr>
<tr><th id="110">110</th><td>    <b>return</b> <a class="typedef" href="#104" title='tensorflow::DeterminantOp::TensorShapes' data-type='typename Base::TensorShapes' data-ref="tensorflow::DeterminantOp::TensorShapes">TensorShapes</a>({<a class="type" href="../framework/tensor_shape.h.html#tensorflow::TensorShape" title='tensorflow::TensorShape' data-ref="tensorflow::TensorShape">TensorShape</a><a class="ref" href="../framework/tensor_shape.h.html#291" title='tensorflow::TensorShape::TensorShape' data-ref="_ZN10tensorflow11TensorShapeC1ESt16initializer_listIxE">(</a><a class="ref" href="../../../../include/c++/5/initializer_list.html#_ZNSt16initializer_listC1Ev" title='std::initializer_list::initializer_list&lt;_E&gt;' data-ref="_ZNSt16initializer_listC1Ev">{</a>})});</td></tr>
<tr><th id="111">111</th><td>  }</td></tr>
<tr><th id="112">112</th><td></td></tr>
<tr><th id="113">113</th><td>  <em>void</em> <dfn class="tu decl def" id="_ZN10tensorflow13DeterminantOp13ComputeMatrixEPNS_15OpKernelContextERKNS_15LinearAlgebraOpIT_E15ConstMatrixMapsEPNS5_10MatrixMapsE" title='tensorflow::DeterminantOp::ComputeMatrix' data-type='void tensorflow::DeterminantOp::ComputeMatrix(tensorflow::OpKernelContext * context, const ConstMatrixMaps &amp; inputs, MatrixMaps * outputs)' data-ref="_ZN10tensorflow13DeterminantOp13ComputeMatrixEPNS_15OpKernelContextERKNS_15LinearAlgebraOpIT_E15ConstMatrixMapsEPNS5_10MatrixMapsE">ComputeMatrix</dfn>(<a class="type" href="../framework/op_kernel.h.html#tensorflow::OpKernelContext" title='tensorflow::OpKernelContext' data-ref="tensorflow::OpKernelContext">OpKernelContext</a>* <dfn class="local col8 decl" id="18context" title='context' data-type='tensorflow::OpKernelContext *' data-ref="18context">context</dfn>, <em>const</em> <a class="typedef" href="#104" title='tensorflow::DeterminantOp::ConstMatrixMaps' data-type='typename Base::ConstMatrixMaps' data-ref="tensorflow::DeterminantOp::ConstMatrixMaps">ConstMatrixMaps</a>&amp; <dfn class="local col9 decl" id="19inputs" title='inputs' data-type='const ConstMatrixMaps &amp;' data-ref="19inputs">inputs</dfn>,</td></tr>
<tr><th id="114">114</th><td>                     <a class="typedef" href="#104" title='tensorflow::DeterminantOp::MatrixMaps' data-type='typename Base::MatrixMaps' data-ref="tensorflow::DeterminantOp::MatrixMaps">MatrixMaps</a>* <dfn class="local col0 decl" id="20outputs" title='outputs' data-type='MatrixMaps *' data-ref="20outputs">outputs</dfn>) final {</td></tr>
<tr><th id="115">115</th><td>    Scalar <dfn class="local col1 decl" id="21sign" title='sign' data-type='Scalar' data-ref="21sign">sign</dfn>;</td></tr>
<tr><th id="116">116</th><td>    <em>const</em> <a class="typedef" href="#104" title='tensorflow::DeterminantOp::RealScalar' data-type='typename Eigen::NumTraits&lt;Scalar&gt;::Real' data-ref="tensorflow::DeterminantOp::RealScalar">RealScalar</a> <dfn class="local col2 decl" id="22log_abs_det" title='log_abs_det' data-type='const RealScalar' data-ref="22log_abs_det">log_abs_det</dfn> = SLogDet(</td></tr>
<tr><th id="117">117</th><td>        <span class="namespace">Eigen::</span><span class='type' title='Eigen::Matrix' data-ref="Eigen::Matrix">Matrix</span>&lt;Scalar, <span class="namespace">Eigen::</span><span class='ref' title='Eigen::Dynamic' data-ref="Eigen::Dynamic">Dynamic</span>, <span class="namespace">Eigen::</span><span class='ref' title='Eigen::Dynamic' data-ref="Eigen::Dynamic">Dynamic</span>&gt;(<a class="local col9 ref" href="#19inputs" title='inputs' data-ref="19inputs">inputs</a>[<var>0</var>]),</td></tr>
<tr><th id="118">118</th><td>        &amp;<a class="local col1 ref" href="#21sign" title='sign' data-ref="21sign">sign</a>);</td></tr>
<tr><th id="119">119</th><td>    <a class="local col0 ref" href="#20outputs" title='outputs' data-ref="20outputs">outputs</a>-&gt;at(<var>0</var>)(<var>0</var>, <var>0</var>) = <a class="local col1 ref" href="#21sign" title='sign' data-ref="21sign">sign</a> * <span class="namespace">std::</span>exp(<a class="local col2 ref" href="#22log_abs_det" title='log_abs_det' data-ref="22log_abs_det">log_abs_det</a>);</td></tr>
<tr><th id="120">120</th><td>  }</td></tr>
<tr><th id="121">121</th><td>};</td></tr>
<tr><th id="122">122</th><td></td></tr>
<tr><th id="123">123</th><td><u>#<span data-ppcond="123">if</span> GOOGLE_CUDA</u></td></tr>
<tr><th id="124">124</th><td></td></tr>
<tr><th id="125">125</th><td><b>typedef</b> Eigen::GpuDevice GPUDevice;</td></tr>
<tr><th id="126">126</th><td></td></tr>
<tr><th id="127">127</th><td><b>template</b> &lt;<b>class</b> Scalar&gt;</td></tr>
<tr><th id="128">128</th><td><b>class</b> DeterminantOpGpu : <b>public</b> AsyncOpKernel {</td></tr>
<tr><th id="129">129</th><td> <b>public</b>:</td></tr>
<tr><th id="130">130</th><td>  <b>explicit</b> DeterminantOpGpu(OpKernelConstruction* context)</td></tr>
<tr><th id="131">131</th><td>      : AsyncOpKernel(context) {}</td></tr>
<tr><th id="132">132</th><td></td></tr>
<tr><th id="133">133</th><td>  <em>void</em> ComputeAsync(OpKernelContext* context, DoneCallback done) final {</td></tr>
<tr><th id="134">134</th><td>    <em>const</em> Tensor&amp; input = context-&gt;input(<var>0</var>);</td></tr>
<tr><th id="135">135</th><td>    <em>const</em> <em>int</em> ndims = input.dims();</td></tr>
<tr><th id="136">136</th><td>    <em>const</em> int64 n = input.dim_size(ndims - <var>1</var>);</td></tr>
<tr><th id="137">137</th><td>    <i>// Validate inputs.</i></td></tr>
<tr><th id="138">138</th><td>    OP_REQUIRES_ASYNC(</td></tr>
<tr><th id="139">139</th><td>        context, ndims &gt;= <var>2</var>,</td></tr>
<tr><th id="140">140</th><td>        errors::InvalidArgument(<q>"Input must have rank &gt;= 2, got "</q>, ndims),</td></tr>
<tr><th id="141">141</th><td>        done);</td></tr>
<tr><th id="142">142</th><td>    OP_REQUIRES_ASYNC(</td></tr>
<tr><th id="143">143</th><td>        context, input.dim_size(ndims - <var>2</var>) == n,</td></tr>
<tr><th id="144">144</th><td>        errors::InvalidArgument(<q>"Input matrices must be square, got"</q>,</td></tr>
<tr><th id="145">145</th><td>                                input.dim_size(ndims - <var>2</var>), <q>" != "</q>, n),</td></tr>
<tr><th id="146">146</th><td>        done);</td></tr>
<tr><th id="147">147</th><td></td></tr>
<tr><th id="148">148</th><td>    <i>// Allocate output.</i></td></tr>
<tr><th id="149">149</th><td>    TensorShape out_shape;</td></tr>
<tr><th id="150">150</th><td>    <b>for</b> (<em>int</em> dim = <var>0</var>; dim &lt; ndims - <var>2</var>; ++dim) {</td></tr>
<tr><th id="151">151</th><td>      out_shape.AddDim(input.dim_size(dim));</td></tr>
<tr><th id="152">152</th><td>    }</td></tr>
<tr><th id="153">153</th><td>    out_shape.AppendShape(TensorShape({}));</td></tr>
<tr><th id="154">154</th><td>    Tensor* out;</td></tr>
<tr><th id="155">155</th><td>    OP_REQUIRES_OK_ASYNC(context, context-&gt;allocate_output(<var>0</var>, out_shape, &amp;out),</td></tr>
<tr><th id="156">156</th><td>                         done);</td></tr>
<tr><th id="157">157</th><td></td></tr>
<tr><th id="158">158</th><td>    <i>// By definition, the determinant of an empty matrix is equal to one.</i></td></tr>
<tr><th id="159">159</th><td>    <em>const</em> GPUDevice&amp; d = context-&gt;eigen_device&lt;GPUDevice&gt;();</td></tr>
<tr><th id="160">160</th><td>    <b>if</b> (input.NumElements() == <var>0</var>) {</td></tr>
<tr><th id="161">161</th><td>      functor::SetOneFunctor&lt;GPUDevice, Scalar&gt; f;</td></tr>
<tr><th id="162">162</th><td>      f(d, out-&gt;<b>template</b> flat&lt;Scalar&gt;());</td></tr>
<tr><th id="163">163</th><td>      done();</td></tr>
<tr><th id="164">164</th><td>      <b>return</b>;</td></tr>
<tr><th id="165">165</th><td>    }</td></tr>
<tr><th id="166">166</th><td></td></tr>
<tr><th id="167">167</th><td>    <i>// TODO(rmlarsen): Convert to absl::make_unique when available.</i></td></tr>
<tr><th id="168">168</th><td>    std::unique_ptr&lt;CudaSolver&gt; solver(<b>new</b> CudaSolver(context));</td></tr>
<tr><th id="169">169</th><td></td></tr>
<tr><th id="170">170</th><td>    <i>// Reuse the input buffer or make a copy for the factorization step,</i></td></tr>
<tr><th id="171">171</th><td><i>    // depending on whether this ops owns it exclusively.</i></td></tr>
<tr><th id="172">172</th><td>    Tensor input_copy;</td></tr>
<tr><th id="173">173</th><td>    OP_REQUIRES_OK_ASYNC(</td></tr>
<tr><th id="174">174</th><td>        context,</td></tr>
<tr><th id="175">175</th><td>        solver-&gt;forward_input_or_allocate_scoped_tensor(</td></tr>
<tr><th id="176">176</th><td>            {<var>0</var>}, DataTypeToEnum&lt;Scalar&gt;::value, input.shape(), &amp;input_copy),</td></tr>
<tr><th id="177">177</th><td>        done);</td></tr>
<tr><th id="178">178</th><td>    <b>if</b> (!input.SharesBufferWith(input_copy)) {</td></tr>
<tr><th id="179">179</th><td>      d.memcpy(input_copy.flat&lt;Scalar&gt;().data(), input.flat&lt;Scalar&gt;().data(),</td></tr>
<tr><th id="180">180</th><td>               input.NumElements() * <b>sizeof</b>(Scalar));</td></tr>
<tr><th id="181">181</th><td>    }</td></tr>
<tr><th id="182">182</th><td>    <em>auto</em> input_copy_reshaped = input_copy.<b>template</b> flat_inner_dims&lt;Scalar, <var>3</var>&gt;();</td></tr>
<tr><th id="183">183</th><td>    <em>const</em> int64 batch_size = input_copy_reshaped.dimension(<var>0</var>);</td></tr>
<tr><th id="184">184</th><td></td></tr>
<tr><th id="185">185</th><td>    <i>// Allocate pivots on the device.</i></td></tr>
<tr><th id="186">186</th><td>    Tensor pivots;</td></tr>
<tr><th id="187">187</th><td>    OP_REQUIRES_OK_ASYNC(</td></tr>
<tr><th id="188">188</th><td>        context,</td></tr>
<tr><th id="189">189</th><td>        solver-&gt;allocate_scoped_tensor(DataTypeToEnum&lt;<em>int</em>&gt;::value,</td></tr>
<tr><th id="190">190</th><td>                                       TensorShape{batch_size, n}, &amp;pivots),</td></tr>
<tr><th id="191">191</th><td>        done);</td></tr>
<tr><th id="192">192</th><td>    <em>auto</em> pivots_mat = pivots.<b>template</b> matrix&lt;<em>int</em>&gt;();</td></tr>
<tr><th id="193">193</th><td></td></tr>
<tr><th id="194">194</th><td>    <i>// Prepare pointer arrays for cuBlas' batch interface.</i></td></tr>
<tr><th id="195">195</th><td><i>    // TODO(rmlarsen): Find a way to encode pointer arrays in pinned host memory</i></td></tr>
<tr><th id="196">196</th><td><i>    // without the ugly casting.</i></td></tr>
<tr><th id="197">197</th><td>    <em>auto</em> input_copy_ptrs = solver-&gt;GetScratchSpace&lt;uint8&gt;(</td></tr>
<tr><th id="198">198</th><td>        <b>sizeof</b>(Scalar*) * batch_size, <q>"input_copy_ptrs"</q>,</td></tr>
<tr><th id="199">199</th><td>        <i>/* on_host */</i> <b>true</b>);</td></tr>
<tr><th id="200">200</th><td>    <em>auto</em> output_reshaped = out-&gt;<b>template</b> flat_inner_dims&lt;Scalar, <var>1</var>&gt;();</td></tr>
<tr><th id="201">201</th><td></td></tr>
<tr><th id="202">202</th><td>    <i>// Compute the partially pivoted LU factorization(s) of the matrix/matrices.</i></td></tr>
<tr><th id="203">203</th><td>    std::vector&lt;DeviceLapackInfo&gt; dev_info;</td></tr>
<tr><th id="204">204</th><td>    <b>if</b> (n / batch_size &lt;= <var>128</var>) {</td></tr>
<tr><th id="205">205</th><td>      <i>// For small matrices or large batch sizes, we use the batched interface</i></td></tr>
<tr><th id="206">206</th><td><i>      // from cuBlas.</i></td></tr>
<tr><th id="207">207</th><td>      <em>const</em> Scalar** input_copy_ptrs_base =</td></tr>
<tr><th id="208">208</th><td>          <b>reinterpret_cast</b>&lt;<em>const</em> Scalar**&gt;(input_copy_ptrs.mutable_data());</td></tr>
<tr><th id="209">209</th><td>      <b>for</b> (<em>int</em> batch = <var>0</var>; batch &lt; batch_size; ++batch) {</td></tr>
<tr><th id="210">210</th><td>        input_copy_ptrs_base[batch] = &amp;input_copy_reshaped(batch, <var>0</var>, <var>0</var>);</td></tr>
<tr><th id="211">211</th><td>      }</td></tr>
<tr><th id="212">212</th><td>      dev_info.push_back(</td></tr>
<tr><th id="213">213</th><td>          solver-&gt;GetDeviceLapackInfo(batch_size, <q>"getrfBatched"</q>));</td></tr>
<tr><th id="214">214</th><td>      OP_REQUIRES_OK_ASYNC(</td></tr>
<tr><th id="215">215</th><td>          context,</td></tr>
<tr><th id="216">216</th><td>          solver-&gt;GetrfBatched(n, input_copy_ptrs_base, n, pivots_mat.data(),</td></tr>
<tr><th id="217">217</th><td>                               &amp;dev_info.back(), batch_size),</td></tr>
<tr><th id="218">218</th><td>          done);</td></tr>
<tr><th id="219">219</th><td>    } <b>else</b> {</td></tr>
<tr><th id="220">220</th><td>      <i>// For small batch sizes we use the non-batched interface from cuSolver,</i></td></tr>
<tr><th id="221">221</th><td><i>      // which is much faster for large matrices.</i></td></tr>
<tr><th id="222">222</th><td>      dev_info.push_back(solver-&gt;GetDeviceLapackInfo(batch_size, <q>"getrf"</q>));</td></tr>
<tr><th id="223">223</th><td>      <b>for</b> (<em>int</em> batch = <var>0</var>; batch &lt; batch_size; ++batch) {</td></tr>
<tr><th id="224">224</th><td>        OP_REQUIRES_OK_ASYNC(</td></tr>
<tr><th id="225">225</th><td>            context,</td></tr>
<tr><th id="226">226</th><td>            solver-&gt;Getrf(n, n, &amp;input_copy_reshaped(batch, <var>0</var>, <var>0</var>), n,</td></tr>
<tr><th id="227">227</th><td>                          &amp;pivots_mat(batch, <var>0</var>), &amp;dev_info.back()(batch)),</td></tr>
<tr><th id="228">228</th><td>            done);</td></tr>
<tr><th id="229">229</th><td>      }</td></tr>
<tr><th id="230">230</th><td>    }</td></tr>
<tr><th id="231">231</th><td></td></tr>
<tr><th id="232">232</th><td>    <i>// Compute the determinant for each batch as (-1)^s * prod(diag(U)),</i></td></tr>
<tr><th id="233">233</th><td><i>    // where s is the order of the permutation encoded in pivots and U is the</i></td></tr>
<tr><th id="234">234</th><td><i>    // upper triangular factor of the LU factorization, which is written to</i></td></tr>
<tr><th id="235">235</th><td><i>    // input_copy by the Getrf{Batched} kernel.</i></td></tr>
<tr><th id="236">236</th><td>    functor::DeterminantFromPivotedLUFunctor&lt;GPUDevice, Scalar&gt; functor;</td></tr>
<tr><th id="237">237</th><td>    functor(d,</td></tr>
<tr><th id="238">238</th><td>            <b>const_cast</b>&lt;<em>const</em> Tensor*&gt;(&amp;input_copy)</td></tr>
<tr><th id="239">239</th><td>                -&gt;<b>template</b> flat_inner_dims&lt;Scalar, <var>3</var>&gt;(),</td></tr>
<tr><th id="240">240</th><td>            pivots_mat.data(), output_reshaped, dev_info.back().mutable_data());</td></tr>
<tr><th id="241">241</th><td></td></tr>
<tr><th id="242">242</th><td>    <i>// Register callback to check info after kernels finish.</i></td></tr>
<tr><th id="243">243</th><td>    <em>auto</em> info_checker = [context, done](</td></tr>
<tr><th id="244">244</th><td>                            <em>const</em> Status&amp; status,</td></tr>
<tr><th id="245">245</th><td>                            <em>const</em> std::vector&lt;HostLapackInfo&gt;&amp; host_infos) {</td></tr>
<tr><th id="246">246</th><td>      <b>if</b> (!status.ok() &amp;&amp; errors::IsInvalidArgument(status) &amp;&amp;</td></tr>
<tr><th id="247">247</th><td>          !host_infos.empty()) {</td></tr>
<tr><th id="248">248</th><td>        <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; host_infos[<var>0</var>].size(); ++i) {</td></tr>
<tr><th id="249">249</th><td>          <i>// It is OK for a matrix to be singular (signaled by info &gt; 0),</i></td></tr>
<tr><th id="250">250</th><td><i>          // corresponding to determinant of zero, but we do want to catch</i></td></tr>
<tr><th id="251">251</th><td><i>          // invalid arguments to Getrf{Batched}.</i></td></tr>
<tr><th id="252">252</th><td>          OP_REQUIRES_ASYNC(</td></tr>
<tr><th id="253">253</th><td>              context, host_infos[<var>0</var>](i) &gt;= <var>0</var>,</td></tr>
<tr><th id="254">254</th><td>              errors::InvalidArgument(<q>"Invalid input argument no. "</q>,</td></tr>
<tr><th id="255">255</th><td>                                      host_infos[<var>0</var>].data()[i],</td></tr>
<tr><th id="256">256</th><td>                                      <q>" for batch index "</q>, i, <q>"."</q>),</td></tr>
<tr><th id="257">257</th><td>              done);</td></tr>
<tr><th id="258">258</th><td>        }</td></tr>
<tr><th id="259">259</th><td>      }</td></tr>
<tr><th id="260">260</th><td>      done();</td></tr>
<tr><th id="261">261</th><td>    };</td></tr>
<tr><th id="262">262</th><td>    CudaSolver::CheckLapackInfoAndDeleteSolverAsync(std::move(solver), dev_info,</td></tr>
<tr><th id="263">263</th><td>                                                    std::move(info_checker));</td></tr>
<tr><th id="264">264</th><td>  }</td></tr>
<tr><th id="265">265</th><td>};</td></tr>
<tr><th id="266">266</th><td></td></tr>
<tr><th id="267">267</th><td><b>template</b> &lt;<b>class</b> Scalar&gt;</td></tr>
<tr><th id="268">268</th><td><b>class</b> LogDeterminantOpGpu : <b>public</b> AsyncOpKernel {</td></tr>
<tr><th id="269">269</th><td> <b>public</b>:</td></tr>
<tr><th id="270">270</th><td>  <b>explicit</b> LogDeterminantOpGpu(OpKernelConstruction* context)</td></tr>
<tr><th id="271">271</th><td>      : AsyncOpKernel(context) {}</td></tr>
<tr><th id="272">272</th><td></td></tr>
<tr><th id="273">273</th><td>  <em>void</em> ComputeAsync(OpKernelContext* context, DoneCallback done) final {</td></tr>
<tr><th id="274">274</th><td>    <em>const</em> Tensor&amp; input = context-&gt;input(<var>0</var>);</td></tr>
<tr><th id="275">275</th><td>    <em>const</em> <em>int</em> ndims = input.dims();</td></tr>
<tr><th id="276">276</th><td>    <em>const</em> int64 n = input.dim_size(ndims - <var>1</var>);</td></tr>
<tr><th id="277">277</th><td>    <i>// Validate inputs.</i></td></tr>
<tr><th id="278">278</th><td>    OP_REQUIRES_ASYNC(</td></tr>
<tr><th id="279">279</th><td>        context, ndims &gt;= <var>2</var>,</td></tr>
<tr><th id="280">280</th><td>        errors::InvalidArgument(<q>"Input must have rank &gt;= 2, got "</q>, ndims),</td></tr>
<tr><th id="281">281</th><td>        done);</td></tr>
<tr><th id="282">282</th><td>    OP_REQUIRES_ASYNC(</td></tr>
<tr><th id="283">283</th><td>        context, input.dim_size(ndims - <var>2</var>) == n,</td></tr>
<tr><th id="284">284</th><td>        errors::InvalidArgument(<q>"Input matrices must be square, got"</q>,</td></tr>
<tr><th id="285">285</th><td>                                input.dim_size(ndims - <var>2</var>), <q>" != "</q>, n),</td></tr>
<tr><th id="286">286</th><td>        done);</td></tr>
<tr><th id="287">287</th><td></td></tr>
<tr><th id="288">288</th><td>    <i>// Allocate output.</i></td></tr>
<tr><th id="289">289</th><td>    TensorShape out_shape;</td></tr>
<tr><th id="290">290</th><td>    <b>for</b> (<em>int</em> dim = <var>0</var>; dim &lt; ndims - <var>2</var>; ++dim) {</td></tr>
<tr><th id="291">291</th><td>      out_shape.AddDim(input.dim_size(dim));</td></tr>
<tr><th id="292">292</th><td>    }</td></tr>
<tr><th id="293">293</th><td>    out_shape.AppendShape(TensorShape({}));</td></tr>
<tr><th id="294">294</th><td>    Tensor* sign;</td></tr>
<tr><th id="295">295</th><td>    OP_REQUIRES_OK_ASYNC(context, context-&gt;allocate_output(<var>0</var>, out_shape, &amp;sign),</td></tr>
<tr><th id="296">296</th><td>                         done);</td></tr>
<tr><th id="297">297</th><td>    Tensor* log_abs_det;</td></tr>
<tr><th id="298">298</th><td>    OP_REQUIRES_OK_ASYNC(</td></tr>
<tr><th id="299">299</th><td>        context, context-&gt;allocate_output(<var>1</var>, out_shape, &amp;log_abs_det), done);</td></tr>
<tr><th id="300">300</th><td></td></tr>
<tr><th id="301">301</th><td>    <i>// By definition, the determinant of an empty matrix is equal to one.</i></td></tr>
<tr><th id="302">302</th><td>    <em>const</em> GPUDevice&amp; d = context-&gt;eigen_device&lt;GPUDevice&gt;();</td></tr>
<tr><th id="303">303</th><td>    <b>if</b> (input.NumElements() == <var>0</var>) {</td></tr>
<tr><th id="304">304</th><td>      functor::SetOneFunctor&lt;GPUDevice, Scalar&gt; one_func;</td></tr>
<tr><th id="305">305</th><td>      one_func(d, sign-&gt;<b>template</b> flat&lt;Scalar&gt;());</td></tr>
<tr><th id="306">306</th><td>      functor::SetZeroFunctor&lt;GPUDevice, Scalar&gt; zero_func;</td></tr>
<tr><th id="307">307</th><td>      zero_func(d, log_abs_det-&gt;<b>template</b> flat&lt;Scalar&gt;());</td></tr>
<tr><th id="308">308</th><td>      done();</td></tr>
<tr><th id="309">309</th><td>      <b>return</b>;</td></tr>
<tr><th id="310">310</th><td>    }</td></tr>
<tr><th id="311">311</th><td></td></tr>
<tr><th id="312">312</th><td>    <i>// TODO(rmlarsen): Convert to absl::make_unique when available.</i></td></tr>
<tr><th id="313">313</th><td>    std::unique_ptr&lt;CudaSolver&gt; solver(<b>new</b> CudaSolver(context));</td></tr>
<tr><th id="314">314</th><td></td></tr>
<tr><th id="315">315</th><td>    <i>// Reuse the input buffer or make a copy for the factorization step,</i></td></tr>
<tr><th id="316">316</th><td><i>    // depending on whether this ops owns it exclusively.</i></td></tr>
<tr><th id="317">317</th><td>    Tensor input_copy;</td></tr>
<tr><th id="318">318</th><td>    OP_REQUIRES_OK_ASYNC(</td></tr>
<tr><th id="319">319</th><td>        context,</td></tr>
<tr><th id="320">320</th><td>        solver-&gt;forward_input_or_allocate_scoped_tensor(</td></tr>
<tr><th id="321">321</th><td>            {<var>0</var>}, DataTypeToEnum&lt;Scalar&gt;::value, input.shape(), &amp;input_copy),</td></tr>
<tr><th id="322">322</th><td>        done);</td></tr>
<tr><th id="323">323</th><td>    <b>if</b> (!input.SharesBufferWith(input_copy)) {</td></tr>
<tr><th id="324">324</th><td>      d.memcpy(input_copy.flat&lt;Scalar&gt;().data(), input.flat&lt;Scalar&gt;().data(),</td></tr>
<tr><th id="325">325</th><td>               input.NumElements() * <b>sizeof</b>(Scalar));</td></tr>
<tr><th id="326">326</th><td>    }</td></tr>
<tr><th id="327">327</th><td>    <em>auto</em> input_copy_reshaped = input_copy.<b>template</b> flat_inner_dims&lt;Scalar, <var>3</var>&gt;();</td></tr>
<tr><th id="328">328</th><td>    <em>const</em> int64 batch_size = input_copy_reshaped.dimension(<var>0</var>);</td></tr>
<tr><th id="329">329</th><td></td></tr>
<tr><th id="330">330</th><td>    <i>// Allocate pivots on the device.</i></td></tr>
<tr><th id="331">331</th><td>    Tensor pivots;</td></tr>
<tr><th id="332">332</th><td>    OP_REQUIRES_OK_ASYNC(</td></tr>
<tr><th id="333">333</th><td>        context,</td></tr>
<tr><th id="334">334</th><td>        solver-&gt;allocate_scoped_tensor(DataTypeToEnum&lt;<em>int</em>&gt;::value,</td></tr>
<tr><th id="335">335</th><td>                                       TensorShape{batch_size, n}, &amp;pivots),</td></tr>
<tr><th id="336">336</th><td>        done);</td></tr>
<tr><th id="337">337</th><td>    <em>auto</em> pivots_mat = pivots.<b>template</b> matrix&lt;<em>int</em>&gt;();</td></tr>
<tr><th id="338">338</th><td></td></tr>
<tr><th id="339">339</th><td>    <i>// Prepare pointer arrays for cuBlas' batch interface.</i></td></tr>
<tr><th id="340">340</th><td><i>    // TODO(rmlarsen): Find a way to encode pointer arrays in pinned host memory</i></td></tr>
<tr><th id="341">341</th><td><i>    // without the ugly casting.</i></td></tr>
<tr><th id="342">342</th><td>    <em>auto</em> input_copy_ptrs = solver-&gt;GetScratchSpace&lt;uint8&gt;(</td></tr>
<tr><th id="343">343</th><td>        <b>sizeof</b>(Scalar*) * batch_size, <q>"input_copy_ptrs"</q>,</td></tr>
<tr><th id="344">344</th><td>        <i>/* on_host */</i> <b>true</b>);</td></tr>
<tr><th id="345">345</th><td></td></tr>
<tr><th id="346">346</th><td>    <i>// Compute the partially pivoted LU factorization(s) of the matrix/matrices.</i></td></tr>
<tr><th id="347">347</th><td>    std::vector&lt;DeviceLapackInfo&gt; dev_info;</td></tr>
<tr><th id="348">348</th><td>    <b>if</b> (n / batch_size &lt;= <var>128</var>) {</td></tr>
<tr><th id="349">349</th><td>      <i>// For small matrices or large batch sizes, we use the batched interface</i></td></tr>
<tr><th id="350">350</th><td><i>      // from cuBlas.</i></td></tr>
<tr><th id="351">351</th><td>      <em>const</em> Scalar** input_copy_ptrs_base =</td></tr>
<tr><th id="352">352</th><td>          <b>reinterpret_cast</b>&lt;<em>const</em> Scalar**&gt;(input_copy_ptrs.mutable_data());</td></tr>
<tr><th id="353">353</th><td>      <b>for</b> (<em>int</em> batch = <var>0</var>; batch &lt; batch_size; ++batch) {</td></tr>
<tr><th id="354">354</th><td>        input_copy_ptrs_base[batch] = &amp;input_copy_reshaped(batch, <var>0</var>, <var>0</var>);</td></tr>
<tr><th id="355">355</th><td>      }</td></tr>
<tr><th id="356">356</th><td>      dev_info.push_back(</td></tr>
<tr><th id="357">357</th><td>          solver-&gt;GetDeviceLapackInfo(batch_size, <q>"getrfBatched"</q>));</td></tr>
<tr><th id="358">358</th><td>      OP_REQUIRES_OK_ASYNC(</td></tr>
<tr><th id="359">359</th><td>          context,</td></tr>
<tr><th id="360">360</th><td>          solver-&gt;GetrfBatched(n, input_copy_ptrs_base, n, pivots_mat.data(),</td></tr>
<tr><th id="361">361</th><td>                               &amp;dev_info.back(), batch_size),</td></tr>
<tr><th id="362">362</th><td>          done);</td></tr>
<tr><th id="363">363</th><td>    } <b>else</b> {</td></tr>
<tr><th id="364">364</th><td>      <i>// For large matrices or small batch sizes we use the non-batched</i></td></tr>
<tr><th id="365">365</th><td><i>      // interface from cuSolver, which is much faster for large matrices.</i></td></tr>
<tr><th id="366">366</th><td>      dev_info.push_back(solver-&gt;GetDeviceLapackInfo(batch_size, <q>"getrf"</q>));</td></tr>
<tr><th id="367">367</th><td>      <b>for</b> (<em>int</em> batch = <var>0</var>; batch &lt; batch_size; ++batch) {</td></tr>
<tr><th id="368">368</th><td>        OP_REQUIRES_OK_ASYNC(</td></tr>
<tr><th id="369">369</th><td>            context,</td></tr>
<tr><th id="370">370</th><td>            solver-&gt;Getrf(n, n, &amp;input_copy_reshaped(batch, <var>0</var>, <var>0</var>), n,</td></tr>
<tr><th id="371">371</th><td>                          &amp;pivots_mat(batch, <var>0</var>), &amp;dev_info.back()(batch)),</td></tr>
<tr><th id="372">372</th><td>            done);</td></tr>
<tr><th id="373">373</th><td>      }</td></tr>
<tr><th id="374">374</th><td>    }</td></tr>
<tr><th id="375">375</th><td></td></tr>
<tr><th id="376">376</th><td>    <em>auto</em> input_copy_reshaped_const =</td></tr>
<tr><th id="377">377</th><td>        <b>const_cast</b>&lt;<em>const</em> Tensor*&gt;(&amp;input_copy)</td></tr>
<tr><th id="378">378</th><td>            -&gt;<b>template</b> flat_inner_dims&lt;Scalar, <var>3</var>&gt;();</td></tr>
<tr><th id="379">379</th><td>    <em>auto</em> sign_reshaped = sign-&gt;flat&lt;Scalar&gt;();</td></tr>
<tr><th id="380">380</th><td>    <em>auto</em> log_abs_det_reshaped = log_abs_det-&gt;flat&lt;Scalar&gt;();</td></tr>
<tr><th id="381">381</th><td>    <i>// Compute the determinant for each batch as (-1)^s * prod(diag(U)),</i></td></tr>
<tr><th id="382">382</th><td><i>    // where s is the order of the permutation encoded in pivots and U is the</i></td></tr>
<tr><th id="383">383</th><td><i>    // upper triangular factor of the LU factorization, which is written to</i></td></tr>
<tr><th id="384">384</th><td><i>    // input_copy by the Getrf{Batched} kernel.</i></td></tr>
<tr><th id="385">385</th><td>    functor::LogDeterminantFromPivotedLUFunctor&lt;GPUDevice, Scalar&gt; functor;</td></tr>
<tr><th id="386">386</th><td>    functor(d, input_copy_reshaped_const, pivots_mat.data(), sign_reshaped,</td></tr>
<tr><th id="387">387</th><td>            log_abs_det_reshaped);</td></tr>
<tr><th id="388">388</th><td></td></tr>
<tr><th id="389">389</th><td>    <i>// Register callback to check info after kernels finish.</i></td></tr>
<tr><th id="390">390</th><td>    <em>auto</em> info_checker = [context, done](</td></tr>
<tr><th id="391">391</th><td>                            <em>const</em> Status&amp; status,</td></tr>
<tr><th id="392">392</th><td>                            <em>const</em> std::vector&lt;HostLapackInfo&gt;&amp; host_infos) {</td></tr>
<tr><th id="393">393</th><td>      <b>if</b> (!status.ok() &amp;&amp; errors::IsInvalidArgument(status) &amp;&amp;</td></tr>
<tr><th id="394">394</th><td>          !host_infos.empty()) {</td></tr>
<tr><th id="395">395</th><td>        <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; host_infos[<var>0</var>].size(); ++i) {</td></tr>
<tr><th id="396">396</th><td>          <i>// It is OK for a matrix to be singular (signaled by info &gt; 0),</i></td></tr>
<tr><th id="397">397</th><td><i>          // corresponding to determinant of zero, but we do want to catch</i></td></tr>
<tr><th id="398">398</th><td><i>          // invalid arguments to Getrf{Batched}.</i></td></tr>
<tr><th id="399">399</th><td>          OP_REQUIRES_ASYNC(</td></tr>
<tr><th id="400">400</th><td>              context, host_infos[<var>0</var>](i) &gt;= <var>0</var>,</td></tr>
<tr><th id="401">401</th><td>              errors::InvalidArgument(<q>"Invalid input argument no. "</q>,</td></tr>
<tr><th id="402">402</th><td>                                      host_infos[<var>0</var>].data()[i],</td></tr>
<tr><th id="403">403</th><td>                                      <q>" for batch index "</q>, i, <q>"."</q>),</td></tr>
<tr><th id="404">404</th><td>              done);</td></tr>
<tr><th id="405">405</th><td>        }</td></tr>
<tr><th id="406">406</th><td>      }</td></tr>
<tr><th id="407">407</th><td>      done();</td></tr>
<tr><th id="408">408</th><td>    };</td></tr>
<tr><th id="409">409</th><td>    CudaSolver::CheckLapackInfoAndDeleteSolverAsync(std::move(solver), dev_info,</td></tr>
<tr><th id="410">410</th><td>                                                    std::move(info_checker));</td></tr>
<tr><th id="411">411</th><td>  }</td></tr>
<tr><th id="412">412</th><td>};</td></tr>
<tr><th id="413">413</th><td></td></tr>
<tr><th id="414">414</th><td>REGISTER_LINALG_OP_GPU(<q>"MatrixDeterminant"</q>, (DeterminantOpGpu&lt;<em>float</em>&gt;), <em>float</em>);</td></tr>
<tr><th id="415">415</th><td>REGISTER_LINALG_OP_GPU(<q>"MatrixDeterminant"</q>, (DeterminantOpGpu&lt;<em>double</em>&gt;), <em>double</em>);</td></tr>
<tr><th id="416">416</th><td>REGISTER_LINALG_OP_GPU(<q>"MatrixDeterminant"</q>, (DeterminantOpGpu&lt;complex64&gt;),</td></tr>
<tr><th id="417">417</th><td>                       complex64);</td></tr>
<tr><th id="418">418</th><td>REGISTER_LINALG_OP_GPU(<q>"MatrixDeterminant"</q>, (DeterminantOpGpu&lt;complex128&gt;),</td></tr>
<tr><th id="419">419</th><td>                       complex128);</td></tr>
<tr><th id="420">420</th><td></td></tr>
<tr><th id="421">421</th><td>REGISTER_LINALG_OP_GPU(<q>"LogMatrixDeterminant"</q>, (LogDeterminantOpGpu&lt;<em>float</em>&gt;),</td></tr>
<tr><th id="422">422</th><td>                       <em>float</em>);</td></tr>
<tr><th id="423">423</th><td>REGISTER_LINALG_OP_GPU(<q>"LogMatrixDeterminant"</q>, (LogDeterminantOpGpu&lt;<em>double</em>&gt;),</td></tr>
<tr><th id="424">424</th><td>                       <em>double</em>);</td></tr>
<tr><th id="425">425</th><td>REGISTER_LINALG_OP_GPU(<q>"LogMatrixDeterminant"</q>, (LogDeterminantOpGpu&lt;complex64&gt;),</td></tr>
<tr><th id="426">426</th><td>                       complex64);</td></tr>
<tr><th id="427">427</th><td>REGISTER_LINALG_OP_GPU(<q>"LogMatrixDeterminant"</q>,</td></tr>
<tr><th id="428">428</th><td>                       (LogDeterminantOpGpu&lt;complex128&gt;), complex128);</td></tr>
<tr><th id="429">429</th><td><u>#<span data-ppcond="123">endif</span>  // GOOGLE_CUDA</u></td></tr>
<tr><th id="430">430</th><td></td></tr>
<tr><th id="431">431</th><td><a class="macro" href="linalg_ops_common.h.html#194" title="constexpr bool should_register_0__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__0__object( should_register_0__flag ? ::tensorflow::register_kernel::Name(&quot;MatrixDeterminant&quot;).Device(DEVICE_CPU).TypeConstraint&lt;float&gt;(&quot;T&quot;).Build() : nullptr, &quot;(DeterminantOp&lt;float&gt;)&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new (DeterminantOp&lt;float&gt;)(context); });" data-ref="_M/REGISTER_LINALG_OP">REGISTER_LINALG_OP</a>(<q>"MatrixDeterminant"</q>, (<a class="type" href="#tensorflow::DeterminantOp" title='tensorflow::DeterminantOp' data-ref="tensorflow::DeterminantOp">DeterminantOp</a>&lt;<em>float</em>&gt;), <em>float</em>);</td></tr>
<tr><th id="432">432</th><td><a class="macro" href="linalg_ops_common.h.html#194" title="constexpr bool should_register_2__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__2__object( should_register_2__flag ? ::tensorflow::register_kernel::Name(&quot;MatrixDeterminant&quot;).Device(DEVICE_CPU).TypeConstraint&lt;double&gt;(&quot;T&quot;).Build() : nullptr, &quot;(DeterminantOp&lt;double&gt;)&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new (DeterminantOp&lt;double&gt;)(context); });" data-ref="_M/REGISTER_LINALG_OP">REGISTER_LINALG_OP</a>(<q>"MatrixDeterminant"</q>, (<a class="type" href="#tensorflow::DeterminantOp" title='tensorflow::DeterminantOp' data-ref="tensorflow::DeterminantOp">DeterminantOp</a>&lt;<em>double</em>&gt;), <em>double</em>);</td></tr>
<tr><th id="433">433</th><td><a class="macro" href="linalg_ops_common.h.html#194" title="constexpr bool should_register_4__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__4__object( should_register_4__flag ? ::tensorflow::register_kernel::Name(&quot;MatrixDeterminant&quot;).Device(DEVICE_CPU).TypeConstraint&lt;complex64&gt;(&quot;T&quot;).Build() : nullptr, &quot;(DeterminantOp&lt;complex64&gt;)&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new (DeterminantOp&lt;complex64&gt;)(context); });" data-ref="_M/REGISTER_LINALG_OP">REGISTER_LINALG_OP</a>(<q>"MatrixDeterminant"</q>, (<a class="type" href="#tensorflow::DeterminantOp" title='tensorflow::DeterminantOp' data-ref="tensorflow::DeterminantOp">DeterminantOp</a>&lt;<a class="typedef" href="../lib/bfloat16/bfloat16.h.html#tensorflow::complex64" title='tensorflow::complex64' data-type='std::complex&lt;float&gt;' data-ref="tensorflow::complex64">complex64</a>&gt;), <a class="typedef" href="../lib/bfloat16/bfloat16.h.html#tensorflow::complex64" title='tensorflow::complex64' data-type='std::complex&lt;float&gt;' data-ref="tensorflow::complex64">complex64</a>);</td></tr>
<tr><th id="434">434</th><td><a class="macro" href="linalg_ops_common.h.html#194" title="constexpr bool should_register_6__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__6__object( should_register_6__flag ? ::tensorflow::register_kernel::Name(&quot;MatrixDeterminant&quot;).Device(DEVICE_CPU).TypeConstraint&lt;complex128&gt;(&quot;T&quot;).Build() : nullptr, &quot;(DeterminantOp&lt;complex128&gt;)&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new (DeterminantOp&lt;complex128&gt;)(context); });" data-ref="_M/REGISTER_LINALG_OP">REGISTER_LINALG_OP</a>(<q>"MatrixDeterminant"</q>, (<a class="type" href="#tensorflow::DeterminantOp" title='tensorflow::DeterminantOp' data-ref="tensorflow::DeterminantOp">DeterminantOp</a>&lt;<a class="typedef" href="../lib/bfloat16/bfloat16.h.html#tensorflow::complex128" title='tensorflow::complex128' data-type='std::complex&lt;double&gt;' data-ref="tensorflow::complex128">complex128</a>&gt;),</td></tr>
<tr><th id="435">435</th><td>                   <a class="typedef" href="../lib/bfloat16/bfloat16.h.html#tensorflow::complex128" title='tensorflow::complex128' data-type='std::complex&lt;double&gt;' data-ref="tensorflow::complex128">complex128</a>);</td></tr>
<tr><th id="436">436</th><td><a class="macro" href="linalg_ops_common.h.html#194" title="constexpr bool should_register_8__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__8__object( should_register_8__flag ? ::tensorflow::register_kernel::Name(&quot;BatchMatrixDeterminant&quot;).Device(DEVICE_CPU).TypeConstraint&lt;float&gt;(&quot;T&quot;).Build() : nullptr, &quot;(DeterminantOp&lt;float&gt;)&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new (DeterminantOp&lt;float&gt;)(context); });" data-ref="_M/REGISTER_LINALG_OP">REGISTER_LINALG_OP</a>(<q>"BatchMatrixDeterminant"</q>, (<a class="type" href="#tensorflow::DeterminantOp" title='tensorflow::DeterminantOp' data-ref="tensorflow::DeterminantOp">DeterminantOp</a>&lt;<em>float</em>&gt;), <em>float</em>);</td></tr>
<tr><th id="437">437</th><td><a class="macro" href="linalg_ops_common.h.html#194" title="constexpr bool should_register_10__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__10__object( should_register_10__flag ? ::tensorflow::register_kernel::Name(&quot;BatchMatrixDeterminant&quot;).Device(DEVICE_CPU).TypeConstraint&lt;double&gt;(&quot;T&quot;).Build() : nullptr, &quot;(DeterminantOp&lt;double&gt;)&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new (DeterminantOp&lt;double&gt;)(context); });" data-ref="_M/REGISTER_LINALG_OP">REGISTER_LINALG_OP</a>(<q>"BatchMatrixDeterminant"</q>, (<a class="type" href="#tensorflow::DeterminantOp" title='tensorflow::DeterminantOp' data-ref="tensorflow::DeterminantOp">DeterminantOp</a>&lt;<em>double</em>&gt;), <em>double</em>);</td></tr>
<tr><th id="438">438</th><td><a class="macro" href="linalg_ops_common.h.html#194" title="constexpr bool should_register_12__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__12__object( should_register_12__flag ? ::tensorflow::register_kernel::Name(&quot;BatchMatrixDeterminant&quot;).Device(DEVICE_CPU).TypeConstraint&lt;complex64&gt;(&quot;T&quot;).Build() : nullptr, &quot;(DeterminantOp&lt;complex64&gt;)&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new (DeterminantOp&lt;complex64&gt;)(context); });" data-ref="_M/REGISTER_LINALG_OP">REGISTER_LINALG_OP</a>(<q>"BatchMatrixDeterminant"</q>, (<a class="type" href="#tensorflow::DeterminantOp" title='tensorflow::DeterminantOp' data-ref="tensorflow::DeterminantOp">DeterminantOp</a>&lt;<a class="typedef" href="../lib/bfloat16/bfloat16.h.html#tensorflow::complex64" title='tensorflow::complex64' data-type='std::complex&lt;float&gt;' data-ref="tensorflow::complex64">complex64</a>&gt;),</td></tr>
<tr><th id="439">439</th><td>                   <a class="typedef" href="../lib/bfloat16/bfloat16.h.html#tensorflow::complex64" title='tensorflow::complex64' data-type='std::complex&lt;float&gt;' data-ref="tensorflow::complex64">complex64</a>);</td></tr>
<tr><th id="440">440</th><td><a class="macro" href="linalg_ops_common.h.html#194" title="constexpr bool should_register_14__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__14__object( should_register_14__flag ? ::tensorflow::register_kernel::Name(&quot;BatchMatrixDeterminant&quot;).Device(DEVICE_CPU).TypeConstraint&lt;complex128&gt;(&quot;T&quot;).Build() : nullptr, &quot;(DeterminantOp&lt;complex128&gt;)&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new (DeterminantOp&lt;complex128&gt;)(context); });" data-ref="_M/REGISTER_LINALG_OP">REGISTER_LINALG_OP</a>(<q>"BatchMatrixDeterminant"</q>, (<a class="type" href="#tensorflow::DeterminantOp" title='tensorflow::DeterminantOp' data-ref="tensorflow::DeterminantOp">DeterminantOp</a>&lt;<a class="typedef" href="../lib/bfloat16/bfloat16.h.html#tensorflow::complex128" title='tensorflow::complex128' data-type='std::complex&lt;double&gt;' data-ref="tensorflow::complex128">complex128</a>&gt;),</td></tr>
<tr><th id="441">441</th><td>                   <a class="typedef" href="../lib/bfloat16/bfloat16.h.html#tensorflow::complex128" title='tensorflow::complex128' data-type='std::complex&lt;double&gt;' data-ref="tensorflow::complex128">complex128</a>);</td></tr>
<tr><th id="442">442</th><td></td></tr>
<tr><th id="443">443</th><td><a class="macro" href="linalg_ops_common.h.html#194" title="constexpr bool should_register_16__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__16__object( should_register_16__flag ? ::tensorflow::register_kernel::Name(&quot;LogMatrixDeterminant&quot;).Device(DEVICE_CPU).TypeConstraint&lt;float&gt;(&quot;T&quot;).Build() : nullptr, &quot;(LogDeterminantOp&lt;float&gt;)&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new (LogDeterminantOp&lt;float&gt;)(context); });" data-ref="_M/REGISTER_LINALG_OP">REGISTER_LINALG_OP</a>(<q>"LogMatrixDeterminant"</q>, (<a class="type" href="#tensorflow::LogDeterminantOp" title='tensorflow::LogDeterminantOp' data-ref="tensorflow::LogDeterminantOp">LogDeterminantOp</a>&lt;<em>float</em>&gt;), <em>float</em>);</td></tr>
<tr><th id="444">444</th><td><a class="macro" href="linalg_ops_common.h.html#194" title="constexpr bool should_register_18__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__18__object( should_register_18__flag ? ::tensorflow::register_kernel::Name(&quot;LogMatrixDeterminant&quot;).Device(DEVICE_CPU).TypeConstraint&lt;double&gt;(&quot;T&quot;).Build() : nullptr, &quot;(LogDeterminantOp&lt;double&gt;)&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new (LogDeterminantOp&lt;double&gt;)(context); });" data-ref="_M/REGISTER_LINALG_OP">REGISTER_LINALG_OP</a>(<q>"LogMatrixDeterminant"</q>, (<a class="type" href="#tensorflow::LogDeterminantOp" title='tensorflow::LogDeterminantOp' data-ref="tensorflow::LogDeterminantOp">LogDeterminantOp</a>&lt;<em>double</em>&gt;), <em>double</em>);</td></tr>
<tr><th id="445">445</th><td><a class="macro" href="linalg_ops_common.h.html#194" title="constexpr bool should_register_20__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__20__object( should_register_20__flag ? ::tensorflow::register_kernel::Name(&quot;LogMatrixDeterminant&quot;).Device(DEVICE_CPU).TypeConstraint&lt;complex64&gt;(&quot;T&quot;).Build() : nullptr, &quot;(LogDeterminantOp&lt;complex64&gt;)&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new (LogDeterminantOp&lt;complex64&gt;)(context); });" data-ref="_M/REGISTER_LINALG_OP">REGISTER_LINALG_OP</a>(<q>"LogMatrixDeterminant"</q>, (<a class="type" href="#tensorflow::LogDeterminantOp" title='tensorflow::LogDeterminantOp' data-ref="tensorflow::LogDeterminantOp">LogDeterminantOp</a>&lt;<a class="typedef" href="../lib/bfloat16/bfloat16.h.html#tensorflow::complex64" title='tensorflow::complex64' data-type='std::complex&lt;float&gt;' data-ref="tensorflow::complex64">complex64</a>&gt;),</td></tr>
<tr><th id="446">446</th><td>                   <a class="typedef" href="../lib/bfloat16/bfloat16.h.html#tensorflow::complex64" title='tensorflow::complex64' data-type='std::complex&lt;float&gt;' data-ref="tensorflow::complex64">complex64</a>);</td></tr>
<tr><th id="447">447</th><td><a class="macro" href="linalg_ops_common.h.html#194" title="constexpr bool should_register_22__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__22__object( should_register_22__flag ? ::tensorflow::register_kernel::Name(&quot;LogMatrixDeterminant&quot;).Device(DEVICE_CPU).TypeConstraint&lt;complex128&gt;(&quot;T&quot;).Build() : nullptr, &quot;(LogDeterminantOp&lt;complex128&gt;)&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new (LogDeterminantOp&lt;complex128&gt;)(context); });" data-ref="_M/REGISTER_LINALG_OP">REGISTER_LINALG_OP</a>(<q>"LogMatrixDeterminant"</q>, (<a class="type" href="#tensorflow::LogDeterminantOp" title='tensorflow::LogDeterminantOp' data-ref="tensorflow::LogDeterminantOp">LogDeterminantOp</a>&lt;<a class="typedef" href="../lib/bfloat16/bfloat16.h.html#tensorflow::complex128" title='tensorflow::complex128' data-type='std::complex&lt;double&gt;' data-ref="tensorflow::complex128">complex128</a>&gt;),</td></tr>
<tr><th id="448">448</th><td>                   <a class="typedef" href="../lib/bfloat16/bfloat16.h.html#tensorflow::complex128" title='tensorflow::complex128' data-type='std::complex&lt;double&gt;' data-ref="tensorflow::complex128">complex128</a>);</td></tr>
<tr><th id="449">449</th><td>}  <i>// namespace tensorflow</i></td></tr>
<tr><th id="450">450</th><td></td></tr>
</table><hr/><p id='footer'>
Generated on <em>2018-Aug-16</em> from project tensorflow revision <em>v1.8</em><br />Powered by <a href='https://woboq.com'><img alt='Woboq' src='https://code.woboq.org/woboq-16.png' width='41' height='16' /></a> <a href='https://code.woboq.org'>Code Browser</a> 2.1
<br/>Generator usage only permitted with license.</p>
</div></body></html>
