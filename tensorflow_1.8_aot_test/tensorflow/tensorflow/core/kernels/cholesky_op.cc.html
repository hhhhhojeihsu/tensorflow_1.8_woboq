<!doctype html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><title>cholesky_op.cc source code [tensorflow/tensorflow/core/kernels/cholesky_op.cc] - Woboq Code Browser</title>
<meta name="woboq:interestingDefinitions" content="tensorflow::CholeskyOp "/>
<link rel="stylesheet" href="https://code.woboq.org/data/qtcreator.css" title="QtCreator"/>
<link rel="alternate stylesheet" href="https://code.woboq.org/data/kdevelop.css" title="KDevelop"/>
<script type="text/javascript" src="https://code.woboq.org/data/jquery/jquery.min.js"></script>
<script type="text/javascript" src="https://code.woboq.org/data/jquery/jquery-ui.min.js"></script>
<script>var file = 'tensorflow/tensorflow/core/kernels/cholesky_op.cc'; var root_path = '../../../..'; var data_path = 'https://code.woboq.org/data';</script>
<script src='https://code.woboq.org/data/codebrowser.js'></script>
</head>
<body><div id='header'><h1 id='breadcrumb'><span>Browse the source code of </span><a href='../../..'>tensorflow</a>/<a href='../..'>tensorflow</a>/<a href='..'>core</a>/<a href='./'>kernels</a>/<a href='cholesky_op.cc.html'>cholesky_op.cc</a></h1></div>
<hr/><div id='content'><table class="code">
<tr><th id="1">1</th><td><i>/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.</i></td></tr>
<tr><th id="2">2</th><td><i></i></td></tr>
<tr><th id="3">3</th><td><i>Licensed under the Apache License, Version 2.0 (the "License");</i></td></tr>
<tr><th id="4">4</th><td><i>you may not use this file except in compliance with the License.</i></td></tr>
<tr><th id="5">5</th><td><i>You may obtain a copy of the License at</i></td></tr>
<tr><th id="6">6</th><td><i></i></td></tr>
<tr><th id="7">7</th><td><i>    <a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></i></td></tr>
<tr><th id="8">8</th><td><i></i></td></tr>
<tr><th id="9">9</th><td><i>Unless required by applicable law or agreed to in writing, software</i></td></tr>
<tr><th id="10">10</th><td><i>distributed under the License is distributed on an "AS IS" BASIS,</i></td></tr>
<tr><th id="11">11</th><td><i>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</i></td></tr>
<tr><th id="12">12</th><td><i>See the License for the specific language governing permissions and</i></td></tr>
<tr><th id="13">13</th><td><i>limitations under the License.</i></td></tr>
<tr><th id="14">14</th><td><i>==============================================================================*/</i></td></tr>
<tr><th id="15">15</th><td><i></i></td></tr>
<tr><th id="16">16</th><td><i>// See docs in ../ops/linalg_ops.cc.</i></td></tr>
<tr><th id="17">17</th><td></td></tr>
<tr><th id="18">18</th><td><u>#<span data-ppcond="18">if</span> GOOGLE_CUDA</u></td></tr>
<tr><th id="19">19</th><td><u>#define EIGEN_USE_GPU</u></td></tr>
<tr><th id="20">20</th><td><u>#<span data-ppcond="18">endif</span>  // GOOGLE_CUDA</u></td></tr>
<tr><th id="21">21</th><td></td></tr>
<tr><th id="22">22</th><td><u>#include <a href="../../../third_party/eigen3/Eigen/Cholesky.html">"third_party/eigen3/Eigen/Cholesky"</a></u></td></tr>
<tr><th id="23">23</th><td><u>#include <a href="../../../third_party/eigen3/Eigen/Core.html">"third_party/eigen3/Eigen/Core"</a></u></td></tr>
<tr><th id="24">24</th><td><u>#include <a href="../framework/kernel_def_builder.h.html">"tensorflow/core/framework/kernel_def_builder.h"</a></u></td></tr>
<tr><th id="25">25</th><td><u>#include <a href="../framework/op_kernel.h.html">"tensorflow/core/framework/op_kernel.h"</a></u></td></tr>
<tr><th id="26">26</th><td><u>#include <a href="../framework/register_types.h.html">"tensorflow/core/framework/register_types.h"</a></u></td></tr>
<tr><th id="27">27</th><td><u>#include <a href="../framework/tensor_shape.h.html">"tensorflow/core/framework/tensor_shape.h"</a></u></td></tr>
<tr><th id="28">28</th><td><u>#include <a href="linalg_ops_common.h.html">"tensorflow/core/kernels/linalg_ops_common.h"</a></u></td></tr>
<tr><th id="29">29</th><td><u>#include <a href="../lib/core/errors.h.html">"tensorflow/core/lib/core/errors.h"</a></u></td></tr>
<tr><th id="30">30</th><td><u>#include <a href="../platform/logging.h.html">"tensorflow/core/platform/logging.h"</a></u></td></tr>
<tr><th id="31">31</th><td><u>#include <a href="../platform/types.h.html">"tensorflow/core/platform/types.h"</a></u></td></tr>
<tr><th id="32">32</th><td></td></tr>
<tr><th id="33">33</th><td><u>#<span data-ppcond="33">if</span> GOOGLE_CUDA</u></td></tr>
<tr><th id="34">34</th><td><u>#include "third_party/eigen3/unsupported/Eigen/CXX11/Tensor"</u></td></tr>
<tr><th id="35">35</th><td><u>#include "tensorflow/core/kernels/cuda_solvers.h"</u></td></tr>
<tr><th id="36">36</th><td><u>#include "tensorflow/core/kernels/matrix_band_part_op.h"</u></td></tr>
<tr><th id="37">37</th><td><u>#include "tensorflow/core/platform/stream_executor.h"</u></td></tr>
<tr><th id="38">38</th><td><u>#<span data-ppcond="33">endif</span></u></td></tr>
<tr><th id="39">39</th><td></td></tr>
<tr><th id="40">40</th><td><b>namespace</b> <span class="namespace">tensorflow</span> {</td></tr>
<tr><th id="41">41</th><td></td></tr>
<tr><th id="42">42</th><td><em>static</em> <em>const</em> <em>char</em> <dfn class="tu decl def" id="tensorflow::kErrMsg" title='tensorflow::kErrMsg' data-type='const char [73]' data-ref="tensorflow::kErrMsg">kErrMsg</dfn>[] =</td></tr>
<tr><th id="43">43</th><td>    <q>"Cholesky decomposition was not successful. The input might not be valid."</q>;</td></tr>
<tr><th id="44">44</th><td></td></tr>
<tr><th id="45">45</th><td><b>template</b> &lt;<b>class</b> Scalar&gt;</td></tr>
<tr><th id="46">46</th><td><b>class</b> <dfn class="type def" id="tensorflow::CholeskyOp" title='tensorflow::CholeskyOp' data-ref="tensorflow::CholeskyOp">CholeskyOp</dfn> : <b>public</b> <a class="type" href="linalg_ops_common.h.html#tensorflow::LinearAlgebraOp" title='tensorflow::LinearAlgebraOp' data-ref="tensorflow::LinearAlgebraOp">LinearAlgebraOp</a>&lt;Scalar&gt; {</td></tr>
<tr><th id="47">47</th><td> <b>public</b>:</td></tr>
<tr><th id="48">48</th><td>  <a class="macro" href="linalg_ops_common.h.html#175" title="typedef LinearAlgebraOp&lt;Scalar&gt; Base; using RealScalar = typename Eigen::NumTraits&lt;Scalar&gt;::Real; using Matrix = typename Base::Matrix; using MatrixMap = typename Base::MatrixMap; using MatrixMaps = typename Base::MatrixMaps; using ConstMatrixMap = typename Base::ConstMatrixMap; using ConstMatrixMaps = typename Base::ConstMatrixMaps; using TensorShapes = typename Base::TensorShapes;" data-ref="_M/INHERIT_LINALG_TYPEDEFS">INHERIT_LINALG_TYPEDEFS</a>(Scalar);</td></tr>
<tr><th id="49">49</th><td></td></tr>
<tr><th id="50">50</th><td>  <b>explicit</b> <dfn class="tu decl def" id="_ZN10tensorflow10CholeskyOpC1EPNS_20OpKernelConstructionE" title='tensorflow::CholeskyOp::CholeskyOp&lt;Scalar&gt;' data-type='void tensorflow::CholeskyOp::CholeskyOp&lt;Scalar&gt;(tensorflow::OpKernelConstruction * context)' data-ref="_ZN10tensorflow10CholeskyOpC1EPNS_20OpKernelConstructionE">CholeskyOp</dfn>(<a class="type" href="../framework/op_kernel.h.html#tensorflow::OpKernelConstruction" title='tensorflow::OpKernelConstruction' data-ref="tensorflow::OpKernelConstruction">OpKernelConstruction</a>* <dfn class="local col1 decl" id="1context" title='context' data-type='tensorflow::OpKernelConstruction *' data-ref="1context">context</dfn>) : <a class="typedef" href="#48" title='tensorflow::CholeskyOp::Base' data-type='LinearAlgebraOp&lt;Scalar&gt;' data-ref="tensorflow::CholeskyOp::Base">Base</a>(<a class="local col1 ref" href="#1context" title='context' data-ref="1context">context</a>) {}</td></tr>
<tr><th id="51">51</th><td></td></tr>
<tr><th id="52">52</th><td>  <em>void</em> <dfn class="tu decl def" id="_ZN10tensorflow10CholeskyOp13ComputeMatrixEPNS_15OpKernelContextERKNS_15LinearAlgebraOpIT_E15ConstMatrixMapsEPNS5_10MatrixMapsE" title='tensorflow::CholeskyOp::ComputeMatrix' data-type='void tensorflow::CholeskyOp::ComputeMatrix(tensorflow::OpKernelContext * context, const ConstMatrixMaps &amp; inputs, MatrixMaps * outputs)' data-ref="_ZN10tensorflow10CholeskyOp13ComputeMatrixEPNS_15OpKernelContextERKNS_15LinearAlgebraOpIT_E15ConstMatrixMapsEPNS5_10MatrixMapsE">ComputeMatrix</dfn>(<a class="type" href="../framework/op_kernel.h.html#tensorflow::OpKernelContext" title='tensorflow::OpKernelContext' data-ref="tensorflow::OpKernelContext">OpKernelContext</a>* <dfn class="local col2 decl" id="2context" title='context' data-type='tensorflow::OpKernelContext *' data-ref="2context">context</dfn>, <em>const</em> <a class="typedef" href="#48" title='tensorflow::CholeskyOp::ConstMatrixMaps' data-type='typename Base::ConstMatrixMaps' data-ref="tensorflow::CholeskyOp::ConstMatrixMaps">ConstMatrixMaps</a>&amp; <dfn class="local col3 decl" id="3inputs" title='inputs' data-type='const ConstMatrixMaps &amp;' data-ref="3inputs">inputs</dfn>,</td></tr>
<tr><th id="53">53</th><td>                     <a class="typedef" href="#48" title='tensorflow::CholeskyOp::MatrixMaps' data-type='typename Base::MatrixMaps' data-ref="tensorflow::CholeskyOp::MatrixMaps">MatrixMaps</a>* <dfn class="local col4 decl" id="4outputs" title='outputs' data-type='MatrixMaps *' data-ref="4outputs">outputs</dfn>) final {</td></tr>
<tr><th id="54">54</th><td>    <em>const</em> <a class="typedef" href="#48" title='tensorflow::CholeskyOp::ConstMatrixMap' data-type='typename Base::ConstMatrixMap' data-ref="tensorflow::CholeskyOp::ConstMatrixMap">ConstMatrixMap</a>&amp; <dfn class="local col5 decl" id="5input" title='input' data-type='const ConstMatrixMap &amp;' data-ref="5input">input</dfn> = <a class="local col3 ref" href="#3inputs" title='inputs' data-ref="3inputs">inputs</a>[<var>0</var>];</td></tr>
<tr><th id="55">55</th><td>    <b>if</b> (<a class="local col5 ref" href="#5input" title='input' data-ref="5input">input</a>.rows() == <var>0</var>) {</td></tr>
<tr><th id="56">56</th><td>      <i>// If X is an empty matrix (0 rows, 0 col), X * X' == X.</i></td></tr>
<tr><th id="57">57</th><td><i>      // Therefore, we return X.</i></td></tr>
<tr><th id="58">58</th><td>      <b>return</b>;</td></tr>
<tr><th id="59">59</th><td>    }</td></tr>
<tr><th id="60">60</th><td>    <i>// Perform the actual LL^T Cholesky decomposition. This will only use</i></td></tr>
<tr><th id="61">61</th><td><i>    // the lower triangular part of data_in by default. The upper triangular</i></td></tr>
<tr><th id="62">62</th><td><i>    // part of the matrix will not be read.</i></td></tr>
<tr><th id="63">63</th><td>    <span class="namespace">Eigen::</span><span class='type' title='Eigen::LLT' data-ref="Eigen::LLT">LLT</span>&lt;</td></tr>
<tr><th id="64">64</th><td>        <span class="namespace">Eigen::</span><span class='type' title='Eigen::Matrix' data-ref="Eigen::Matrix">Matrix</span>&lt;Scalar, <span class="namespace">Eigen::</span><span class='ref' title='Eigen::Dynamic' data-ref="Eigen::Dynamic">Dynamic</span>, <span class="namespace">Eigen::</span><span class='ref' title='Eigen::Dynamic' data-ref="Eigen::Dynamic">Dynamic</span>, <span class="namespace">Eigen::</span><span class='enum' title='Eigen::StorageOptions::RowMajor' data-ref="Eigen::StorageOptions::RowMajor">RowMajor</span>&gt;&gt;</td></tr>
<tr><th id="65">65</th><td>        <dfn class="local col6 decl" id="6llt_decomposition" title='llt_decomposition' data-type='Eigen::LLT&lt;Eigen::Matrix&lt;Scalar, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt; &gt;' data-ref="6llt_decomposition">llt_decomposition</dfn>(<a class="local col5 ref" href="#5input" title='input' data-ref="5input">input</a>);</td></tr>
<tr><th id="66">66</th><td></td></tr>
<tr><th id="67">67</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1537" title="do { if (!(__builtin_expect(!!(llt_decomposition.info() == Eigen::Success), 1))) { (context)-&gt;CtxFailure(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/cholesky_op.cc&quot;, 68, (errors::InvalidArgument(kErrMsg))); return; } } while (0)" data-ref="_M/OP_REQUIRES">OP_REQUIRES</a>(<a class="local col2 ref" href="#2context" title='context' data-ref="2context">context</a>, <a class="local col6 ref" href="#6llt_decomposition" title='llt_decomposition' data-ref="6llt_decomposition">llt_decomposition</a>.info() == Eigen::<span class='enum' title='Eigen::ComputationInfo::Success' data-ref="Eigen::ComputationInfo::Success">Success</span>,</td></tr>
<tr><th id="68">68</th><td>                errors::<a class="ref" href="../lib/core/errors.h.html#103" title='tensorflow::errors::InvalidArgument' data-ref="_ZN10tensorflow6errors15InvalidArgumentEDpT_">InvalidArgument</a>(<a class="tu ref" href="#tensorflow::kErrMsg" title='tensorflow::kErrMsg' data-use='r' data-ref="tensorflow::kErrMsg">kErrMsg</a>));</td></tr>
<tr><th id="69">69</th><td></td></tr>
<tr><th id="70">70</th><td>    <i>// Output the lower triangular in a dense form.</i></td></tr>
<tr><th id="71">71</th><td>    <a class="local col4 ref" href="#4outputs" title='outputs' data-ref="4outputs">outputs</a>-&gt;at(<var>0</var>) = <a class="local col6 ref" href="#6llt_decomposition" title='llt_decomposition' data-ref="6llt_decomposition">llt_decomposition</a>.matrixL();</td></tr>
<tr><th id="72">72</th><td>  }</td></tr>
<tr><th id="73">73</th><td>};</td></tr>
<tr><th id="74">74</th><td></td></tr>
<tr><th id="75">75</th><td><u>#<span data-ppcond="75">if</span> GOOGLE_CUDA</u></td></tr>
<tr><th id="76">76</th><td><b>typedef</b> Eigen::GpuDevice GPUDevice;</td></tr>
<tr><th id="77">77</th><td></td></tr>
<tr><th id="78">78</th><td><b>namespace</b> functor {</td></tr>
<tr><th id="79">79</th><td><u>#define DECLARE_GPU_SPEC(T)                                            \</u></td></tr>
<tr><th id="80">80</th><td><u>  template &lt;&gt;                                                          \</u></td></tr>
<tr><th id="81">81</th><td><u>  struct MatrixBandPartFunctor&lt;GPUDevice, T&gt; {                         \</u></td></tr>
<tr><th id="82">82</th><td><u>    void operator()(OpKernelContext* context, const GPUDevice&amp; device, \</u></td></tr>
<tr><th id="83">83</th><td><u>                    int num_upper_diags, int num_lower_diags,          \</u></td></tr>
<tr><th id="84">84</th><td><u>                    typename TTypes&lt;T, 3&gt;::ConstTensor input,          \</u></td></tr>
<tr><th id="85">85</th><td><u>                    typename TTypes&lt;T, 3&gt;::Tensor output);             \</u></td></tr>
<tr><th id="86">86</th><td><u>  };                                                                   \</u></td></tr>
<tr><th id="87">87</th><td><u>  extern template struct MatrixBandPartFunctor&lt;GPUDevice, T&gt;;</u></td></tr>
<tr><th id="88">88</th><td></td></tr>
<tr><th id="89">89</th><td>TF_CALL_GPU_NUMBER_TYPES(DECLARE_GPU_SPEC);</td></tr>
<tr><th id="90">90</th><td>TF_CALL_complex64(DECLARE_GPU_SPEC);</td></tr>
<tr><th id="91">91</th><td>TF_CALL_complex128(DECLARE_GPU_SPEC);</td></tr>
<tr><th id="92">92</th><td>}  <i>// namespace functor</i></td></tr>
<tr><th id="93">93</th><td></td></tr>
<tr><th id="94">94</th><td><b>template</b> &lt;<b>class</b> Scalar&gt;</td></tr>
<tr><th id="95">95</th><td><b>class</b> CholeskyOpGpu : <b>public</b> AsyncOpKernel {</td></tr>
<tr><th id="96">96</th><td> <b>public</b>:</td></tr>
<tr><th id="97">97</th><td>  <b>explicit</b> CholeskyOpGpu(OpKernelConstruction* context)</td></tr>
<tr><th id="98">98</th><td>      : AsyncOpKernel(context) {}</td></tr>
<tr><th id="99">99</th><td></td></tr>
<tr><th id="100">100</th><td>  <em>void</em> ComputeAsync(OpKernelContext* context, DoneCallback done) final {</td></tr>
<tr><th id="101">101</th><td>    <em>const</em> Tensor&amp; input = context-&gt;input(<var>0</var>);</td></tr>
<tr><th id="102">102</th><td>    <em>const</em> <em>int</em> ndims = input.dims();</td></tr>
<tr><th id="103">103</th><td>    <em>const</em> int64 n = input.dim_size(ndims - <var>1</var>);</td></tr>
<tr><th id="104">104</th><td>    <i>// Validate inputs.</i></td></tr>
<tr><th id="105">105</th><td>    OP_REQUIRES_ASYNC(</td></tr>
<tr><th id="106">106</th><td>        context, ndims &gt;= <var>2</var>,</td></tr>
<tr><th id="107">107</th><td>        errors::InvalidArgument(<q>"Input must have rank &gt;= 2, got "</q>, ndims),</td></tr>
<tr><th id="108">108</th><td>        done);</td></tr>
<tr><th id="109">109</th><td>    OP_REQUIRES_ASYNC(</td></tr>
<tr><th id="110">110</th><td>        context, input.dim_size(ndims - <var>2</var>) == n,</td></tr>
<tr><th id="111">111</th><td>        errors::InvalidArgument(<q>"Input matrices must be squares, got"</q>,</td></tr>
<tr><th id="112">112</th><td>                                input.dim_size(ndims - <var>2</var>), <q>" != "</q>, n),</td></tr>
<tr><th id="113">113</th><td>        done);</td></tr>
<tr><th id="114">114</th><td></td></tr>
<tr><th id="115">115</th><td>    <b>if</b> (input.NumElements() == <var>0</var>) {</td></tr>
<tr><th id="116">116</th><td>      <i>// If X is an empty matrix (0 rows, 0 col), X * X' == X.</i></td></tr>
<tr><th id="117">117</th><td><i>      // Therefore, we return X.</i></td></tr>
<tr><th id="118">118</th><td>      context-&gt;set_output(<var>0</var>, input);</td></tr>
<tr><th id="119">119</th><td>      done();</td></tr>
<tr><th id="120">120</th><td>      <b>return</b>;</td></tr>
<tr><th id="121">121</th><td>    }</td></tr>
<tr><th id="122">122</th><td></td></tr>
<tr><th id="123">123</th><td>    <i>// Allocate output.</i></td></tr>
<tr><th id="124">124</th><td><i>    // TODO(rmlarsen): Convert to std::make_unique when available.</i></td></tr>
<tr><th id="125">125</th><td>    std::unique_ptr&lt;CudaSolver&gt; solver(<b>new</b> CudaSolver(context));</td></tr>
<tr><th id="126">126</th><td>    Tensor* output;</td></tr>
<tr><th id="127">127</th><td>    OP_REQUIRES_OK_ASYNC(context,</td></tr>
<tr><th id="128">128</th><td>                         context-&gt;forward_input_or_allocate_output(</td></tr>
<tr><th id="129">129</th><td>                             {<var>0</var>}, <var>0</var>, input.shape(), &amp;output),</td></tr>
<tr><th id="130">130</th><td>                         done);</td></tr>
<tr><th id="131">131</th><td></td></tr>
<tr><th id="132">132</th><td>    <i>// Copy the lower triangular part of the input matrices to the output and</i></td></tr>
<tr><th id="133">133</th><td><i>    // set the strictly upper triangular part to zero. We use a pre-existing</i></td></tr>
<tr><th id="134">134</th><td><i>    // kernel MatrixBandPart to do this for all matrices in the batch at once,</i></td></tr>
<tr><th id="135">135</th><td><i>    // before we launch each of the Cholesky factorization kernels in paralle.</i></td></tr>
<tr><th id="136">136</th><td>    <em>auto</em> input_reshaped = input.<b>template</b> flat_inner_dims&lt;Scalar, <var>3</var>&gt;();</td></tr>
<tr><th id="137">137</th><td>    <em>auto</em> output_reshaped = output-&gt;<b>template</b> flat_inner_dims&lt;Scalar, <var>3</var>&gt;();</td></tr>
<tr><th id="138">138</th><td>    functor::MatrixBandPartFunctor&lt;GPUDevice, Scalar&gt; band_part;</td></tr>
<tr><th id="139">139</th><td>    band_part(context, context-&gt;eigen_device&lt;GPUDevice&gt;(),</td></tr>
<tr><th id="140">140</th><td>              n <i>/* num_lower_diags */</i>, <var>0</var> <i>/* num_upper_diags */</i>, input_reshaped,</td></tr>
<tr><th id="141">141</th><td>              output_reshaped);</td></tr>
<tr><th id="142">142</th><td></td></tr>
<tr><th id="143">143</th><td>    <i>// Launch a Cholesky kernel for each matrix in the batch.</i></td></tr>
<tr><th id="144">144</th><td>    <em>const</em> int64 batch_size = input_reshaped.dimension(<var>0</var>);</td></tr>
<tr><th id="145">145</th><td>    std::vector&lt;DeviceLapackInfo&gt; dev_info;</td></tr>
<tr><th id="146">146</th><td>    dev_info.push_back(solver-&gt;GetDeviceLapackInfo(batch_size, <q>"potrf"</q>));</td></tr>
<tr><th id="147">147</th><td>    <i>// TODO(rmlarsen): Parallelize over batches if it turns out to be</i></td></tr>
<tr><th id="148">148</th><td><i>    // an important use case.</i></td></tr>
<tr><th id="149">149</th><td>    <b>for</b> (<em>int</em> batch = <var>0</var>; batch &lt; batch_size; ++batch) {</td></tr>
<tr><th id="150">150</th><td>      OP_REQUIRES_OK_ASYNC(context,</td></tr>
<tr><th id="151">151</th><td>                           solver-&gt;Potrf(CUBLAS_FILL_MODE_UPPER, n,</td></tr>
<tr><th id="152">152</th><td>                                         &amp;output_reshaped(batch, <var>0</var>, <var>0</var>), n,</td></tr>
<tr><th id="153">153</th><td>                                         &amp;dev_info.back()(batch)),</td></tr>
<tr><th id="154">154</th><td>                           done);</td></tr>
<tr><th id="155">155</th><td>    }</td></tr>
<tr><th id="156">156</th><td></td></tr>
<tr><th id="157">157</th><td>    <i>// Register callback to check info after kernels finish.</i></td></tr>
<tr><th id="158">158</th><td>    <em>auto</em> info_checker = [context, done](</td></tr>
<tr><th id="159">159</th><td>                            <em>const</em> Status&amp; status,</td></tr>
<tr><th id="160">160</th><td>                            <em>const</em> std::vector&lt;HostLapackInfo&gt;&amp; <i>/* unused */</i>) {</td></tr>
<tr><th id="161">161</th><td>      OP_REQUIRES_ASYNC(context, status.ok(), errors::InvalidArgument(kErrMsg),</td></tr>
<tr><th id="162">162</th><td>                        done);</td></tr>
<tr><th id="163">163</th><td>      done();</td></tr>
<tr><th id="164">164</th><td>    };</td></tr>
<tr><th id="165">165</th><td>    CudaSolver::CheckLapackInfoAndDeleteSolverAsync(std::move(solver), dev_info,</td></tr>
<tr><th id="166">166</th><td>                                                    std::move(info_checker));</td></tr>
<tr><th id="167">167</th><td>  }</td></tr>
<tr><th id="168">168</th><td>};</td></tr>
<tr><th id="169">169</th><td></td></tr>
<tr><th id="170">170</th><td>REGISTER_LINALG_OP_GPU(<q>"Cholesky"</q>, (CholeskyOpGpu&lt;<em>float</em>&gt;), <em>float</em>);</td></tr>
<tr><th id="171">171</th><td>REGISTER_LINALG_OP_GPU(<q>"Cholesky"</q>, (CholeskyOpGpu&lt;<em>double</em>&gt;), <em>double</em>);</td></tr>
<tr><th id="172">172</th><td>REGISTER_LINALG_OP_GPU(<q>"Cholesky"</q>, (CholeskyOpGpu&lt;complex64&gt;), complex64);</td></tr>
<tr><th id="173">173</th><td>REGISTER_LINALG_OP_GPU(<q>"Cholesky"</q>, (CholeskyOpGpu&lt;complex128&gt;), complex128);</td></tr>
<tr><th id="174">174</th><td></td></tr>
<tr><th id="175">175</th><td><u>#<span data-ppcond="75">endif</span>  // GOOGLE_CUDA</u></td></tr>
<tr><th id="176">176</th><td></td></tr>
<tr><th id="177">177</th><td><a class="macro" href="linalg_ops_common.h.html#194" title="constexpr bool should_register_0__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__0__object( should_register_0__flag ? ::tensorflow::register_kernel::Name(&quot;Cholesky&quot;).Device(DEVICE_CPU).TypeConstraint&lt;float&gt;(&quot;T&quot;).Build() : nullptr, &quot;(CholeskyOp&lt;float&gt;)&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new (CholeskyOp&lt;float&gt;)(context); });" data-ref="_M/REGISTER_LINALG_OP">REGISTER_LINALG_OP</a>(<q>"Cholesky"</q>, (<a class="type" href="#tensorflow::CholeskyOp" title='tensorflow::CholeskyOp' data-ref="tensorflow::CholeskyOp">CholeskyOp</a>&lt;<em>float</em>&gt;), <em>float</em>);</td></tr>
<tr><th id="178">178</th><td><a class="macro" href="linalg_ops_common.h.html#194" title="constexpr bool should_register_2__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__2__object( should_register_2__flag ? ::tensorflow::register_kernel::Name(&quot;Cholesky&quot;).Device(DEVICE_CPU).TypeConstraint&lt;double&gt;(&quot;T&quot;).Build() : nullptr, &quot;(CholeskyOp&lt;double&gt;)&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new (CholeskyOp&lt;double&gt;)(context); });" data-ref="_M/REGISTER_LINALG_OP">REGISTER_LINALG_OP</a>(<q>"Cholesky"</q>, (<a class="type" href="#tensorflow::CholeskyOp" title='tensorflow::CholeskyOp' data-ref="tensorflow::CholeskyOp">CholeskyOp</a>&lt;<em>double</em>&gt;), <em>double</em>);</td></tr>
<tr><th id="179">179</th><td><a class="macro" href="linalg_ops_common.h.html#194" title="constexpr bool should_register_4__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__4__object( should_register_4__flag ? ::tensorflow::register_kernel::Name(&quot;Cholesky&quot;).Device(DEVICE_CPU).TypeConstraint&lt;complex64&gt;(&quot;T&quot;).Build() : nullptr, &quot;(CholeskyOp&lt;complex64&gt;)&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new (CholeskyOp&lt;complex64&gt;)(context); });" data-ref="_M/REGISTER_LINALG_OP">REGISTER_LINALG_OP</a>(<q>"Cholesky"</q>, (<a class="type" href="#tensorflow::CholeskyOp" title='tensorflow::CholeskyOp' data-ref="tensorflow::CholeskyOp">CholeskyOp</a>&lt;<a class="typedef" href="../lib/bfloat16/bfloat16.h.html#tensorflow::complex64" title='tensorflow::complex64' data-type='std::complex&lt;float&gt;' data-ref="tensorflow::complex64">complex64</a>&gt;), <a class="typedef" href="../lib/bfloat16/bfloat16.h.html#tensorflow::complex64" title='tensorflow::complex64' data-type='std::complex&lt;float&gt;' data-ref="tensorflow::complex64">complex64</a>);</td></tr>
<tr><th id="180">180</th><td><a class="macro" href="linalg_ops_common.h.html#194" title="constexpr bool should_register_6__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__6__object( should_register_6__flag ? ::tensorflow::register_kernel::Name(&quot;Cholesky&quot;).Device(DEVICE_CPU).TypeConstraint&lt;complex128&gt;(&quot;T&quot;).Build() : nullptr, &quot;(CholeskyOp&lt;complex128&gt;)&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new (CholeskyOp&lt;complex128&gt;)(context); });" data-ref="_M/REGISTER_LINALG_OP">REGISTER_LINALG_OP</a>(<q>"Cholesky"</q>, (<a class="type" href="#tensorflow::CholeskyOp" title='tensorflow::CholeskyOp' data-ref="tensorflow::CholeskyOp">CholeskyOp</a>&lt;<a class="typedef" href="../lib/bfloat16/bfloat16.h.html#tensorflow::complex128" title='tensorflow::complex128' data-type='std::complex&lt;double&gt;' data-ref="tensorflow::complex128">complex128</a>&gt;), <a class="typedef" href="../lib/bfloat16/bfloat16.h.html#tensorflow::complex128" title='tensorflow::complex128' data-type='std::complex&lt;double&gt;' data-ref="tensorflow::complex128">complex128</a>);</td></tr>
<tr><th id="181">181</th><td><a class="macro" href="linalg_ops_common.h.html#194" title="constexpr bool should_register_8__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__8__object( should_register_8__flag ? ::tensorflow::register_kernel::Name(&quot;BatchCholesky&quot;).Device(DEVICE_CPU).TypeConstraint&lt;float&gt;(&quot;T&quot;).Build() : nullptr, &quot;(CholeskyOp&lt;float&gt;)&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new (CholeskyOp&lt;float&gt;)(context); });" data-ref="_M/REGISTER_LINALG_OP">REGISTER_LINALG_OP</a>(<q>"BatchCholesky"</q>, (<a class="type" href="#tensorflow::CholeskyOp" title='tensorflow::CholeskyOp' data-ref="tensorflow::CholeskyOp">CholeskyOp</a>&lt;<em>float</em>&gt;), <em>float</em>);</td></tr>
<tr><th id="182">182</th><td><a class="macro" href="linalg_ops_common.h.html#194" title="constexpr bool should_register_10__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__10__object( should_register_10__flag ? ::tensorflow::register_kernel::Name(&quot;BatchCholesky&quot;).Device(DEVICE_CPU).TypeConstraint&lt;double&gt;(&quot;T&quot;).Build() : nullptr, &quot;(CholeskyOp&lt;double&gt;)&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new (CholeskyOp&lt;double&gt;)(context); });" data-ref="_M/REGISTER_LINALG_OP">REGISTER_LINALG_OP</a>(<q>"BatchCholesky"</q>, (<a class="type" href="#tensorflow::CholeskyOp" title='tensorflow::CholeskyOp' data-ref="tensorflow::CholeskyOp">CholeskyOp</a>&lt;<em>double</em>&gt;), <em>double</em>);</td></tr>
<tr><th id="183">183</th><td></td></tr>
<tr><th id="184">184</th><td>}  <i>// namespace tensorflow</i></td></tr>
<tr><th id="185">185</th><td></td></tr>
</table><hr/><p id='footer'>
Generated on <em>2018-Aug-16</em> from project tensorflow revision <em>v1.8</em><br />Powered by <a href='https://woboq.com'><img alt='Woboq' src='https://code.woboq.org/woboq-16.png' width='41' height='16' /></a> <a href='https://code.woboq.org'>Code Browser</a> 2.1
<br/>Generator usage only permitted with license.</p>
</div></body></html>
