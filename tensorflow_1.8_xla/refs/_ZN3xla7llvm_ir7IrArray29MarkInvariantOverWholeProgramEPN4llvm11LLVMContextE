<def f='tensorflow/tensorflow/compiler/xla/service/llvm_ir/ir_array.h' l='263' ll='270' type='void xla::llvm_ir::IrArray::MarkInvariantOverWholeProgram(llvm::LLVMContext * context)'/>
<doc f='tensorflow/tensorflow/compiler/xla/service/llvm_ir/ir_array.h' l='243'>// Promises LLVM that the data pointed to by this IrArray never changes after
  // it&apos;s first loaded.
  //
  // The temporal scope of this promise is the &quot;whole program&quot; from LLVM&apos;s point
  // of view, but how this translates to HLOs differs between backends.
  //
  // In the single-threaded CPU backend, we emit one function that
  // runs all the HLOs in sequence, so the whole program is the whole HLO
  // module.
  //
  // In the GPU backend, we emit one GPU kernel per top-level HLO (i.e. per HLO
  // in the entry computation).  From LLVM&apos;s perspective, launching a new kernel
  // is like launching a new program, and so the whole program is one top-level
  // HLO.  Since the scope of the promise is smaller than in the CPU backend, we
  // can mark more things as invariant in the GPU backend.
  //
  // Marking loads as invariant is particularly helpful on GPUs because
  // invariant loads can be lowered to PTX ld.global.nc (equivalent to CUDA&apos;s
  // __ldg intrinsic).  These loads use a special cache, and can be
  // significantly faster than regular loads.</doc>
<use f='tensorflow/tensorflow/compiler/xla/service/llvm_ir/alias_analysis.cc' l='86' u='c' c='_ZN3xla7llvm_ir13AliasAnalysis31AddAliasingInformationToIrArrayERKNS_14HloInstructionEPNS0_7IrArrayE'/>
