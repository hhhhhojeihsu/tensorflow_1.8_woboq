<!doctype html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><title>conv_grad_input_ops.cc source code [tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc] - Woboq Code Browser</title>
<meta name="woboq:interestingDefinitions" content="tensorflow::Conv2DCustomBackpropInputOp,tensorflow::Conv2DFastBackpropInputOp,tensorflow::LaunchConv2DBackpropInputOp "/>
<link rel="stylesheet" href="https://code.woboq.org/data/qtcreator.css" title="QtCreator"/>
<link rel="alternate stylesheet" href="https://code.woboq.org/data/kdevelop.css" title="KDevelop"/>
<script type="text/javascript" src="https://code.woboq.org/data/jquery/jquery.min.js"></script>
<script type="text/javascript" src="https://code.woboq.org/data/jquery/jquery-ui.min.js"></script>
<script>var file = 'tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc'; var root_path = '../../../..'; var data_path = 'https://code.woboq.org/data';</script>
<script src='https://code.woboq.org/data/codebrowser.js'></script>
</head>
<body><div id='header'><h1 id='breadcrumb'><span>Browse the source code of </span><a href='../../..'>tensorflow</a>/<a href='../..'>tensorflow</a>/<a href='..'>core</a>/<a href='./'>kernels</a>/<a href='conv_grad_input_ops.cc.html'>conv_grad_input_ops.cc</a></h1></div>
<hr/><div id='content'><table class="code">
<tr><th id="1">1</th><td><i>/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.</i></td></tr>
<tr><th id="2">2</th><td><i></i></td></tr>
<tr><th id="3">3</th><td><i>Licensed under the Apache License, Version 2.0 (the "License");</i></td></tr>
<tr><th id="4">4</th><td><i>you may not use this file except in compliance with the License.</i></td></tr>
<tr><th id="5">5</th><td><i>You may obtain a copy of the License at</i></td></tr>
<tr><th id="6">6</th><td><i></i></td></tr>
<tr><th id="7">7</th><td><i>    <a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></i></td></tr>
<tr><th id="8">8</th><td><i></i></td></tr>
<tr><th id="9">9</th><td><i>Unless required by applicable law or agreed to in writing, software</i></td></tr>
<tr><th id="10">10</th><td><i>distributed under the License is distributed on an "AS IS" BASIS,</i></td></tr>
<tr><th id="11">11</th><td><i>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</i></td></tr>
<tr><th id="12">12</th><td><i>See the License for the specific language governing permissions and</i></td></tr>
<tr><th id="13">13</th><td><i>limitations under the License.</i></td></tr>
<tr><th id="14">14</th><td><i>==============================================================================*/</i></td></tr>
<tr><th id="15">15</th><td><i></i></td></tr>
<tr><th id="16">16</th><td><i>// See docs in ../ops/nn_ops.cc.</i></td></tr>
<tr><th id="17">17</th><td></td></tr>
<tr><th id="18">18</th><td><u>#define <dfn class="macro" id="_M/USE_EIGEN_TENSOR" data-ref="_M/USE_EIGEN_TENSOR">USE_EIGEN_TENSOR</dfn></u></td></tr>
<tr><th id="19">19</th><td><u>#define <dfn class="macro" id="_M/EIGEN_USE_THREADS" data-ref="_M/EIGEN_USE_THREADS">EIGEN_USE_THREADS</dfn></u></td></tr>
<tr><th id="20">20</th><td></td></tr>
<tr><th id="21">21</th><td><u>#include <a href="conv_grad_ops.h.html">"tensorflow/core/kernels/conv_grad_ops.h"</a></u></td></tr>
<tr><th id="22">22</th><td></td></tr>
<tr><th id="23">23</th><td><u>#include <a href="../../../../include/c++/5/algorithm.html">&lt;algorithm&gt;</a></u></td></tr>
<tr><th id="24">24</th><td><u>#include <a href="../../../../include/c++/5/vector.html">&lt;vector&gt;</a></u></td></tr>
<tr><th id="25">25</th><td></td></tr>
<tr><th id="26">26</th><td><u>#include <a href="../framework/numeric_op.h.html">"tensorflow/core/framework/numeric_op.h"</a></u></td></tr>
<tr><th id="27">27</th><td><u>#include <a href="../framework/op_kernel.h.html">"tensorflow/core/framework/op_kernel.h"</a></u></td></tr>
<tr><th id="28">28</th><td><u>#include <a href="../framework/register_types.h.html">"tensorflow/core/framework/register_types.h"</a></u></td></tr>
<tr><th id="29">29</th><td><u>#include <a href="../framework/tensor.h.html">"tensorflow/core/framework/tensor.h"</a></u></td></tr>
<tr><th id="30">30</th><td><u>#include <a href="../framework/tensor_shape.h.html">"tensorflow/core/framework/tensor_shape.h"</a></u></td></tr>
<tr><th id="31">31</th><td><u>#include <a href="../framework/tensor_slice.h.html">"tensorflow/core/framework/tensor_slice.h"</a></u></td></tr>
<tr><th id="32">32</th><td><u>#include <a href="conv_2d.h.html">"tensorflow/core/kernels/conv_2d.h"</a></u></td></tr>
<tr><th id="33">33</th><td><u>#<span data-ppcond="33">ifdef</span> <span class="macro" data-ref="_M/TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS">TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS</span></u></td></tr>
<tr><th id="34">34</th><td><u>#include "tensorflow/core/kernels/xsmm_conv2d.h"</u></td></tr>
<tr><th id="35">35</th><td><u>#<span data-ppcond="33">endif</span></u></td></tr>
<tr><th id="36">36</th><td><u>#include <a href="ops_util.h.html">"tensorflow/core/kernels/ops_util.h"</a></u></td></tr>
<tr><th id="37">37</th><td><u>#include <a href="../lib/core/errors.h.html">"tensorflow/core/lib/core/errors.h"</a></u></td></tr>
<tr><th id="38">38</th><td><u>#include <a href="../lib/gtl/array_slice.h.html">"tensorflow/core/lib/gtl/array_slice.h"</a></u></td></tr>
<tr><th id="39">39</th><td><u>#include <a href="../platform/logging.h.html">"tensorflow/core/platform/logging.h"</a></u></td></tr>
<tr><th id="40">40</th><td><u>#include <a href="../platform/macros.h.html">"tensorflow/core/platform/macros.h"</a></u></td></tr>
<tr><th id="41">41</th><td><u>#include <a href="../util/padding.h.html">"tensorflow/core/util/padding.h"</a></u></td></tr>
<tr><th id="42">42</th><td><u>#include <a href="../util/tensor_format.h.html">"tensorflow/core/util/tensor_format.h"</a></u></td></tr>
<tr><th id="43">43</th><td><u>#include <a href="../util/use_cudnn.h.html">"tensorflow/core/util/use_cudnn.h"</a></u></td></tr>
<tr><th id="44">44</th><td><u>#include <a href="../util/work_sharder.h.html">"tensorflow/core/util/work_sharder.h"</a></u></td></tr>
<tr><th id="45">45</th><td></td></tr>
<tr><th id="46">46</th><td><u>#<span data-ppcond="46">if</span> GOOGLE_CUDA</u></td></tr>
<tr><th id="47">47</th><td><u>#include "tensorflow/core/kernels/conv_ops_gpu.h"</u></td></tr>
<tr><th id="48">48</th><td><u>#include "tensorflow/core/platform/stream_executor.h"</u></td></tr>
<tr><th id="49">49</th><td><u>#<span data-ppcond="46">endif</span>  // GOOGLE_CUDA</u></td></tr>
<tr><th id="50">50</th><td></td></tr>
<tr><th id="51">51</th><td><b>namespace</b> {</td></tr>
<tr><th id="52">52</th><td></td></tr>
<tr><th id="53">53</th><td><i>// Returns in 'im_data' (assumes to be zero-initialized) image patch in storage</i></td></tr>
<tr><th id="54">54</th><td><i>// order (height, width, depth), constructed from patches in 'col_data', which</i></td></tr>
<tr><th id="55">55</th><td><i>// is required to be in storage order (out_height * out_width, filter_height,</i></td></tr>
<tr><th id="56">56</th><td><i>// filter_width, in_depth).  Implementation by Yangqing Jia (jiayq).</i></td></tr>
<tr><th id="57">57</th><td><b>template</b> &lt;<b>typename</b> T&gt;</td></tr>
<tr><th id="58">58</th><td><em>void</em> <dfn class="tu decl def" id="_ZN12_GLOBAL__N_16Col2imEPKT_iiiiiiiiiiiPS0_" title='(anonymous namespace)::Col2im' data-type='void (anonymous namespace)::Col2im(const T * col_data, const int depth, const int height, const int width, const int filter_h, const int filter_w, const int pad_t, const int pad_l, const int pad_b, const int pad_r, const int stride_h, const int stride_w, T * im_data)' data-ref="_ZN12_GLOBAL__N_16Col2imEPKT_iiiiiiiiiiiPS0_">Col2im</dfn>(<em>const</em> T* <dfn class="local col1 decl" id="1col_data" title='col_data' data-type='const T *' data-ref="1col_data">col_data</dfn>, <em>const</em> <em>int</em> <dfn class="local col2 decl" id="2depth" title='depth' data-type='const int' data-ref="2depth">depth</dfn>, <em>const</em> <em>int</em> <dfn class="local col3 decl" id="3height" title='height' data-type='const int' data-ref="3height">height</dfn>,</td></tr>
<tr><th id="59">59</th><td>            <em>const</em> <em>int</em> <dfn class="local col4 decl" id="4width" title='width' data-type='const int' data-ref="4width">width</dfn>, <em>const</em> <em>int</em> <dfn class="local col5 decl" id="5filter_h" title='filter_h' data-type='const int' data-ref="5filter_h">filter_h</dfn>, <em>const</em> <em>int</em> <dfn class="local col6 decl" id="6filter_w" title='filter_w' data-type='const int' data-ref="6filter_w">filter_w</dfn>,</td></tr>
<tr><th id="60">60</th><td>            <em>const</em> <em>int</em> <dfn class="local col7 decl" id="7pad_t" title='pad_t' data-type='const int' data-ref="7pad_t">pad_t</dfn>, <em>const</em> <em>int</em> <dfn class="local col8 decl" id="8pad_l" title='pad_l' data-type='const int' data-ref="8pad_l">pad_l</dfn>, <em>const</em> <em>int</em> <dfn class="local col9 decl" id="9pad_b" title='pad_b' data-type='const int' data-ref="9pad_b">pad_b</dfn>, <em>const</em> <em>int</em> <dfn class="local col0 decl" id="10pad_r" title='pad_r' data-type='const int' data-ref="10pad_r">pad_r</dfn>,</td></tr>
<tr><th id="61">61</th><td>            <em>const</em> <em>int</em> <dfn class="local col1 decl" id="11stride_h" title='stride_h' data-type='const int' data-ref="11stride_h">stride_h</dfn>, <em>const</em> <em>int</em> <dfn class="local col2 decl" id="12stride_w" title='stride_w' data-type='const int' data-ref="12stride_w">stride_w</dfn>, T* <dfn class="local col3 decl" id="13im_data" title='im_data' data-type='T *' data-ref="13im_data">im_data</dfn>) {</td></tr>
<tr><th id="62">62</th><td>  <em>int</em> <dfn class="local col4 decl" id="14height_col" title='height_col' data-type='int' data-ref="14height_col">height_col</dfn> = (<a class="local col3 ref" href="#3height" title='height' data-ref="3height">height</a> + <a class="local col7 ref" href="#7pad_t" title='pad_t' data-ref="7pad_t">pad_t</a> + <a class="local col9 ref" href="#9pad_b" title='pad_b' data-ref="9pad_b">pad_b</a> - <a class="local col5 ref" href="#5filter_h" title='filter_h' data-ref="5filter_h">filter_h</a>) / <a class="local col1 ref" href="#11stride_h" title='stride_h' data-ref="11stride_h">stride_h</a> + <var>1</var>;</td></tr>
<tr><th id="63">63</th><td>  <em>int</em> <dfn class="local col5 decl" id="15width_col" title='width_col' data-type='int' data-ref="15width_col">width_col</dfn> = (<a class="local col4 ref" href="#4width" title='width' data-ref="4width">width</a> + <a class="local col8 ref" href="#8pad_l" title='pad_l' data-ref="8pad_l">pad_l</a> + <a class="local col0 ref" href="#10pad_r" title='pad_r' data-ref="10pad_r">pad_r</a> - <a class="local col6 ref" href="#6filter_w" title='filter_w' data-ref="6filter_w">filter_w</a>) / <a class="local col2 ref" href="#12stride_w" title='stride_w' data-ref="12stride_w">stride_w</a> + <var>1</var>;</td></tr>
<tr><th id="64">64</th><td>  <em>int</em> <dfn class="local col6 decl" id="16h_pad" title='h_pad' data-type='int' data-ref="16h_pad">h_pad</dfn> = -<a class="local col7 ref" href="#7pad_t" title='pad_t' data-ref="7pad_t">pad_t</a>;</td></tr>
<tr><th id="65">65</th><td>  <b>for</b> (<em>int</em> <dfn class="local col7 decl" id="17h" title='h' data-type='int' data-ref="17h">h</dfn> = <var>0</var>; <a class="local col7 ref" href="#17h" title='h' data-ref="17h">h</a> &lt; <a class="local col4 ref" href="#14height_col" title='height_col' data-ref="14height_col">height_col</a>; ++<a class="local col7 ref" href="#17h" title='h' data-ref="17h">h</a>) {</td></tr>
<tr><th id="66">66</th><td>    <em>int</em> <dfn class="local col8 decl" id="18w_pad" title='w_pad' data-type='int' data-ref="18w_pad">w_pad</dfn> = -<a class="local col8 ref" href="#8pad_l" title='pad_l' data-ref="8pad_l">pad_l</a>;</td></tr>
<tr><th id="67">67</th><td>    <b>for</b> (<em>int</em> <dfn class="local col9 decl" id="19w" title='w' data-type='int' data-ref="19w">w</dfn> = <var>0</var>; <a class="local col9 ref" href="#19w" title='w' data-ref="19w">w</a> &lt; <a class="local col5 ref" href="#15width_col" title='width_col' data-ref="15width_col">width_col</a>; ++<a class="local col9 ref" href="#19w" title='w' data-ref="19w">w</a>) {</td></tr>
<tr><th id="68">68</th><td>      T* <dfn class="local col0 decl" id="20im_patch_data" title='im_patch_data' data-type='T *' data-ref="20im_patch_data">im_patch_data</dfn> = <a class="local col3 ref" href="#13im_data" title='im_data' data-ref="13im_data">im_data</a> + (<a class="local col6 ref" href="#16h_pad" title='h_pad' data-ref="16h_pad">h_pad</a> * <a class="local col4 ref" href="#4width" title='width' data-ref="4width">width</a> + <a class="local col8 ref" href="#18w_pad" title='w_pad' data-ref="18w_pad">w_pad</a>) * <a class="local col2 ref" href="#2depth" title='depth' data-ref="2depth">depth</a>;</td></tr>
<tr><th id="69">69</th><td>      <b>for</b> (<em>int</em> <dfn class="local col1 decl" id="21ih" title='ih' data-type='int' data-ref="21ih">ih</dfn> = <a class="local col6 ref" href="#16h_pad" title='h_pad' data-ref="16h_pad">h_pad</a>; <a class="local col1 ref" href="#21ih" title='ih' data-ref="21ih">ih</a> &lt; <a class="local col6 ref" href="#16h_pad" title='h_pad' data-ref="16h_pad">h_pad</a> + <a class="local col5 ref" href="#5filter_h" title='filter_h' data-ref="5filter_h">filter_h</a>; ++<a class="local col1 ref" href="#21ih" title='ih' data-ref="21ih">ih</a>) {</td></tr>
<tr><th id="70">70</th><td>        <b>for</b> (<em>int</em> <dfn class="local col2 decl" id="22iw" title='iw' data-type='int' data-ref="22iw">iw</dfn> = <a class="local col8 ref" href="#18w_pad" title='w_pad' data-ref="18w_pad">w_pad</a>; <a class="local col2 ref" href="#22iw" title='iw' data-ref="22iw">iw</a> &lt; <a class="local col8 ref" href="#18w_pad" title='w_pad' data-ref="18w_pad">w_pad</a> + <a class="local col6 ref" href="#6filter_w" title='filter_w' data-ref="6filter_w">filter_w</a>; ++<a class="local col2 ref" href="#22iw" title='iw' data-ref="22iw">iw</a>) {</td></tr>
<tr><th id="71">71</th><td>          <b>if</b> (<a class="local col1 ref" href="#21ih" title='ih' data-ref="21ih">ih</a> &gt;= <var>0</var> &amp;&amp; <a class="local col1 ref" href="#21ih" title='ih' data-ref="21ih">ih</a> &lt; <a class="local col3 ref" href="#3height" title='height' data-ref="3height">height</a> &amp;&amp; <a class="local col2 ref" href="#22iw" title='iw' data-ref="22iw">iw</a> &gt;= <var>0</var> &amp;&amp; <a class="local col2 ref" href="#22iw" title='iw' data-ref="22iw">iw</a> &lt; <a class="local col4 ref" href="#4width" title='width' data-ref="4width">width</a>) {</td></tr>
<tr><th id="72">72</th><td>            <i>// TODO(andydavis) Vectorize this loop (if compiler does not).</i></td></tr>
<tr><th id="73">73</th><td>            <b>for</b> (<em>int</em> <dfn class="local col3 decl" id="23i" title='i' data-type='int' data-ref="23i">i</dfn> = <var>0</var>; <a class="local col3 ref" href="#23i" title='i' data-ref="23i">i</a> &lt; <a class="local col2 ref" href="#2depth" title='depth' data-ref="2depth">depth</a>; ++<a class="local col3 ref" href="#23i" title='i' data-ref="23i">i</a>) {</td></tr>
<tr><th id="74">74</th><td>              <a class="local col0 ref" href="#20im_patch_data" title='im_patch_data' data-ref="20im_patch_data">im_patch_data</a>[<a class="local col3 ref" href="#23i" title='i' data-ref="23i">i</a>] += <a class="local col1 ref" href="#1col_data" title='col_data' data-ref="1col_data">col_data</a>[<a class="local col3 ref" href="#23i" title='i' data-ref="23i">i</a>];</td></tr>
<tr><th id="75">75</th><td>            }</td></tr>
<tr><th id="76">76</th><td>          }</td></tr>
<tr><th id="77">77</th><td>          <a class="local col0 ref" href="#20im_patch_data" title='im_patch_data' data-ref="20im_patch_data">im_patch_data</a> += <a class="local col2 ref" href="#2depth" title='depth' data-ref="2depth">depth</a>;</td></tr>
<tr><th id="78">78</th><td>          <a class="local col1 ref" href="#1col_data" title='col_data' data-ref="1col_data">col_data</a> += <a class="local col2 ref" href="#2depth" title='depth' data-ref="2depth">depth</a>;</td></tr>
<tr><th id="79">79</th><td>        }</td></tr>
<tr><th id="80">80</th><td>        <i>// Jump over remaining number of depth.</i></td></tr>
<tr><th id="81">81</th><td>        <a class="local col0 ref" href="#20im_patch_data" title='im_patch_data' data-ref="20im_patch_data">im_patch_data</a> += <a class="local col2 ref" href="#2depth" title='depth' data-ref="2depth">depth</a> * (<a class="local col4 ref" href="#4width" title='width' data-ref="4width">width</a> - <a class="local col6 ref" href="#6filter_w" title='filter_w' data-ref="6filter_w">filter_w</a>);</td></tr>
<tr><th id="82">82</th><td>      }</td></tr>
<tr><th id="83">83</th><td>      <a class="local col8 ref" href="#18w_pad" title='w_pad' data-ref="18w_pad">w_pad</a> += <a class="local col2 ref" href="#12stride_w" title='stride_w' data-ref="12stride_w">stride_w</a>;</td></tr>
<tr><th id="84">84</th><td>    }</td></tr>
<tr><th id="85">85</th><td>    <a class="local col6 ref" href="#16h_pad" title='h_pad' data-ref="16h_pad">h_pad</a> += <a class="local col1 ref" href="#11stride_h" title='stride_h' data-ref="11stride_h">stride_h</a>;</td></tr>
<tr><th id="86">86</th><td>  }</td></tr>
<tr><th id="87">87</th><td>}</td></tr>
<tr><th id="88">88</th><td></td></tr>
<tr><th id="89">89</th><td>}  <i>// namespace</i></td></tr>
<tr><th id="90">90</th><td></td></tr>
<tr><th id="91">91</th><td><b>namespace</b> <span class="namespace">tensorflow</span> {</td></tr>
<tr><th id="92">92</th><td></td></tr>
<tr><th id="93">93</th><td><b>typedef</b> <span class="namespace">Eigen::</span><span class='type' title='Eigen::ThreadPoolDevice' data-ref="Eigen::ThreadPoolDevice">ThreadPoolDevice</span> <dfn class="typedef" id="tensorflow::CPUDevice" title='tensorflow::CPUDevice' data-type='Eigen::ThreadPoolDevice' data-ref="tensorflow::CPUDevice">CPUDevice</dfn>;</td></tr>
<tr><th id="94">94</th><td><b>typedef</b> <span class="namespace">Eigen::</span><span class='type' title='Eigen::GpuDevice' data-ref="Eigen::GpuDevice">GpuDevice</span> <dfn class="typedef" id="tensorflow::GPUDevice" title='tensorflow::GPUDevice' data-type='Eigen::GpuDevice' data-ref="tensorflow::GPUDevice">GPUDevice</dfn>;</td></tr>
<tr><th id="95">95</th><td></td></tr>
<tr><th id="96">96</th><td><i>// The fast versions using eigen computations directly. They are only enabled</i></td></tr>
<tr><th id="97">97</th><td><i>// for CPU for now since nvcc times out when trying to compile them.</i></td></tr>
<tr><th id="98">98</th><td><i>// TODO(yangke): enable them for GPUs when we have a faster compiler.</i></td></tr>
<tr><th id="99">99</th><td></td></tr>
<tr><th id="100">100</th><td><b>template</b> &lt;<b>typename</b> T&gt;</td></tr>
<tr><th id="101">101</th><td><b>struct</b> <dfn class="type def" id="tensorflow::LaunchConv2DBackpropInputOp" title='tensorflow::LaunchConv2DBackpropInputOp' data-ref="tensorflow::LaunchConv2DBackpropInputOp">LaunchConv2DBackpropInputOp</dfn>&lt;<a class="typedef" href="#tensorflow::CPUDevice" title='tensorflow::CPUDevice' data-type='Eigen::ThreadPoolDevice' data-ref="tensorflow::CPUDevice">CPUDevice</a>, T&gt; {</td></tr>
<tr><th id="102">102</th><td>  <em>void</em> <dfn class="tu decl def" id="_ZN10tensorflow27LaunchConv2DBackpropInputOpIN5Eigen16ThreadPoolDeviceET_EclEPNS_15OpKernelContextEbbRKNS_6TensorES9_iiRKNS_7PaddingEPS7_NS_12TensorFormatE" title='tensorflow::LaunchConv2DBackpropInputOp&lt;Eigen::ThreadPoolDevice, type-parameter-0-0&gt;::operator()' data-type='void tensorflow::LaunchConv2DBackpropInputOp&lt;Eigen::ThreadPoolDevice, type-parameter-0-0&gt;::operator()(tensorflow::OpKernelContext * ctx, bool use_cudnn, bool cudnn_use_autotune, const tensorflow::Tensor &amp; out_backprop, const tensorflow::Tensor &amp; filter, int row_stride, int col_stride, const tensorflow::Padding &amp; padding, tensorflow::Tensor * in_backprop, tensorflow::TensorFormat data_format)' data-ref="_ZN10tensorflow27LaunchConv2DBackpropInputOpIN5Eigen16ThreadPoolDeviceET_EclEPNS_15OpKernelContextEbbRKNS_6TensorES9_iiRKNS_7PaddingEPS7_NS_12TensorFormatE"><b>operator</b>()</dfn>(<a class="type" href="../framework/op_kernel.h.html#tensorflow::OpKernelContext" title='tensorflow::OpKernelContext' data-ref="tensorflow::OpKernelContext">OpKernelContext</a>* <dfn class="local col4 decl" id="24ctx" title='ctx' data-type='tensorflow::OpKernelContext *' data-ref="24ctx">ctx</dfn>, <em>bool</em> <dfn class="local col5 decl" id="25use_cudnn" title='use_cudnn' data-type='bool' data-ref="25use_cudnn">use_cudnn</dfn>, <em>bool</em> <dfn class="local col6 decl" id="26cudnn_use_autotune" title='cudnn_use_autotune' data-type='bool' data-ref="26cudnn_use_autotune">cudnn_use_autotune</dfn>,</td></tr>
<tr><th id="103">103</th><td>                  <em>const</em> <a class="type" href="../framework/tensor.h.html#tensorflow::Tensor" title='tensorflow::Tensor' data-ref="tensorflow::Tensor">Tensor</a>&amp; <dfn class="local col7 decl" id="27out_backprop" title='out_backprop' data-type='const tensorflow::Tensor &amp;' data-ref="27out_backprop">out_backprop</dfn>, <em>const</em> <a class="type" href="../framework/tensor.h.html#tensorflow::Tensor" title='tensorflow::Tensor' data-ref="tensorflow::Tensor">Tensor</a>&amp; <dfn class="local col8 decl" id="28filter" title='filter' data-type='const tensorflow::Tensor &amp;' data-ref="28filter">filter</dfn>,</td></tr>
<tr><th id="104">104</th><td>                  <em>int</em> <dfn class="local col9 decl" id="29row_stride" title='row_stride' data-type='int' data-ref="29row_stride">row_stride</dfn>, <em>int</em> <dfn class="local col0 decl" id="30col_stride" title='col_stride' data-type='int' data-ref="30col_stride">col_stride</dfn>, <em>const</em> <a class="type" href="../util/padding.h.html#tensorflow::Padding" title='tensorflow::Padding' data-ref="tensorflow::Padding">Padding</a>&amp; <dfn class="local col1 decl" id="31padding" title='padding' data-type='const tensorflow::Padding &amp;' data-ref="31padding">padding</dfn>,</td></tr>
<tr><th id="105">105</th><td>                  <a class="type" href="../framework/tensor.h.html#tensorflow::Tensor" title='tensorflow::Tensor' data-ref="tensorflow::Tensor">Tensor</a>* <dfn class="local col2 decl" id="32in_backprop" title='in_backprop' data-type='tensorflow::Tensor *' data-ref="32in_backprop">in_backprop</dfn>, <a class="type" href="../util/tensor_format.h.html#tensorflow::TensorFormat" title='tensorflow::TensorFormat' data-ref="tensorflow::TensorFormat">TensorFormat</a> <dfn class="local col3 decl" id="33data_format" title='data_format' data-type='tensorflow::TensorFormat' data-ref="33data_format">data_format</dfn>) {</td></tr>
<tr><th id="106">106</th><td>    <em>const</em> <a class="typedef" href="#tensorflow::CPUDevice" title='tensorflow::CPUDevice' data-type='Eigen::ThreadPoolDevice' data-ref="tensorflow::CPUDevice">CPUDevice</a>&amp; <dfn class="local col4 decl" id="34d" title='d' data-type='const CPUDevice &amp;' data-ref="34d">d</dfn> = <a class="local col4 ref" href="#24ctx" title='ctx' data-ref="24ctx">ctx</a>-&gt;<a class="ref" href="../framework/op_kernel.h.html#_ZNK10tensorflow15OpKernelContext12eigen_deviceEv" title='tensorflow::OpKernelContext::eigen_device' data-ref="_ZNK10tensorflow15OpKernelContext12eigen_deviceEv">eigen_device</a>&lt;<a class="typedef" href="#tensorflow::CPUDevice" title='tensorflow::CPUDevice' data-type='Eigen::ThreadPoolDevice' data-ref="tensorflow::CPUDevice">CPUDevice</a>&gt;();</td></tr>
<tr><th id="107">107</th><td>    <span class="namespace">functor::</span><a class="type" href="conv_2d.h.html#tensorflow::functor::SpatialConvolutionBackwardInput" title='tensorflow::functor::SpatialConvolutionBackwardInput' data-ref="tensorflow::functor::SpatialConvolutionBackwardInput">SpatialConvolutionBackwardInput</a>&lt;<a class="typedef" href="#tensorflow::CPUDevice" title='tensorflow::CPUDevice' data-type='Eigen::ThreadPoolDevice' data-ref="tensorflow::CPUDevice">CPUDevice</a>, T&gt;()(</td></tr>
<tr><th id="108">108</th><td>        <a class="local col4 ref" href="#34d" title='d' data-ref="34d">d</a>, <a class="local col2 ref" href="#32in_backprop" title='in_backprop' data-ref="32in_backprop">in_backprop</a>-&gt;tensor&lt;T, <var>4</var>&gt;(), <a class="local col8 ref" href="#28filter" title='filter' data-ref="28filter">filter</a>.tensor&lt;T, <var>4</var>&gt;(),</td></tr>
<tr><th id="109">109</th><td>        <a class="local col7 ref" href="#27out_backprop" title='out_backprop' data-ref="27out_backprop">out_backprop</a>.tensor&lt;T, <var>4</var>&gt;(), <a class="local col9 ref" href="#29row_stride" title='row_stride' data-ref="29row_stride">row_stride</a>, <a class="local col0 ref" href="#30col_stride" title='col_stride' data-ref="30col_stride">col_stride</a>,</td></tr>
<tr><th id="110">110</th><td>        <i>/*row_dilation=*/</i><var>1</var>, <i>/*col_dilation=*/</i><var>1</var>);</td></tr>
<tr><th id="111">111</th><td>  }</td></tr>
<tr><th id="112">112</th><td>};</td></tr>
<tr><th id="113">113</th><td></td></tr>
<tr><th id="114">114</th><td><u>#<span data-ppcond="114">ifdef</span> <span class="macro" data-ref="_M/TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS">TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS</span></u></td></tr>
<tr><th id="115">115</th><td><b>template</b> &lt;<b>typename</b> Device, <b>class</b> T&gt;</td></tr>
<tr><th id="116">116</th><td><b>struct</b> LaunchXsmmBackwardInputConvolution {</td></tr>
<tr><th id="117">117</th><td>  <em>bool</em> <b>operator</b>()(OpKernelContext* context, <em>const</em> Device&amp; d,</td></tr>
<tr><th id="118">118</th><td>                  <b>typename</b> TTypes&lt;T, <var>4</var>&gt;::Tensor input_backward,</td></tr>
<tr><th id="119">119</th><td>                  <b>typename</b> TTypes&lt;T, <var>4</var>&gt;::ConstTensor kernel,</td></tr>
<tr><th id="120">120</th><td>                  <b>typename</b> TTypes&lt;T, <var>4</var>&gt;::ConstTensor output_backward,</td></tr>
<tr><th id="121">121</th><td>                  <em>int</em> input_rows, <em>int</em> input_cols, <em>int</em> row_stride,</td></tr>
<tr><th id="122">122</th><td>                  <em>int</em> col_stride, <em>int</em> pad_h, <em>int</em> pad_w,</td></tr>
<tr><th id="123">123</th><td>                  TensorFormat data_format) <em>const</em> {</td></tr>
<tr><th id="124">124</th><td>    <b>return</b> <b>false</b>;</td></tr>
<tr><th id="125">125</th><td>  }</td></tr>
<tr><th id="126">126</th><td>};</td></tr>
<tr><th id="127">127</th><td></td></tr>
<tr><th id="128">128</th><td><b>template</b> &lt;&gt;</td></tr>
<tr><th id="129">129</th><td><b>struct</b> LaunchXsmmBackwardInputConvolution&lt;CPUDevice, <em>float</em>&gt; {</td></tr>
<tr><th id="130">130</th><td>  <em>bool</em> <b>operator</b>()(OpKernelContext* context, <em>const</em> CPUDevice&amp; d,</td></tr>
<tr><th id="131">131</th><td>                  <b>typename</b> TTypes&lt;<em>float</em>, <var>4</var>&gt;::Tensor input_backward,</td></tr>
<tr><th id="132">132</th><td>                  <b>typename</b> TTypes&lt;<em>float</em>, <var>4</var>&gt;::ConstTensor kernel,</td></tr>
<tr><th id="133">133</th><td>                  <b>typename</b> TTypes&lt;<em>float</em>, <var>4</var>&gt;::ConstTensor output_backward,</td></tr>
<tr><th id="134">134</th><td>                  <em>int</em> input_rows, <em>int</em> input_cols, <em>int</em> row_stride,</td></tr>
<tr><th id="135">135</th><td>                  <em>int</em> col_stride, <em>int</em> pad_h, <em>int</em> pad_w,</td></tr>
<tr><th id="136">136</th><td>                  TensorFormat data_format) <em>const</em> {</td></tr>
<tr><th id="137">137</th><td>    <em>auto</em> batch = input_backward.dimension(<var>0</var>);</td></tr>
<tr><th id="138">138</th><td>    <em>auto</em> in_depth = input_backward.dimension(<var>3</var>);</td></tr>
<tr><th id="139">139</th><td>    <em>auto</em> out_depth = output_backward.dimension(<var>3</var>);</td></tr>
<tr><th id="140">140</th><td>    <em>auto</em> filter_rows = kernel.dimension(<var>0</var>);</td></tr>
<tr><th id="141">141</th><td>    <em>auto</em> filter_cols = kernel.dimension(<var>1</var>);</td></tr>
<tr><th id="142">142</th><td>    <em>auto</em> num_threads =</td></tr>
<tr><th id="143">143</th><td>        context-&gt;device()-&gt;tensorflow_cpu_worker_threads()-&gt;num_threads;</td></tr>
<tr><th id="144">144</th><td>    <i>// See libxsmm_dnn.h for this struct definition.</i></td></tr>
<tr><th id="145">145</th><td>    libxsmm_dnn_conv_desc desc;</td></tr>
<tr><th id="146">146</th><td>    desc.N = batch;</td></tr>
<tr><th id="147">147</th><td>    desc.C = in_depth;</td></tr>
<tr><th id="148">148</th><td>    desc.H = input_rows;</td></tr>
<tr><th id="149">149</th><td>    desc.W = input_cols;</td></tr>
<tr><th id="150">150</th><td>    desc.K = out_depth;</td></tr>
<tr><th id="151">151</th><td>    desc.R = filter_rows;</td></tr>
<tr><th id="152">152</th><td>    desc.S = filter_cols;</td></tr>
<tr><th id="153">153</th><td>    desc.u = row_stride;</td></tr>
<tr><th id="154">154</th><td>    desc.v = col_stride;</td></tr>
<tr><th id="155">155</th><td>    desc.pad_h = pad_h;</td></tr>
<tr><th id="156">156</th><td>    desc.pad_w = pad_w;</td></tr>
<tr><th id="157">157</th><td>    desc.pad_h_in = <var>0</var>;</td></tr>
<tr><th id="158">158</th><td>    desc.pad_w_in = <var>0</var>;</td></tr>
<tr><th id="159">159</th><td>    desc.pad_h_out = <var>0</var>;</td></tr>
<tr><th id="160">160</th><td>    desc.pad_w_out = <var>0</var>;</td></tr>
<tr><th id="161">161</th><td>    desc.threads = num_threads;</td></tr>
<tr><th id="162">162</th><td>    desc.algo = LIBXSMM_DNN_CONV_ALGO_DIRECT;</td></tr>
<tr><th id="163">163</th><td>    desc.buffer_format = LIBXSMM_DNN_TENSOR_FORMAT_NHWC;</td></tr>
<tr><th id="164">164</th><td>    desc.filter_format =</td></tr>
<tr><th id="165">165</th><td>        LIBXSMM_DNN_TENSOR_FORMAT_LIBXSMM;  <i>// LIBXSMM_DNN_TENSOR_FORMAT_RSCK;</i></td></tr>
<tr><th id="166">166</th><td>    desc.fuse_ops = LIBXSMM_DNN_CONV_FUSE_NONE;</td></tr>
<tr><th id="167">167</th><td>    desc.options = LIBXSMM_DNN_CONV_OPTION_WU_EXT_FILTER_REDUCE_OVERWRITE;</td></tr>
<tr><th id="168">168</th><td>    desc.datatype = LIBXSMM_DNN_DATATYPE_F32;</td></tr>
<tr><th id="169">169</th><td></td></tr>
<tr><th id="170">170</th><td>    <em>auto</em> input_ptr = input_backward.data();</td></tr>
<tr><th id="171">171</th><td>    <em>auto</em> filter_ptr = kernel.data();</td></tr>
<tr><th id="172">172</th><td>    <em>auto</em> output_ptr = output_backward.data();</td></tr>
<tr><th id="173">173</th><td></td></tr>
<tr><th id="174">174</th><td>    <em>bool</em> success = functor::XsmmBkwInputConv2D&lt;CPUDevice, <em>float</em>&gt;()(</td></tr>
<tr><th id="175">175</th><td>        context, desc, input_ptr, filter_ptr, output_ptr);</td></tr>
<tr><th id="176">176</th><td>    <b>return</b> success;</td></tr>
<tr><th id="177">177</th><td>  }</td></tr>
<tr><th id="178">178</th><td>};</td></tr>
<tr><th id="179">179</th><td><u>#<span data-ppcond="114">endif</span></u></td></tr>
<tr><th id="180">180</th><td></td></tr>
<tr><th id="181">181</th><td><b>template</b> &lt;<b>typename</b> Device, <b>class</b> T&gt;</td></tr>
<tr><th id="182">182</th><td><b>class</b> <dfn class="type def" id="tensorflow::Conv2DFastBackpropInputOp" title='tensorflow::Conv2DFastBackpropInputOp' data-ref="tensorflow::Conv2DFastBackpropInputOp">Conv2DFastBackpropInputOp</dfn> : <b>public</b> <a class="type" href="../framework/op_kernel.h.html#tensorflow::OpKernel" title='tensorflow::OpKernel' data-ref="tensorflow::OpKernel">OpKernel</a> {</td></tr>
<tr><th id="183">183</th><td> <b>public</b>:</td></tr>
<tr><th id="184">184</th><td>  <b>explicit</b> <dfn class="tu decl def" id="_ZN10tensorflow25Conv2DFastBackpropInputOpC1EPNS_20OpKernelConstructionE" title='tensorflow::Conv2DFastBackpropInputOp::Conv2DFastBackpropInputOp&lt;Device, T&gt;' data-type='void tensorflow::Conv2DFastBackpropInputOp::Conv2DFastBackpropInputOp&lt;Device, T&gt;(tensorflow::OpKernelConstruction * context)' data-ref="_ZN10tensorflow25Conv2DFastBackpropInputOpC1EPNS_20OpKernelConstructionE">Conv2DFastBackpropInputOp</dfn>(<a class="type" href="../framework/op_kernel.h.html#tensorflow::OpKernelConstruction" title='tensorflow::OpKernelConstruction' data-ref="tensorflow::OpKernelConstruction">OpKernelConstruction</a>* <dfn class="local col5 decl" id="35context" title='context' data-type='tensorflow::OpKernelConstruction *' data-ref="35context">context</dfn>)</td></tr>
<tr><th id="185">185</th><td>      : <a class="type" href="../framework/op_kernel.h.html#tensorflow::OpKernel" title='tensorflow::OpKernel' data-ref="tensorflow::OpKernel">OpKernel</a>(<a class="local col5 ref" href="#35context" title='context' data-ref="35context">context</a>) {</td></tr>
<tr><th id="186">186</th><td>    <a class="typedef" href="../../../../include/c++/5/bits/stringfwd.h.html#std::string" title='std::string' data-type='basic_string&lt;char&gt;' data-ref="std::string">string</a> <a class="ref fake" href="../../../../include/c++/5/bits/basic_string.h.html#_ZNSt12basic_stringC1Ev" title='std::basic_string::basic_string&lt;_CharT, _Traits, _Alloc&gt;' data-ref="_ZNSt12basic_stringC1Ev"></a><dfn class="local col6 decl" id="36data_format" title='data_format' data-type='string' data-ref="36data_format">data_format</dfn>;</td></tr>
<tr><th id="187">187</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1545" title="do { ::tensorflow::Status _s(context-&gt;GetAttr(&quot;data_format&quot;, &amp;data_format)); if (!(__builtin_expect(!!(_s.ok()), 1))) { (context)-&gt;CtxFailureWithWarning(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 187, _s); return; } } while (0)" data-ref="_M/OP_REQUIRES_OK">OP_REQUIRES_OK</a>(<a class="local col5 ref" href="#35context" title='context' data-ref="35context">context</a>, <a class="local col5 ref" href="#35context" title='context' data-ref="35context">context</a>-&gt;<a class="ref" href="../framework/op_kernel.h.html#_ZNK10tensorflow20OpKernelConstruction7GetAttrENS_11StringPieceEPT_" title='tensorflow::OpKernelConstruction::GetAttr' data-ref="_ZNK10tensorflow20OpKernelConstruction7GetAttrENS_11StringPieceEPT_">GetAttr</a>(<a class="ref fake" href="../lib/core/stringpiece.h.html#_ZN10tensorflow11StringPieceC1EPKc" title='tensorflow::StringPiece::StringPiece' data-ref="_ZN10tensorflow11StringPieceC1EPKc"></a><q>"data_format"</q>, &amp;<a class="local col6 ref" href="#36data_format" title='data_format' data-ref="36data_format">data_format</a>));</td></tr>
<tr><th id="188">188</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1537" title="do { if (!(__builtin_expect(!!(FormatFromString(data_format, &amp;data_format_)), 1))) { (context)-&gt;CtxFailure(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 189, (errors::InvalidArgument(&quot;Invalid data format&quot;))); return; } } while (0)" data-ref="_M/OP_REQUIRES">OP_REQUIRES</a>(<a class="local col5 ref" href="#35context" title='context' data-ref="35context">context</a>, FormatFromString(<a class="local col6 ref" href="#36data_format" title='data_format' data-ref="36data_format">data_format</a>, &amp;<a class="tu member" href="#tensorflow::Conv2DFastBackpropInputOp::data_format_" title='tensorflow::Conv2DFastBackpropInputOp::data_format_' data-use='a' data-ref="tensorflow::Conv2DFastBackpropInputOp::data_format_">data_format_</a>),</td></tr>
<tr><th id="189">189</th><td>                errors::<a class="ref" href="../lib/core/errors.h.html#103" title='tensorflow::errors::InvalidArgument' data-ref="_ZN10tensorflow6errors15InvalidArgumentEDpT_">InvalidArgument</a>(<q>"Invalid data format"</q>));</td></tr>
<tr><th id="190">190</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1537" title="do { if (!(__builtin_expect(!!(data_format_ == FORMAT_NHWC), 1))) { (context)-&gt;CtxFailure(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 192, (errors::InvalidArgument( &quot;Eigen Conv2DFastBackpropInputOp only supports NHWC.&quot;))); return; } } while (0)" data-ref="_M/OP_REQUIRES">OP_REQUIRES</a>(<a class="local col5 ref" href="#35context" title='context' data-ref="35context">context</a>, <a class="tu member" href="#tensorflow::Conv2DFastBackpropInputOp::data_format_" title='tensorflow::Conv2DFastBackpropInputOp::data_format_' data-ref="tensorflow::Conv2DFastBackpropInputOp::data_format_">data_format_</a> == <a class="enum" href="../util/tensor_format.h.html#tensorflow::TensorFormat::FORMAT_NHWC" title='tensorflow::TensorFormat::FORMAT_NHWC' data-ref="tensorflow::TensorFormat::FORMAT_NHWC">FORMAT_NHWC</a>,</td></tr>
<tr><th id="191">191</th><td>                errors::<a class="ref" href="../lib/core/errors.h.html#103" title='tensorflow::errors::InvalidArgument' data-ref="_ZN10tensorflow6errors15InvalidArgumentEDpT_">InvalidArgument</a>(</td></tr>
<tr><th id="192">192</th><td>                    <q>"Eigen Conv2DFastBackpropInputOp only supports NHWC."</q>));</td></tr>
<tr><th id="193">193</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1545" title="do { ::tensorflow::Status _s(context-&gt;GetAttr(&quot;strides&quot;, &amp;strides_)); if (!(__builtin_expect(!!(_s.ok()), 1))) { (context)-&gt;CtxFailureWithWarning(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 193, _s); return; } } while (0)" data-ref="_M/OP_REQUIRES_OK">OP_REQUIRES_OK</a>(<a class="local col5 ref" href="#35context" title='context' data-ref="35context">context</a>, <a class="local col5 ref" href="#35context" title='context' data-ref="35context">context</a>-&gt;GetAttr(<q>"strides"</q>, &amp;<a class="tu member" href="#tensorflow::Conv2DFastBackpropInputOp::strides_" title='tensorflow::Conv2DFastBackpropInputOp::strides_' data-use='a' data-ref="tensorflow::Conv2DFastBackpropInputOp::strides_">strides_</a>));</td></tr>
<tr><th id="194">194</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1537" title="do { if (!(__builtin_expect(!!(strides_.size() == 4), 1))) { (context)-&gt;CtxFailure(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 196, (errors::InvalidArgument(&quot;Sliding window strides field must &quot; &quot;specify 4 dimensions&quot;))); return; } } while (0)" data-ref="_M/OP_REQUIRES">OP_REQUIRES</a>(<a class="local col5 ref" href="#35context" title='context' data-ref="35context">context</a>, <a class="tu member" href="#tensorflow::Conv2DFastBackpropInputOp::strides_" title='tensorflow::Conv2DFastBackpropInputOp::strides_' data-use='m' data-ref="tensorflow::Conv2DFastBackpropInputOp::strides_">strides_</a>.<a class="ref" href="../../../../include/c++/5/bits/stl_vector.h.html#_ZNKSt6vector4sizeEv" title='std::vector::size' data-ref="_ZNKSt6vector4sizeEv">size</a>() == <var>4</var>,</td></tr>
<tr><th id="195">195</th><td>                errors::<a class="ref" href="../lib/core/errors.h.html#103" title='tensorflow::errors::InvalidArgument' data-ref="_ZN10tensorflow6errors15InvalidArgumentEDpT_">InvalidArgument</a>(<q>"Sliding window strides field must "</q></td></tr>
<tr><th id="196">196</th><td>                                        <q>"specify 4 dimensions"</q>));</td></tr>
<tr><th id="197">197</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1537" title="do { if (!(__builtin_expect(!!((strides_[0] == 1 &amp;&amp; strides_[3] == 1)), 1))) { (context)-&gt;CtxFailure(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 200, (errors::InvalidArgument(&quot;Current implementation does not yet support &quot; &quot;strides in the batch and depth dimensions.&quot;))); return; } } while (0)" data-ref="_M/OP_REQUIRES">OP_REQUIRES</a>(</td></tr>
<tr><th id="198">198</th><td>        <a class="local col5 ref" href="#35context" title='context' data-ref="35context">context</a>, (<a class="tu member" href="#tensorflow::Conv2DFastBackpropInputOp::strides_" title='tensorflow::Conv2DFastBackpropInputOp::strides_' data-ref="tensorflow::Conv2DFastBackpropInputOp::strides_">strides_</a>[<var>0</var>] == <var>1</var> &amp;&amp; <a class="tu member" href="#tensorflow::Conv2DFastBackpropInputOp::strides_" title='tensorflow::Conv2DFastBackpropInputOp::strides_' data-ref="tensorflow::Conv2DFastBackpropInputOp::strides_">strides_</a>[<var>3</var>] == <var>1</var>),</td></tr>
<tr><th id="199">199</th><td>        errors::<a class="ref" href="../lib/core/errors.h.html#103" title='tensorflow::errors::InvalidArgument' data-ref="_ZN10tensorflow6errors15InvalidArgumentEDpT_">InvalidArgument</a>(<q>"Current implementation does not yet support "</q></td></tr>
<tr><th id="200">200</th><td>                                <q>"strides in the batch and depth dimensions."</q>));</td></tr>
<tr><th id="201">201</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1537" title="do { if (!(__builtin_expect(!!(strides_[1] &gt; 0 &amp;&amp; strides_[2] &gt; 0), 1))) { (context)-&gt;CtxFailure(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 203, (errors::InvalidArgument( &quot;Row and column strides should be larger than 0.&quot;))); return; } } while (0)" data-ref="_M/OP_REQUIRES">OP_REQUIRES</a>(<a class="local col5 ref" href="#35context" title='context' data-ref="35context">context</a>, <a class="tu member" href="#tensorflow::Conv2DFastBackpropInputOp::strides_" title='tensorflow::Conv2DFastBackpropInputOp::strides_' data-ref="tensorflow::Conv2DFastBackpropInputOp::strides_">strides_</a>[<var>1</var>] &gt; <var>0</var> &amp;&amp; <a class="tu member" href="#tensorflow::Conv2DFastBackpropInputOp::strides_" title='tensorflow::Conv2DFastBackpropInputOp::strides_' data-ref="tensorflow::Conv2DFastBackpropInputOp::strides_">strides_</a>[<var>2</var>] &gt; <var>0</var>,</td></tr>
<tr><th id="202">202</th><td>                errors::<a class="ref" href="../lib/core/errors.h.html#103" title='tensorflow::errors::InvalidArgument' data-ref="_ZN10tensorflow6errors15InvalidArgumentEDpT_">InvalidArgument</a>(</td></tr>
<tr><th id="203">203</th><td>                    <q>"Row and column strides should be larger than 0."</q>));</td></tr>
<tr><th id="204">204</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1545" title="do { ::tensorflow::Status _s(context-&gt;GetAttr(&quot;padding&quot;, &amp;padding_)); if (!(__builtin_expect(!!(_s.ok()), 1))) { (context)-&gt;CtxFailureWithWarning(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 204, _s); return; } } while (0)" data-ref="_M/OP_REQUIRES_OK">OP_REQUIRES_OK</a>(<a class="local col5 ref" href="#35context" title='context' data-ref="35context">context</a>, <a class="local col5 ref" href="#35context" title='context' data-ref="35context">context</a>-&gt;GetAttr(<q>"padding"</q>, &amp;<a class="tu member" href="#tensorflow::Conv2DFastBackpropInputOp::padding_" title='tensorflow::Conv2DFastBackpropInputOp::padding_' data-use='a' data-ref="tensorflow::Conv2DFastBackpropInputOp::padding_">padding_</a>));</td></tr>
<tr><th id="205">205</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1545" title="do { ::tensorflow::Status _s(context-&gt;GetAttr(&quot;dilations&quot;, &amp;dilations_)); if (!(__builtin_expect(!!(_s.ok()), 1))) { (context)-&gt;CtxFailureWithWarning(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 205, _s); return; } } while (0)" data-ref="_M/OP_REQUIRES_OK">OP_REQUIRES_OK</a>(<a class="local col5 ref" href="#35context" title='context' data-ref="35context">context</a>, <a class="local col5 ref" href="#35context" title='context' data-ref="35context">context</a>-&gt;GetAttr(<q>"dilations"</q>, &amp;<a class="tu member" href="#tensorflow::Conv2DFastBackpropInputOp::dilations_" title='tensorflow::Conv2DFastBackpropInputOp::dilations_' data-use='a' data-ref="tensorflow::Conv2DFastBackpropInputOp::dilations_">dilations_</a>));</td></tr>
<tr><th id="206">206</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1537" title="do { if (!(__builtin_expect(!!(dilations_.size() == 4), 1))) { (context)-&gt;CtxFailure(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 208, (errors::InvalidArgument(&quot;Sliding window dilations field must &quot; &quot;specify 4 dimensions&quot;))); return; } } while (0)" data-ref="_M/OP_REQUIRES">OP_REQUIRES</a>(<a class="local col5 ref" href="#35context" title='context' data-ref="35context">context</a>, <a class="tu member" href="#tensorflow::Conv2DFastBackpropInputOp::dilations_" title='tensorflow::Conv2DFastBackpropInputOp::dilations_' data-use='m' data-ref="tensorflow::Conv2DFastBackpropInputOp::dilations_">dilations_</a>.<a class="ref" href="../../../../include/c++/5/bits/stl_vector.h.html#_ZNKSt6vector4sizeEv" title='std::vector::size' data-ref="_ZNKSt6vector4sizeEv">size</a>() == <var>4</var>,</td></tr>
<tr><th id="207">207</th><td>                errors::<a class="ref" href="../lib/core/errors.h.html#103" title='tensorflow::errors::InvalidArgument' data-ref="_ZN10tensorflow6errors15InvalidArgumentEDpT_">InvalidArgument</a>(<q>"Sliding window dilations field must "</q></td></tr>
<tr><th id="208">208</th><td>                                        <q>"specify 4 dimensions"</q>));</td></tr>
<tr><th id="209">209</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1537" title="do { if (!(__builtin_expect(!!((dilations_[0] &amp;&amp; dilations_[3])), 1))) { (context)-&gt;CtxFailure(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 212, (errors::InvalidArgument( &quot;Current implementation does not yet support &quot; &quot;dilations in the batch and depth dimensions.&quot;))); return; } } while (0)" data-ref="_M/OP_REQUIRES">OP_REQUIRES</a>(<a class="local col5 ref" href="#35context" title='context' data-ref="35context">context</a>, (<a class="tu member" href="#tensorflow::Conv2DFastBackpropInputOp::dilations_" title='tensorflow::Conv2DFastBackpropInputOp::dilations_' data-use='r' data-ref="tensorflow::Conv2DFastBackpropInputOp::dilations_">dilations_</a>[<var>0</var>] &amp;&amp; <a class="tu member" href="#tensorflow::Conv2DFastBackpropInputOp::dilations_" title='tensorflow::Conv2DFastBackpropInputOp::dilations_' data-use='r' data-ref="tensorflow::Conv2DFastBackpropInputOp::dilations_">dilations_</a>[<var>3</var>]),</td></tr>
<tr><th id="210">210</th><td>                errors::<a class="ref" href="../lib/core/errors.h.html#103" title='tensorflow::errors::InvalidArgument' data-ref="_ZN10tensorflow6errors15InvalidArgumentEDpT_">InvalidArgument</a>(</td></tr>
<tr><th id="211">211</th><td>                    <q>"Current implementation does not yet support "</q></td></tr>
<tr><th id="212">212</th><td>                    <q>"dilations in the batch and depth dimensions."</q>));</td></tr>
<tr><th id="213">213</th><td>    <i>// TODO(yangzihao): Add a CPU implementation for dilated convolution.</i></td></tr>
<tr><th id="214">214</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1537" title="do { if (!(__builtin_expect(!!((dilations_[1] == 1 &amp;&amp; dilations_[2] == 1)), 1))) { (context)-&gt;CtxFailure(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 217, (errors::InvalidArgument( &quot;Current Eigen and libxsmm implementations do not &quot; &quot;yet support dilation rates larger than 1.&quot;))); return; } } while (0)" data-ref="_M/OP_REQUIRES">OP_REQUIRES</a>(<a class="local col5 ref" href="#35context" title='context' data-ref="35context">context</a>, (<a class="tu member" href="#tensorflow::Conv2DFastBackpropInputOp::dilations_" title='tensorflow::Conv2DFastBackpropInputOp::dilations_' data-ref="tensorflow::Conv2DFastBackpropInputOp::dilations_">dilations_</a>[<var>1</var>] == <var>1</var> &amp;&amp; <a class="tu member" href="#tensorflow::Conv2DFastBackpropInputOp::dilations_" title='tensorflow::Conv2DFastBackpropInputOp::dilations_' data-ref="tensorflow::Conv2DFastBackpropInputOp::dilations_">dilations_</a>[<var>2</var>] == <var>1</var>),</td></tr>
<tr><th id="215">215</th><td>                errors::<a class="ref" href="../lib/core/errors.h.html#103" title='tensorflow::errors::InvalidArgument' data-ref="_ZN10tensorflow6errors15InvalidArgumentEDpT_">InvalidArgument</a>(</td></tr>
<tr><th id="216">216</th><td>                    <q>"Current Eigen and libxsmm implementations do not "</q></td></tr>
<tr><th id="217">217</th><td>                    <q>"yet support dilation rates larger than 1."</q>));</td></tr>
<tr><th id="218">218</th><td>  }</td></tr>
<tr><th id="219">219</th><td></td></tr>
<tr><th id="220">220</th><td>  <em>void</em> <dfn class="virtual decl def" id="_ZN10tensorflow25Conv2DFastBackpropInputOp7ComputeEPNS_15OpKernelContextE" title='tensorflow::Conv2DFastBackpropInputOp::Compute' data-ref="_ZN10tensorflow25Conv2DFastBackpropInputOp7ComputeEPNS_15OpKernelContextE">Compute</dfn>(<a class="type" href="../framework/op_kernel.h.html#tensorflow::OpKernelContext" title='tensorflow::OpKernelContext' data-ref="tensorflow::OpKernelContext">OpKernelContext</a>* <dfn class="local col7 decl" id="37context" title='context' data-type='tensorflow::OpKernelContext *' data-ref="37context">context</dfn>) override {</td></tr>
<tr><th id="221">221</th><td>    <em>const</em> <a class="type" href="../framework/tensor.h.html#tensorflow::Tensor" title='tensorflow::Tensor' data-ref="tensorflow::Tensor">Tensor</a>&amp; <dfn class="local col8 decl" id="38input_sizes" title='input_sizes' data-type='const tensorflow::Tensor &amp;' data-ref="38input_sizes">input_sizes</dfn> = <a class="local col7 ref" href="#37context" title='context' data-ref="37context">context</a>-&gt;<a class="ref" href="../framework/op_kernel.h.html#_ZN10tensorflow15OpKernelContext5inputEi" title='tensorflow::OpKernelContext::input' data-ref="_ZN10tensorflow15OpKernelContext5inputEi">input</a>(<var>0</var>);</td></tr>
<tr><th id="222">222</th><td>    <em>const</em> <a class="type" href="../framework/tensor.h.html#tensorflow::Tensor" title='tensorflow::Tensor' data-ref="tensorflow::Tensor">Tensor</a>&amp; <dfn class="local col9 decl" id="39filter" title='filter' data-type='const tensorflow::Tensor &amp;' data-ref="39filter">filter</dfn> = <a class="local col7 ref" href="#37context" title='context' data-ref="37context">context</a>-&gt;<a class="ref" href="../framework/op_kernel.h.html#_ZN10tensorflow15OpKernelContext5inputEi" title='tensorflow::OpKernelContext::input' data-ref="_ZN10tensorflow15OpKernelContext5inputEi">input</a>(<var>1</var>);</td></tr>
<tr><th id="223">223</th><td>    <em>const</em> <a class="type" href="../framework/tensor.h.html#tensorflow::Tensor" title='tensorflow::Tensor' data-ref="tensorflow::Tensor">Tensor</a>&amp; <dfn class="local col0 decl" id="40out_backprop" title='out_backprop' data-type='const tensorflow::Tensor &amp;' data-ref="40out_backprop">out_backprop</dfn> = <a class="local col7 ref" href="#37context" title='context' data-ref="37context">context</a>-&gt;<a class="ref" href="../framework/op_kernel.h.html#_ZN10tensorflow15OpKernelContext5inputEi" title='tensorflow::OpKernelContext::input' data-ref="_ZN10tensorflow15OpKernelContext5inputEi">input</a>(<var>2</var>);</td></tr>
<tr><th id="224">224</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1537" title="do { if (!(__builtin_expect(!!(TensorShapeUtils::IsVector(input_sizes.shape())), 1))) { (context)-&gt;CtxFailure(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 228, (errors::InvalidArgument( &quot;Conv2DBackpropInput: input_sizes input must be 1-dim, not &quot;, input_sizes.dims()))); return; } } while (0)" data-ref="_M/OP_REQUIRES">OP_REQUIRES</a>(</td></tr>
<tr><th id="225">225</th><td>        <a class="local col7 ref" href="#37context" title='context' data-ref="37context">context</a>, <a class="type" href="../framework/tensor_shape.h.html#tensorflow::TensorShapeUtils" title='tensorflow::TensorShapeUtils' data-ref="tensorflow::TensorShapeUtils">TensorShapeUtils</a>::<a class="ref" href="../framework/tensor_shape.h.html#_ZN10tensorflow16TensorShapeUtils8IsVectorERKNS_11TensorShapeE" title='tensorflow::TensorShapeUtils::IsVector' data-ref="_ZN10tensorflow16TensorShapeUtils8IsVectorERKNS_11TensorShapeE">IsVector</a>(<a class="local col8 ref" href="#38input_sizes" title='input_sizes' data-ref="38input_sizes">input_sizes</a>.<a class="ref" href="../framework/tensor.h.html#_ZNK10tensorflow6Tensor5shapeEv" title='tensorflow::Tensor::shape' data-ref="_ZNK10tensorflow6Tensor5shapeEv">shape</a>()),</td></tr>
<tr><th id="226">226</th><td>        errors::<a class="ref" href="../lib/core/errors.h.html#103" title='tensorflow::errors::InvalidArgument' data-ref="_ZN10tensorflow6errors15InvalidArgumentEDpT_">InvalidArgument</a>(</td></tr>
<tr><th id="227">227</th><td>            <q>"Conv2DBackpropInput: input_sizes input must be 1-dim, not "</q>,</td></tr>
<tr><th id="228">228</th><td>            <a class="local col8 ref" href="#38input_sizes" title='input_sizes' data-ref="38input_sizes">input_sizes</a>.<a class="ref" href="../framework/tensor.h.html#_ZNK10tensorflow6Tensor4dimsEv" title='tensorflow::Tensor::dims' data-ref="_ZNK10tensorflow6Tensor4dimsEv">dims</a>()));</td></tr>
<tr><th id="229">229</th><td>    <a class="type" href="../framework/tensor_shape.h.html#tensorflow::TensorShape" title='tensorflow::TensorShape' data-ref="tensorflow::TensorShape">TensorShape</a> <a class="ref fake" href="../framework/tensor_shape.h.html#289" title='tensorflow::TensorShape::TensorShape' data-ref="_ZN10tensorflow11TensorShapeC1Ev"></a><dfn class="local col1 decl" id="41input_shape" title='input_shape' data-type='tensorflow::TensorShape' data-ref="41input_shape">input_shape</dfn>;</td></tr>
<tr><th id="230">230</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1545" title="do { ::tensorflow::Status _s(TensorShapeUtils::MakeShape( input_sizes.vec&lt;int32&gt;(), &amp;input_shape)); if (!(__builtin_expect(!!(_s.ok()), 1))) { (context)-&gt;CtxFailureWithWarning(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 231, _s); return; } } while (0)" data-ref="_M/OP_REQUIRES_OK">OP_REQUIRES_OK</a>(<a class="local col7 ref" href="#37context" title='context' data-ref="37context">context</a>, <a class="type" href="../framework/tensor_shape.h.html#tensorflow::TensorShapeUtils" title='tensorflow::TensorShapeUtils' data-ref="tensorflow::TensorShapeUtils">TensorShapeUtils</a>::<a class="ref" href="../framework/tensor_shape.h.html#_ZN10tensorflow16TensorShapeUtils9MakeShapeENS_3gtl10ArraySliceIiEEPNS_11TensorShapeE" title='tensorflow::TensorShapeUtils::MakeShape' data-ref="_ZN10tensorflow16TensorShapeUtils9MakeShapeENS_3gtl10ArraySliceIiEEPNS_11TensorShapeE">MakeShape</a>(</td></tr>
<tr><th id="231">231</th><td>                                <a class="ref fake" href="../lib/gtl/array_slice.h.html#_ZN10tensorflow3gtl10ArraySliceC1ERKT_" title='tensorflow::gtl::ArraySlice::ArraySlice&lt;T&gt;' data-ref="_ZN10tensorflow3gtl10ArraySliceC1ERKT_"></a><a class="local col8 ref" href="#38input_sizes" title='input_sizes' data-ref="38input_sizes">input_sizes</a>.<a class="ref" href="../framework/tensor.h.html#_ZNK10tensorflow6Tensor3vecEv" title='tensorflow::Tensor::vec' data-ref="_ZNK10tensorflow6Tensor3vecEv">vec</a>&lt;<a class="typedef" href="../platform/default/integral_types.h.html#tensorflow::int32" title='tensorflow::int32' data-type='int' data-ref="tensorflow::int32">int32</a>&gt;(), &amp;<a class="local col1 ref" href="#41input_shape" title='input_shape' data-ref="41input_shape">input_shape</a>));</td></tr>
<tr><th id="232">232</th><td></td></tr>
<tr><th id="233">233</th><td>    <a class="type" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions" title='tensorflow::ConvBackpropDimensions' data-ref="tensorflow::ConvBackpropDimensions">ConvBackpropDimensions</a> <a class="ref fake" href="conv_grad_ops.h.html#228" title='tensorflow::ConvBackpropDimensions::ConvBackpropDimensions' data-ref="_ZN10tensorflow22ConvBackpropDimensionsC1Ev"></a><dfn class="local col2 decl" id="42dims" title='dims' data-type='tensorflow::ConvBackpropDimensions' data-ref="42dims">dims</dfn>;</td></tr>
<tr><th id="234">234</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1545" title="do { ::tensorflow::Status _s(ConvBackpropComputeDimensions( &quot;Conv2DFastBackpropInput&quot;, 2, input_shape, filter.shape(), out_backprop.shape(), strides_, padding_, data_format_, &amp;dims)); if (!(__builtin_expect(!!(_s.ok()), 1))) { (context)-&gt;CtxFailureWithWarning(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 238, _s); return; } } while (0)" data-ref="_M/OP_REQUIRES_OK">OP_REQUIRES_OK</a>(<a class="local col7 ref" href="#37context" title='context' data-ref="37context">context</a>,</td></tr>
<tr><th id="235">235</th><td>                   ConvBackpropComputeDimensions(</td></tr>
<tr><th id="236">236</th><td>                       <q>"Conv2DFastBackpropInput"</q>, <i>/*num_spatial_dims=*/</i><var>2</var>,</td></tr>
<tr><th id="237">237</th><td>                       <a class="local col1 ref" href="#41input_shape" title='input_shape' data-ref="41input_shape">input_shape</a>, <a class="local col9 ref" href="#39filter" title='filter' data-ref="39filter">filter</a>.<a class="ref" href="../framework/tensor.h.html#_ZNK10tensorflow6Tensor5shapeEv" title='tensorflow::Tensor::shape' data-ref="_ZNK10tensorflow6Tensor5shapeEv">shape</a>(), <a class="local col0 ref" href="#40out_backprop" title='out_backprop' data-ref="40out_backprop">out_backprop</a>.<a class="ref" href="../framework/tensor.h.html#_ZNK10tensorflow6Tensor5shapeEv" title='tensorflow::Tensor::shape' data-ref="_ZNK10tensorflow6Tensor5shapeEv">shape</a>(),</td></tr>
<tr><th id="238">238</th><td>                       <a class="tu member" href="#tensorflow::Conv2DFastBackpropInputOp::strides_" title='tensorflow::Conv2DFastBackpropInputOp::strides_' data-ref="tensorflow::Conv2DFastBackpropInputOp::strides_">strides_</a>, <a class="tu member" href="#tensorflow::Conv2DFastBackpropInputOp::padding_" title='tensorflow::Conv2DFastBackpropInputOp::padding_' data-ref="tensorflow::Conv2DFastBackpropInputOp::padding_">padding_</a>, <a class="tu member" href="#tensorflow::Conv2DFastBackpropInputOp::data_format_" title='tensorflow::Conv2DFastBackpropInputOp::data_format_' data-ref="tensorflow::Conv2DFastBackpropInputOp::data_format_">data_format_</a>, &amp;<a class="local col2 ref" href="#42dims" title='dims' data-ref="42dims">dims</a>));</td></tr>
<tr><th id="239">239</th><td></td></tr>
<tr><th id="240">240</th><td>    <a class="type" href="../framework/tensor.h.html#tensorflow::Tensor" title='tensorflow::Tensor' data-ref="tensorflow::Tensor">Tensor</a>* <dfn class="local col3 decl" id="43in_backprop" title='in_backprop' data-type='tensorflow::Tensor *' data-ref="43in_backprop">in_backprop</dfn> = <b>nullptr</b>;</td></tr>
<tr><th id="241">241</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1545" title="do { ::tensorflow::Status _s(context-&gt;allocate_output(0, input_shape, &amp;in_backprop)); if (!(__builtin_expect(!!(_s.ok()), 1))) { (context)-&gt;CtxFailureWithWarning(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 242, _s); return; } } while (0)" data-ref="_M/OP_REQUIRES_OK">OP_REQUIRES_OK</a>(<a class="local col7 ref" href="#37context" title='context' data-ref="37context">context</a>,</td></tr>
<tr><th id="242">242</th><td>                   <a class="local col7 ref" href="#37context" title='context' data-ref="37context">context</a>-&gt;<a class="ref" href="../framework/op_kernel.h.html#_ZN10tensorflow15OpKernelContext15allocate_outputEiRKNS_11TensorShapeEPPNS_6TensorE" title='tensorflow::OpKernelContext::allocate_output' data-ref="_ZN10tensorflow15OpKernelContext15allocate_outputEiRKNS_11TensorShapeEPPNS_6TensorE">allocate_output</a>(<var>0</var>, <a class="local col1 ref" href="#41input_shape" title='input_shape' data-ref="41input_shape">input_shape</a>, &amp;<a class="local col3 ref" href="#43in_backprop" title='in_backprop' data-ref="43in_backprop">in_backprop</a>));</td></tr>
<tr><th id="243">243</th><td></td></tr>
<tr><th id="244">244</th><td>    <i>// If there is nothing to compute, return.</i></td></tr>
<tr><th id="245">245</th><td>    <b>if</b> (<a class="local col1 ref" href="#41input_shape" title='input_shape' data-ref="41input_shape">input_shape</a>.<a class="ref" href="../framework/tensor_shape.h.html#_ZNK10tensorflow14TensorShapeRep12num_elementsEv" title='tensorflow::TensorShapeRep::num_elements' data-ref="_ZNK10tensorflow14TensorShapeRep12num_elementsEv">num_elements</a>() == <var>0</var>) {</td></tr>
<tr><th id="246">246</th><td>      <b>return</b>;</td></tr>
<tr><th id="247">247</th><td>    }</td></tr>
<tr><th id="248">248</th><td></td></tr>
<tr><th id="249">249</th><td><u>#<span data-ppcond="249">if</span> defined <span class="macro" data-ref="_M/TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS">TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS</span> &amp;&amp; \</u></td></tr>
<tr><th id="250">250</th><td><u>    defined <span class="macro" data-ref="_M/TENSORFLOW_USE_LIBXSMM_BACKWARD_CONVOLUTIONS">TENSORFLOW_USE_LIBXSMM_BACKWARD_CONVOLUTIONS</span></u></td></tr>
<tr><th id="251">251</th><td>    int64 pad_top, pad_bottom;</td></tr>
<tr><th id="252">252</th><td>    int64 pad_left, pad_right;</td></tr>
<tr><th id="253">253</th><td>    OP_REQUIRES_OK(</td></tr>
<tr><th id="254">254</th><td>        context,</td></tr>
<tr><th id="255">255</th><td>        GetWindowedOutputSizeVerbose(</td></tr>
<tr><th id="256">256</th><td>            dims.spatial_dims[<var>0</var>].input_size, dims.spatial_dims[<var>0</var>].filter_size,</td></tr>
<tr><th id="257">257</th><td>            dims.spatial_dims[<var>0</var>].stride, padding_,</td></tr>
<tr><th id="258">258</th><td>            &amp;dims.spatial_dims[<var>0</var>].output_size, &amp;pad_top, &amp;pad_bottom));</td></tr>
<tr><th id="259">259</th><td>    OP_REQUIRES_OK(</td></tr>
<tr><th id="260">260</th><td>        context,</td></tr>
<tr><th id="261">261</th><td>        GetWindowedOutputSizeVerbose(</td></tr>
<tr><th id="262">262</th><td>            dims.spatial_dims[<var>1</var>].input_size, dims.spatial_dims[<var>1</var>].filter_size,</td></tr>
<tr><th id="263">263</th><td>            dims.spatial_dims[<var>1</var>].stride, padding_,</td></tr>
<tr><th id="264">264</th><td>            &amp;dims.spatial_dims[<var>1</var>].output_size, &amp;pad_left, &amp;pad_right));</td></tr>
<tr><th id="265">265</th><td></td></tr>
<tr><th id="266">266</th><td>    <b>if</b> (pad_left == pad_right &amp;&amp; pad_top == pad_bottom) {</td></tr>
<tr><th id="267">267</th><td>      <b>if</b> (LaunchXsmmBackwardInputConvolution&lt;Device, T&gt;()(</td></tr>
<tr><th id="268">268</th><td>              context, context-&gt;eigen_device&lt;Device&gt;(),</td></tr>
<tr><th id="269">269</th><td>              in_backprop-&gt;tensor&lt;T, <var>4</var>&gt;(), filter.tensor&lt;T, <var>4</var>&gt;(),</td></tr>
<tr><th id="270">270</th><td>              out_backprop.tensor&lt;T, <var>4</var>&gt;(), dims.spatial_dims[<var>0</var>].input_size,</td></tr>
<tr><th id="271">271</th><td>              dims.spatial_dims[<var>1</var>].input_size,</td></tr>
<tr><th id="272">272</th><td>              <b>static_cast</b>&lt;<em>int</em>&gt;(dims.spatial_dims[<var>0</var>].stride),</td></tr>
<tr><th id="273">273</th><td>              <b>static_cast</b>&lt;<em>int</em>&gt;(dims.spatial_dims[<var>1</var>].stride),</td></tr>
<tr><th id="274">274</th><td>              <b>static_cast</b>&lt;<em>int</em>&gt;(pad_top), <b>static_cast</b>&lt;<em>int</em>&gt;(pad_left),</td></tr>
<tr><th id="275">275</th><td>              data_format_)) {</td></tr>
<tr><th id="276">276</th><td>        <b>return</b>;</td></tr>
<tr><th id="277">277</th><td>      }</td></tr>
<tr><th id="278">278</th><td>    }</td></tr>
<tr><th id="279">279</th><td><u>#<span data-ppcond="249">endif</span></u></td></tr>
<tr><th id="280">280</th><td></td></tr>
<tr><th id="281">281</th><td>    <a class="type" href="conv_grad_ops.h.html#tensorflow::LaunchConv2DBackpropInputOp" title='tensorflow::LaunchConv2DBackpropInputOp' data-ref="tensorflow::LaunchConv2DBackpropInputOp">LaunchConv2DBackpropInputOp</a>&lt;Device, T&gt;()(</td></tr>
<tr><th id="282">282</th><td>        <a class="local col7 ref" href="#37context" title='context' data-ref="37context">context</a>, <b>false</b>, <b>false</b>, <a class="local col0 ref" href="#40out_backprop" title='out_backprop' data-ref="40out_backprop">out_backprop</a>, <a class="local col9 ref" href="#39filter" title='filter' data-ref="39filter">filter</a>,</td></tr>
<tr><th id="283">283</th><td>        <a class="local col2 ref" href="#42dims" title='dims' data-ref="42dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a><a class="ref" href="../lib/gtl/inlined_vector.h.html#_ZN10tensorflow3gtl13InlinedVectorixEm" title='tensorflow::gtl::InlinedVector::operator[]' data-ref="_ZN10tensorflow3gtl13InlinedVectorixEm">[<var>0</var>]</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::stride" title='tensorflow::ConvBackpropSpatialDimension::stride' data-ref="tensorflow::ConvBackpropSpatialDimension::stride">stride</a>, <a class="local col2 ref" href="#42dims" title='dims' data-ref="42dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a><a class="ref" href="../lib/gtl/inlined_vector.h.html#_ZN10tensorflow3gtl13InlinedVectorixEm" title='tensorflow::gtl::InlinedVector::operator[]' data-ref="_ZN10tensorflow3gtl13InlinedVectorixEm">[<var>1</var>]</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::stride" title='tensorflow::ConvBackpropSpatialDimension::stride' data-ref="tensorflow::ConvBackpropSpatialDimension::stride">stride</a>, <a class="tu member" href="#tensorflow::Conv2DFastBackpropInputOp::padding_" title='tensorflow::Conv2DFastBackpropInputOp::padding_' data-ref="tensorflow::Conv2DFastBackpropInputOp::padding_">padding_</a>,</td></tr>
<tr><th id="284">284</th><td>        <a class="local col3 ref" href="#43in_backprop" title='in_backprop' data-ref="43in_backprop">in_backprop</a>, <a class="tu member" href="#tensorflow::Conv2DFastBackpropInputOp::data_format_" title='tensorflow::Conv2DFastBackpropInputOp::data_format_' data-ref="tensorflow::Conv2DFastBackpropInputOp::data_format_">data_format_</a>);</td></tr>
<tr><th id="285">285</th><td>  }</td></tr>
<tr><th id="286">286</th><td></td></tr>
<tr><th id="287">287</th><td> <b>private</b>:</td></tr>
<tr><th id="288">288</th><td>  <span class="namespace">std::</span><a class="type" href="../../../../include/c++/5/bits/stl_vector.h.html#std::vector" title='std::vector' data-ref="std::vector">vector</a>&lt;<a class="typedef" href="../platform/default/integral_types.h.html#tensorflow::int32" title='tensorflow::int32' data-type='int' data-ref="tensorflow::int32">int32</a>&gt; <dfn class="tu decl" id="tensorflow::Conv2DFastBackpropInputOp::dilations_" title='tensorflow::Conv2DFastBackpropInputOp::dilations_' data-type='std::vector&lt;int32&gt;' data-ref="tensorflow::Conv2DFastBackpropInputOp::dilations_">dilations_</dfn>;</td></tr>
<tr><th id="289">289</th><td>  <span class="namespace">std::</span><a class="type" href="../../../../include/c++/5/bits/stl_vector.h.html#std::vector" title='std::vector' data-ref="std::vector">vector</a>&lt;<a class="typedef" href="../platform/default/integral_types.h.html#tensorflow::int32" title='tensorflow::int32' data-type='int' data-ref="tensorflow::int32">int32</a>&gt; <dfn class="tu decl" id="tensorflow::Conv2DFastBackpropInputOp::strides_" title='tensorflow::Conv2DFastBackpropInputOp::strides_' data-type='std::vector&lt;int32&gt;' data-ref="tensorflow::Conv2DFastBackpropInputOp::strides_">strides_</dfn>;</td></tr>
<tr><th id="290">290</th><td>  <a class="type" href="../util/padding.h.html#tensorflow::Padding" title='tensorflow::Padding' data-ref="tensorflow::Padding">Padding</a> <dfn class="tu decl" id="tensorflow::Conv2DFastBackpropInputOp::padding_" title='tensorflow::Conv2DFastBackpropInputOp::padding_' data-type='tensorflow::Padding' data-ref="tensorflow::Conv2DFastBackpropInputOp::padding_">padding_</dfn>;</td></tr>
<tr><th id="291">291</th><td>  <a class="type" href="../util/tensor_format.h.html#tensorflow::TensorFormat" title='tensorflow::TensorFormat' data-ref="tensorflow::TensorFormat">TensorFormat</a> <dfn class="tu decl" id="tensorflow::Conv2DFastBackpropInputOp::data_format_" title='tensorflow::Conv2DFastBackpropInputOp::data_format_' data-type='tensorflow::TensorFormat' data-ref="tensorflow::Conv2DFastBackpropInputOp::data_format_">data_format_</dfn>;</td></tr>
<tr><th id="292">292</th><td></td></tr>
<tr><th id="293">293</th><td>  <a class="macro" href="../platform/macros.h.html#91" title="Conv2DFastBackpropInputOp(const Conv2DFastBackpropInputOp&amp;) = delete; void operator=(const Conv2DFastBackpropInputOp&amp;) = delete" data-ref="_M/TF_DISALLOW_COPY_AND_ASSIGN">TF_DISALLOW_COPY_AND_ASSIGN</a>(<dfn class="tu decl def" id="_ZN10tensorflow25Conv2DFastBackpropInputOpC1ERKNS_25Conv2DFastBackpropInputOpIT_T0_EE" title='tensorflow::Conv2DFastBackpropInputOp::Conv2DFastBackpropInputOp&lt;Device, T&gt;' data-type='void tensorflow::Conv2DFastBackpropInputOp::Conv2DFastBackpropInputOp&lt;Device, T&gt;(const Conv2DFastBackpropInputOp&lt;Device, T&gt; &amp; )' data-ref="_ZN10tensorflow25Conv2DFastBackpropInputOpC1ERKNS_25Conv2DFastBackpropInputOpIT_T0_EE">Conv2DFastBackpropInputOp</dfn>);</td></tr>
<tr><th id="294">294</th><td>};</td></tr>
<tr><th id="295">295</th><td></td></tr>
<tr><th id="296">296</th><td><i>// Based on implementation written by Yangqing Jia (jiayq).</i></td></tr>
<tr><th id="297">297</th><td><b>template</b> &lt;<b>typename</b> Device, <b>class</b> T&gt;</td></tr>
<tr><th id="298">298</th><td><b>class</b> <dfn class="type def" id="tensorflow::Conv2DCustomBackpropInputOp" title='tensorflow::Conv2DCustomBackpropInputOp' data-ref="tensorflow::Conv2DCustomBackpropInputOp">Conv2DCustomBackpropInputOp</dfn> : <b>public</b> <a class="type" href="../framework/op_kernel.h.html#tensorflow::OpKernel" title='tensorflow::OpKernel' data-ref="tensorflow::OpKernel">OpKernel</a> {</td></tr>
<tr><th id="299">299</th><td> <b>public</b>:</td></tr>
<tr><th id="300">300</th><td>  <b>explicit</b> <dfn class="tu decl def" id="_ZN10tensorflow27Conv2DCustomBackpropInputOpC1EPNS_20OpKernelConstructionE" title='tensorflow::Conv2DCustomBackpropInputOp::Conv2DCustomBackpropInputOp&lt;Device, T&gt;' data-type='void tensorflow::Conv2DCustomBackpropInputOp::Conv2DCustomBackpropInputOp&lt;Device, T&gt;(tensorflow::OpKernelConstruction * context)' data-ref="_ZN10tensorflow27Conv2DCustomBackpropInputOpC1EPNS_20OpKernelConstructionE">Conv2DCustomBackpropInputOp</dfn>(<a class="type" href="../framework/op_kernel.h.html#tensorflow::OpKernelConstruction" title='tensorflow::OpKernelConstruction' data-ref="tensorflow::OpKernelConstruction">OpKernelConstruction</a>* <dfn class="local col4 decl" id="44context" title='context' data-type='tensorflow::OpKernelConstruction *' data-ref="44context">context</dfn>)</td></tr>
<tr><th id="301">301</th><td>      : <a class="type" href="../framework/op_kernel.h.html#tensorflow::OpKernel" title='tensorflow::OpKernel' data-ref="tensorflow::OpKernel">OpKernel</a>(<a class="local col4 ref" href="#44context" title='context' data-ref="44context">context</a>) {</td></tr>
<tr><th id="302">302</th><td>    <a class="typedef" href="../../../../include/c++/5/bits/stringfwd.h.html#std::string" title='std::string' data-type='basic_string&lt;char&gt;' data-ref="std::string">string</a> <a class="ref fake" href="../../../../include/c++/5/bits/basic_string.h.html#_ZNSt12basic_stringC1Ev" title='std::basic_string::basic_string&lt;_CharT, _Traits, _Alloc&gt;' data-ref="_ZNSt12basic_stringC1Ev"></a><dfn class="local col5 decl" id="45data_format" title='data_format' data-type='string' data-ref="45data_format">data_format</dfn>;</td></tr>
<tr><th id="303">303</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1545" title="do { ::tensorflow::Status _s(context-&gt;GetAttr(&quot;data_format&quot;, &amp;data_format)); if (!(__builtin_expect(!!(_s.ok()), 1))) { (context)-&gt;CtxFailureWithWarning(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 303, _s); return; } } while (0)" data-ref="_M/OP_REQUIRES_OK">OP_REQUIRES_OK</a>(<a class="local col4 ref" href="#44context" title='context' data-ref="44context">context</a>, <a class="local col4 ref" href="#44context" title='context' data-ref="44context">context</a>-&gt;<a class="ref" href="../framework/op_kernel.h.html#_ZNK10tensorflow20OpKernelConstruction7GetAttrENS_11StringPieceEPT_" title='tensorflow::OpKernelConstruction::GetAttr' data-ref="_ZNK10tensorflow20OpKernelConstruction7GetAttrENS_11StringPieceEPT_">GetAttr</a>(<a class="ref fake" href="../lib/core/stringpiece.h.html#_ZN10tensorflow11StringPieceC1EPKc" title='tensorflow::StringPiece::StringPiece' data-ref="_ZN10tensorflow11StringPieceC1EPKc"></a><q>"data_format"</q>, &amp;<a class="local col5 ref" href="#45data_format" title='data_format' data-ref="45data_format">data_format</a>));</td></tr>
<tr><th id="304">304</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1537" title="do { if (!(__builtin_expect(!!(FormatFromString(data_format, &amp;data_format_)), 1))) { (context)-&gt;CtxFailure(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 305, (errors::InvalidArgument(&quot;Invalid data format&quot;))); return; } } while (0)" data-ref="_M/OP_REQUIRES">OP_REQUIRES</a>(<a class="local col4 ref" href="#44context" title='context' data-ref="44context">context</a>, FormatFromString(<a class="local col5 ref" href="#45data_format" title='data_format' data-ref="45data_format">data_format</a>, &amp;<a class="tu member" href="#tensorflow::Conv2DCustomBackpropInputOp::data_format_" title='tensorflow::Conv2DCustomBackpropInputOp::data_format_' data-use='a' data-ref="tensorflow::Conv2DCustomBackpropInputOp::data_format_">data_format_</a>),</td></tr>
<tr><th id="305">305</th><td>                errors::<a class="ref" href="../lib/core/errors.h.html#103" title='tensorflow::errors::InvalidArgument' data-ref="_ZN10tensorflow6errors15InvalidArgumentEDpT_">InvalidArgument</a>(<q>"Invalid data format"</q>));</td></tr>
<tr><th id="306">306</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1537" title="do { if (!(__builtin_expect(!!(data_format_ == FORMAT_NHWC), 1))) { (context)-&gt;CtxFailure(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 308, (errors::InvalidArgument( &quot;Conv2DCustomBackpropInputOp only supports NHWC.&quot;))); return; } } while (0)" data-ref="_M/OP_REQUIRES">OP_REQUIRES</a>(<a class="local col4 ref" href="#44context" title='context' data-ref="44context">context</a>, <a class="tu member" href="#tensorflow::Conv2DCustomBackpropInputOp::data_format_" title='tensorflow::Conv2DCustomBackpropInputOp::data_format_' data-ref="tensorflow::Conv2DCustomBackpropInputOp::data_format_">data_format_</a> == <a class="enum" href="../util/tensor_format.h.html#tensorflow::TensorFormat::FORMAT_NHWC" title='tensorflow::TensorFormat::FORMAT_NHWC' data-ref="tensorflow::TensorFormat::FORMAT_NHWC">FORMAT_NHWC</a>,</td></tr>
<tr><th id="307">307</th><td>                errors::<a class="ref" href="../lib/core/errors.h.html#103" title='tensorflow::errors::InvalidArgument' data-ref="_ZN10tensorflow6errors15InvalidArgumentEDpT_">InvalidArgument</a>(</td></tr>
<tr><th id="308">308</th><td>                    <q>"Conv2DCustomBackpropInputOp only supports NHWC."</q>));</td></tr>
<tr><th id="309">309</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1545" title="do { ::tensorflow::Status _s(context-&gt;GetAttr(&quot;strides&quot;, &amp;strides_)); if (!(__builtin_expect(!!(_s.ok()), 1))) { (context)-&gt;CtxFailureWithWarning(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 309, _s); return; } } while (0)" data-ref="_M/OP_REQUIRES_OK">OP_REQUIRES_OK</a>(<a class="local col4 ref" href="#44context" title='context' data-ref="44context">context</a>, <a class="local col4 ref" href="#44context" title='context' data-ref="44context">context</a>-&gt;GetAttr(<q>"strides"</q>, &amp;<a class="tu member" href="#tensorflow::Conv2DCustomBackpropInputOp::strides_" title='tensorflow::Conv2DCustomBackpropInputOp::strides_' data-use='a' data-ref="tensorflow::Conv2DCustomBackpropInputOp::strides_">strides_</a>));</td></tr>
<tr><th id="310">310</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1537" title="do { if (!(__builtin_expect(!!(strides_.size() == 4), 1))) { (context)-&gt;CtxFailure(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 312, (errors::InvalidArgument(&quot;Sliding window strides field must &quot; &quot;specify 4 dimensions&quot;))); return; } } while (0)" data-ref="_M/OP_REQUIRES">OP_REQUIRES</a>(<a class="local col4 ref" href="#44context" title='context' data-ref="44context">context</a>, <a class="tu member" href="#tensorflow::Conv2DCustomBackpropInputOp::strides_" title='tensorflow::Conv2DCustomBackpropInputOp::strides_' data-use='m' data-ref="tensorflow::Conv2DCustomBackpropInputOp::strides_">strides_</a>.<a class="ref" href="../../../../include/c++/5/bits/stl_vector.h.html#_ZNKSt6vector4sizeEv" title='std::vector::size' data-ref="_ZNKSt6vector4sizeEv">size</a>() == <var>4</var>,</td></tr>
<tr><th id="311">311</th><td>                errors::<a class="ref" href="../lib/core/errors.h.html#103" title='tensorflow::errors::InvalidArgument' data-ref="_ZN10tensorflow6errors15InvalidArgumentEDpT_">InvalidArgument</a>(<q>"Sliding window strides field must "</q></td></tr>
<tr><th id="312">312</th><td>                                        <q>"specify 4 dimensions"</q>));</td></tr>
<tr><th id="313">313</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1537" title="do { if (!(__builtin_expect(!!((strides_[0] == 1 &amp;&amp; strides_[3] == 1)), 1))) { (context)-&gt;CtxFailure(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 316, (errors::InvalidArgument(&quot;Current implementation does not yet support &quot; &quot;strides in the batch and depth dimensions.&quot;))); return; } } while (0)" data-ref="_M/OP_REQUIRES">OP_REQUIRES</a>(</td></tr>
<tr><th id="314">314</th><td>        <a class="local col4 ref" href="#44context" title='context' data-ref="44context">context</a>, (<a class="tu member" href="#tensorflow::Conv2DCustomBackpropInputOp::strides_" title='tensorflow::Conv2DCustomBackpropInputOp::strides_' data-ref="tensorflow::Conv2DCustomBackpropInputOp::strides_">strides_</a>[<var>0</var>] == <var>1</var> &amp;&amp; <a class="tu member" href="#tensorflow::Conv2DCustomBackpropInputOp::strides_" title='tensorflow::Conv2DCustomBackpropInputOp::strides_' data-ref="tensorflow::Conv2DCustomBackpropInputOp::strides_">strides_</a>[<var>3</var>] == <var>1</var>),</td></tr>
<tr><th id="315">315</th><td>        errors::<a class="ref" href="../lib/core/errors.h.html#103" title='tensorflow::errors::InvalidArgument' data-ref="_ZN10tensorflow6errors15InvalidArgumentEDpT_">InvalidArgument</a>(<q>"Current implementation does not yet support "</q></td></tr>
<tr><th id="316">316</th><td>                                <q>"strides in the batch and depth dimensions."</q>));</td></tr>
<tr><th id="317">317</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1537" title="do { if (!(__builtin_expect(!!(strides_[1] &gt; 0 &amp;&amp; strides_[2] &gt; 0), 1))) { (context)-&gt;CtxFailure(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 319, (errors::InvalidArgument( &quot;Row and column strides should be larger than 0.&quot;))); return; } } while (0)" data-ref="_M/OP_REQUIRES">OP_REQUIRES</a>(<a class="local col4 ref" href="#44context" title='context' data-ref="44context">context</a>, <a class="tu member" href="#tensorflow::Conv2DCustomBackpropInputOp::strides_" title='tensorflow::Conv2DCustomBackpropInputOp::strides_' data-ref="tensorflow::Conv2DCustomBackpropInputOp::strides_">strides_</a>[<var>1</var>] &gt; <var>0</var> &amp;&amp; <a class="tu member" href="#tensorflow::Conv2DCustomBackpropInputOp::strides_" title='tensorflow::Conv2DCustomBackpropInputOp::strides_' data-ref="tensorflow::Conv2DCustomBackpropInputOp::strides_">strides_</a>[<var>2</var>] &gt; <var>0</var>,</td></tr>
<tr><th id="318">318</th><td>                errors::<a class="ref" href="../lib/core/errors.h.html#103" title='tensorflow::errors::InvalidArgument' data-ref="_ZN10tensorflow6errors15InvalidArgumentEDpT_">InvalidArgument</a>(</td></tr>
<tr><th id="319">319</th><td>                    <q>"Row and column strides should be larger than 0."</q>));</td></tr>
<tr><th id="320">320</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1545" title="do { ::tensorflow::Status _s(context-&gt;GetAttr(&quot;padding&quot;, &amp;padding_)); if (!(__builtin_expect(!!(_s.ok()), 1))) { (context)-&gt;CtxFailureWithWarning(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 320, _s); return; } } while (0)" data-ref="_M/OP_REQUIRES_OK">OP_REQUIRES_OK</a>(<a class="local col4 ref" href="#44context" title='context' data-ref="44context">context</a>, <a class="local col4 ref" href="#44context" title='context' data-ref="44context">context</a>-&gt;GetAttr(<q>"padding"</q>, &amp;<a class="tu member" href="#tensorflow::Conv2DCustomBackpropInputOp::padding_" title='tensorflow::Conv2DCustomBackpropInputOp::padding_' data-use='a' data-ref="tensorflow::Conv2DCustomBackpropInputOp::padding_">padding_</a>));</td></tr>
<tr><th id="321">321</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1545" title="do { ::tensorflow::Status _s(context-&gt;GetAttr(&quot;dilations&quot;, &amp;dilations_)); if (!(__builtin_expect(!!(_s.ok()), 1))) { (context)-&gt;CtxFailureWithWarning(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 321, _s); return; } } while (0)" data-ref="_M/OP_REQUIRES_OK">OP_REQUIRES_OK</a>(<a class="local col4 ref" href="#44context" title='context' data-ref="44context">context</a>, <a class="local col4 ref" href="#44context" title='context' data-ref="44context">context</a>-&gt;GetAttr(<q>"dilations"</q>, &amp;<a class="tu member" href="#tensorflow::Conv2DCustomBackpropInputOp::dilations_" title='tensorflow::Conv2DCustomBackpropInputOp::dilations_' data-use='a' data-ref="tensorflow::Conv2DCustomBackpropInputOp::dilations_">dilations_</a>));</td></tr>
<tr><th id="322">322</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1537" title="do { if (!(__builtin_expect(!!(dilations_.size() == 4), 1))) { (context)-&gt;CtxFailure(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 324, (errors::InvalidArgument(&quot;Sliding window dilations field must &quot; &quot;specify 4 dimensions&quot;))); return; } } while (0)" data-ref="_M/OP_REQUIRES">OP_REQUIRES</a>(<a class="local col4 ref" href="#44context" title='context' data-ref="44context">context</a>, <a class="tu member" href="#tensorflow::Conv2DCustomBackpropInputOp::dilations_" title='tensorflow::Conv2DCustomBackpropInputOp::dilations_' data-use='m' data-ref="tensorflow::Conv2DCustomBackpropInputOp::dilations_">dilations_</a>.<a class="ref" href="../../../../include/c++/5/bits/stl_vector.h.html#_ZNKSt6vector4sizeEv" title='std::vector::size' data-ref="_ZNKSt6vector4sizeEv">size</a>() == <var>4</var>,</td></tr>
<tr><th id="323">323</th><td>                errors::<a class="ref" href="../lib/core/errors.h.html#103" title='tensorflow::errors::InvalidArgument' data-ref="_ZN10tensorflow6errors15InvalidArgumentEDpT_">InvalidArgument</a>(<q>"Sliding window dilations field must "</q></td></tr>
<tr><th id="324">324</th><td>                                        <q>"specify 4 dimensions"</q>));</td></tr>
<tr><th id="325">325</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1537" title="do { if (!(__builtin_expect(!!((dilations_[0] == 1 &amp;&amp; dilations_[3] == 1)), 1))) { (context)-&gt;CtxFailure(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 328, (errors::InvalidArgument( &quot;Current implementation does not yet support &quot; &quot;dilations in the batch and depth dimensions.&quot;))); return; } } while (0)" data-ref="_M/OP_REQUIRES">OP_REQUIRES</a>(<a class="local col4 ref" href="#44context" title='context' data-ref="44context">context</a>, (<a class="tu member" href="#tensorflow::Conv2DCustomBackpropInputOp::dilations_" title='tensorflow::Conv2DCustomBackpropInputOp::dilations_' data-ref="tensorflow::Conv2DCustomBackpropInputOp::dilations_">dilations_</a>[<var>0</var>] == <var>1</var> &amp;&amp; <a class="tu member" href="#tensorflow::Conv2DCustomBackpropInputOp::dilations_" title='tensorflow::Conv2DCustomBackpropInputOp::dilations_' data-ref="tensorflow::Conv2DCustomBackpropInputOp::dilations_">dilations_</a>[<var>3</var>] == <var>1</var>),</td></tr>
<tr><th id="326">326</th><td>                errors::<a class="ref" href="../lib/core/errors.h.html#103" title='tensorflow::errors::InvalidArgument' data-ref="_ZN10tensorflow6errors15InvalidArgumentEDpT_">InvalidArgument</a>(</td></tr>
<tr><th id="327">327</th><td>                    <q>"Current implementation does not yet support "</q></td></tr>
<tr><th id="328">328</th><td>                    <q>"dilations in the batch and depth dimensions."</q>));</td></tr>
<tr><th id="329">329</th><td>    <i>// TODO(yangzihao): Add a CPU implementation for dilated convolution.</i></td></tr>
<tr><th id="330">330</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1537" title="do { if (!(__builtin_expect(!!((dilations_[1] == 1 &amp;&amp; dilations_[2] == 1)), 1))) { (context)-&gt;CtxFailure(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 333, (errors::InvalidArgument( &quot;Current libxsmm and customized CPU implementations do &quot; &quot;not yet support dilation rates larger than 1.&quot;))); return; } } while (0)" data-ref="_M/OP_REQUIRES">OP_REQUIRES</a>(<a class="local col4 ref" href="#44context" title='context' data-ref="44context">context</a>, (<a class="tu member" href="#tensorflow::Conv2DCustomBackpropInputOp::dilations_" title='tensorflow::Conv2DCustomBackpropInputOp::dilations_' data-ref="tensorflow::Conv2DCustomBackpropInputOp::dilations_">dilations_</a>[<var>1</var>] == <var>1</var> &amp;&amp; <a class="tu member" href="#tensorflow::Conv2DCustomBackpropInputOp::dilations_" title='tensorflow::Conv2DCustomBackpropInputOp::dilations_' data-ref="tensorflow::Conv2DCustomBackpropInputOp::dilations_">dilations_</a>[<var>2</var>] == <var>1</var>),</td></tr>
<tr><th id="331">331</th><td>                errors::<a class="ref" href="../lib/core/errors.h.html#103" title='tensorflow::errors::InvalidArgument' data-ref="_ZN10tensorflow6errors15InvalidArgumentEDpT_">InvalidArgument</a>(</td></tr>
<tr><th id="332">332</th><td>                    <q>"Current libxsmm and customized CPU implementations do "</q></td></tr>
<tr><th id="333">333</th><td>                    <q>"not yet support dilation rates larger than 1."</q>));</td></tr>
<tr><th id="334">334</th><td>  }</td></tr>
<tr><th id="335">335</th><td></td></tr>
<tr><th id="336">336</th><td>  <em>void</em> <dfn class="virtual decl def" id="_ZN10tensorflow27Conv2DCustomBackpropInputOp7ComputeEPNS_15OpKernelContextE" title='tensorflow::Conv2DCustomBackpropInputOp::Compute' data-ref="_ZN10tensorflow27Conv2DCustomBackpropInputOp7ComputeEPNS_15OpKernelContextE">Compute</dfn>(<a class="type" href="../framework/op_kernel.h.html#tensorflow::OpKernelContext" title='tensorflow::OpKernelContext' data-ref="tensorflow::OpKernelContext">OpKernelContext</a>* <dfn class="local col6 decl" id="46context" title='context' data-type='tensorflow::OpKernelContext *' data-ref="46context">context</dfn>) override {</td></tr>
<tr><th id="337">337</th><td>    <em>const</em> <a class="type" href="../framework/tensor.h.html#tensorflow::Tensor" title='tensorflow::Tensor' data-ref="tensorflow::Tensor">Tensor</a>&amp; <dfn class="local col7 decl" id="47input_sizes" title='input_sizes' data-type='const tensorflow::Tensor &amp;' data-ref="47input_sizes">input_sizes</dfn> = <a class="local col6 ref" href="#46context" title='context' data-ref="46context">context</a>-&gt;<a class="ref" href="../framework/op_kernel.h.html#_ZN10tensorflow15OpKernelContext5inputEi" title='tensorflow::OpKernelContext::input' data-ref="_ZN10tensorflow15OpKernelContext5inputEi">input</a>(<var>0</var>);</td></tr>
<tr><th id="338">338</th><td>    <em>const</em> <a class="type" href="../framework/tensor.h.html#tensorflow::Tensor" title='tensorflow::Tensor' data-ref="tensorflow::Tensor">Tensor</a>&amp; <dfn class="local col8 decl" id="48filter" title='filter' data-type='const tensorflow::Tensor &amp;' data-ref="48filter">filter</dfn> = <a class="local col6 ref" href="#46context" title='context' data-ref="46context">context</a>-&gt;<a class="ref" href="../framework/op_kernel.h.html#_ZN10tensorflow15OpKernelContext5inputEi" title='tensorflow::OpKernelContext::input' data-ref="_ZN10tensorflow15OpKernelContext5inputEi">input</a>(<var>1</var>);</td></tr>
<tr><th id="339">339</th><td>    <em>const</em> <a class="type" href="../framework/tensor.h.html#tensorflow::Tensor" title='tensorflow::Tensor' data-ref="tensorflow::Tensor">Tensor</a>&amp; <dfn class="local col9 decl" id="49out_backprop" title='out_backprop' data-type='const tensorflow::Tensor &amp;' data-ref="49out_backprop">out_backprop</dfn> = <a class="local col6 ref" href="#46context" title='context' data-ref="46context">context</a>-&gt;<a class="ref" href="../framework/op_kernel.h.html#_ZN10tensorflow15OpKernelContext5inputEi" title='tensorflow::OpKernelContext::input' data-ref="_ZN10tensorflow15OpKernelContext5inputEi">input</a>(<var>2</var>);</td></tr>
<tr><th id="340">340</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1537" title="do { if (!(__builtin_expect(!!(TensorShapeUtils::IsVector(input_sizes.shape())), 1))) { (context)-&gt;CtxFailure(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 344, (errors::InvalidArgument( &quot;Conv2DBackpropInput: input_sizes input must be 1-dim, not &quot;, input_sizes.dims()))); return; } } while (0)" data-ref="_M/OP_REQUIRES">OP_REQUIRES</a>(</td></tr>
<tr><th id="341">341</th><td>        <a class="local col6 ref" href="#46context" title='context' data-ref="46context">context</a>, <a class="type" href="../framework/tensor_shape.h.html#tensorflow::TensorShapeUtils" title='tensorflow::TensorShapeUtils' data-ref="tensorflow::TensorShapeUtils">TensorShapeUtils</a>::<a class="ref" href="../framework/tensor_shape.h.html#_ZN10tensorflow16TensorShapeUtils8IsVectorERKNS_11TensorShapeE" title='tensorflow::TensorShapeUtils::IsVector' data-ref="_ZN10tensorflow16TensorShapeUtils8IsVectorERKNS_11TensorShapeE">IsVector</a>(<a class="local col7 ref" href="#47input_sizes" title='input_sizes' data-ref="47input_sizes">input_sizes</a>.<a class="ref" href="../framework/tensor.h.html#_ZNK10tensorflow6Tensor5shapeEv" title='tensorflow::Tensor::shape' data-ref="_ZNK10tensorflow6Tensor5shapeEv">shape</a>()),</td></tr>
<tr><th id="342">342</th><td>        errors::<a class="ref" href="../lib/core/errors.h.html#103" title='tensorflow::errors::InvalidArgument' data-ref="_ZN10tensorflow6errors15InvalidArgumentEDpT_">InvalidArgument</a>(</td></tr>
<tr><th id="343">343</th><td>            <q>"Conv2DBackpropInput: input_sizes input must be 1-dim, not "</q>,</td></tr>
<tr><th id="344">344</th><td>            <a class="local col7 ref" href="#47input_sizes" title='input_sizes' data-ref="47input_sizes">input_sizes</a>.<a class="ref" href="../framework/tensor.h.html#_ZNK10tensorflow6Tensor4dimsEv" title='tensorflow::Tensor::dims' data-ref="_ZNK10tensorflow6Tensor4dimsEv">dims</a>()));</td></tr>
<tr><th id="345">345</th><td>    <a class="type" href="../framework/tensor_shape.h.html#tensorflow::TensorShape" title='tensorflow::TensorShape' data-ref="tensorflow::TensorShape">TensorShape</a> <a class="ref fake" href="../framework/tensor_shape.h.html#289" title='tensorflow::TensorShape::TensorShape' data-ref="_ZN10tensorflow11TensorShapeC1Ev"></a><dfn class="local col0 decl" id="50input_shape" title='input_shape' data-type='tensorflow::TensorShape' data-ref="50input_shape">input_shape</dfn>;</td></tr>
<tr><th id="346">346</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1545" title="do { ::tensorflow::Status _s(TensorShapeUtils::MakeShape( input_sizes.vec&lt;int32&gt;(), &amp;input_shape)); if (!(__builtin_expect(!!(_s.ok()), 1))) { (context)-&gt;CtxFailureWithWarning(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 347, _s); return; } } while (0)" data-ref="_M/OP_REQUIRES_OK">OP_REQUIRES_OK</a>(<a class="local col6 ref" href="#46context" title='context' data-ref="46context">context</a>, <a class="type" href="../framework/tensor_shape.h.html#tensorflow::TensorShapeUtils" title='tensorflow::TensorShapeUtils' data-ref="tensorflow::TensorShapeUtils">TensorShapeUtils</a>::<a class="ref" href="../framework/tensor_shape.h.html#_ZN10tensorflow16TensorShapeUtils9MakeShapeENS_3gtl10ArraySliceIiEEPNS_11TensorShapeE" title='tensorflow::TensorShapeUtils::MakeShape' data-ref="_ZN10tensorflow16TensorShapeUtils9MakeShapeENS_3gtl10ArraySliceIiEEPNS_11TensorShapeE">MakeShape</a>(</td></tr>
<tr><th id="347">347</th><td>                                <a class="ref fake" href="../lib/gtl/array_slice.h.html#_ZN10tensorflow3gtl10ArraySliceC1ERKT_" title='tensorflow::gtl::ArraySlice::ArraySlice&lt;T&gt;' data-ref="_ZN10tensorflow3gtl10ArraySliceC1ERKT_"></a><a class="local col7 ref" href="#47input_sizes" title='input_sizes' data-ref="47input_sizes">input_sizes</a>.<a class="ref" href="../framework/tensor.h.html#_ZNK10tensorflow6Tensor3vecEv" title='tensorflow::Tensor::vec' data-ref="_ZNK10tensorflow6Tensor3vecEv">vec</a>&lt;<a class="typedef" href="../platform/default/integral_types.h.html#tensorflow::int32" title='tensorflow::int32' data-type='int' data-ref="tensorflow::int32">int32</a>&gt;(), &amp;<a class="local col0 ref" href="#50input_shape" title='input_shape' data-ref="50input_shape">input_shape</a>));</td></tr>
<tr><th id="348">348</th><td></td></tr>
<tr><th id="349">349</th><td>    <a class="type" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions" title='tensorflow::ConvBackpropDimensions' data-ref="tensorflow::ConvBackpropDimensions">ConvBackpropDimensions</a> <a class="ref fake" href="conv_grad_ops.h.html#228" title='tensorflow::ConvBackpropDimensions::ConvBackpropDimensions' data-ref="_ZN10tensorflow22ConvBackpropDimensionsC1Ev"></a><dfn class="local col1 decl" id="51dims" title='dims' data-type='tensorflow::ConvBackpropDimensions' data-ref="51dims">dims</dfn>;</td></tr>
<tr><th id="350">350</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1545" title="do { ::tensorflow::Status _s(ConvBackpropComputeDimensions( &quot;Conv2DCustomBackpropInput&quot;, 2, input_shape, filter.shape(), out_backprop.shape(), strides_, padding_, data_format_, &amp;dims)); if (!(__builtin_expect(!!(_s.ok()), 1))) { (context)-&gt;CtxFailureWithWarning(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 354, _s); return; } } while (0)" data-ref="_M/OP_REQUIRES_OK">OP_REQUIRES_OK</a>(<a class="local col6 ref" href="#46context" title='context' data-ref="46context">context</a>,</td></tr>
<tr><th id="351">351</th><td>                   ConvBackpropComputeDimensions(</td></tr>
<tr><th id="352">352</th><td>                       <q>"Conv2DCustomBackpropInput"</q>, <i>/*num_spatial_dims=*/</i><var>2</var>,</td></tr>
<tr><th id="353">353</th><td>                       <a class="local col0 ref" href="#50input_shape" title='input_shape' data-ref="50input_shape">input_shape</a>, <a class="local col8 ref" href="#48filter" title='filter' data-ref="48filter">filter</a>.<a class="ref" href="../framework/tensor.h.html#_ZNK10tensorflow6Tensor5shapeEv" title='tensorflow::Tensor::shape' data-ref="_ZNK10tensorflow6Tensor5shapeEv">shape</a>(), <a class="local col9 ref" href="#49out_backprop" title='out_backprop' data-ref="49out_backprop">out_backprop</a>.<a class="ref" href="../framework/tensor.h.html#_ZNK10tensorflow6Tensor5shapeEv" title='tensorflow::Tensor::shape' data-ref="_ZNK10tensorflow6Tensor5shapeEv">shape</a>(),</td></tr>
<tr><th id="354">354</th><td>                       <a class="tu member" href="#tensorflow::Conv2DCustomBackpropInputOp::strides_" title='tensorflow::Conv2DCustomBackpropInputOp::strides_' data-ref="tensorflow::Conv2DCustomBackpropInputOp::strides_">strides_</a>, <a class="tu member" href="#tensorflow::Conv2DCustomBackpropInputOp::padding_" title='tensorflow::Conv2DCustomBackpropInputOp::padding_' data-ref="tensorflow::Conv2DCustomBackpropInputOp::padding_">padding_</a>, <a class="tu member" href="#tensorflow::Conv2DCustomBackpropInputOp::data_format_" title='tensorflow::Conv2DCustomBackpropInputOp::data_format_' data-ref="tensorflow::Conv2DCustomBackpropInputOp::data_format_">data_format_</a>, &amp;<a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>));</td></tr>
<tr><th id="355">355</th><td></td></tr>
<tr><th id="356">356</th><td>    <a class="type" href="../framework/tensor.h.html#tensorflow::Tensor" title='tensorflow::Tensor' data-ref="tensorflow::Tensor">Tensor</a>* <dfn class="local col2 decl" id="52in_backprop" title='in_backprop' data-type='tensorflow::Tensor *' data-ref="52in_backprop">in_backprop</dfn> = <b>nullptr</b>;</td></tr>
<tr><th id="357">357</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1545" title="do { ::tensorflow::Status _s(context-&gt;allocate_output(0, input_shape, &amp;in_backprop)); if (!(__builtin_expect(!!(_s.ok()), 1))) { (context)-&gt;CtxFailureWithWarning(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 358, _s); return; } } while (0)" data-ref="_M/OP_REQUIRES_OK">OP_REQUIRES_OK</a>(<a class="local col6 ref" href="#46context" title='context' data-ref="46context">context</a>,</td></tr>
<tr><th id="358">358</th><td>                   <a class="local col6 ref" href="#46context" title='context' data-ref="46context">context</a>-&gt;<a class="ref" href="../framework/op_kernel.h.html#_ZN10tensorflow15OpKernelContext15allocate_outputEiRKNS_11TensorShapeEPPNS_6TensorE" title='tensorflow::OpKernelContext::allocate_output' data-ref="_ZN10tensorflow15OpKernelContext15allocate_outputEiRKNS_11TensorShapeEPPNS_6TensorE">allocate_output</a>(<var>0</var>, <a class="local col0 ref" href="#50input_shape" title='input_shape' data-ref="50input_shape">input_shape</a>, &amp;<a class="local col2 ref" href="#52in_backprop" title='in_backprop' data-ref="52in_backprop">in_backprop</a>));</td></tr>
<tr><th id="359">359</th><td></td></tr>
<tr><th id="360">360</th><td>    <i>// If there is nothing to compute, return.</i></td></tr>
<tr><th id="361">361</th><td>    <b>if</b> (<a class="local col0 ref" href="#50input_shape" title='input_shape' data-ref="50input_shape">input_shape</a>.<a class="ref" href="../framework/tensor_shape.h.html#_ZNK10tensorflow14TensorShapeRep12num_elementsEv" title='tensorflow::TensorShapeRep::num_elements' data-ref="_ZNK10tensorflow14TensorShapeRep12num_elementsEv">num_elements</a>() == <var>0</var>) {</td></tr>
<tr><th id="362">362</th><td>      <b>return</b>;</td></tr>
<tr><th id="363">363</th><td>    }</td></tr>
<tr><th id="364">364</th><td></td></tr>
<tr><th id="365">365</th><td><i>// TODO(andydavis) Consider moving code shared with</i></td></tr>
<tr><th id="366">366</th><td><i>// Conv2DCustomBackpropFilterOp into a shared helper function.</i></td></tr>
<tr><th id="367">367</th><td><u>#<span data-ppcond="367">if</span> defined <span class="macro" data-ref="_M/TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS">TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS</span> &amp;&amp; \</u></td></tr>
<tr><th id="368">368</th><td><u>    defined <span class="macro" data-ref="_M/TENSORFLOW_USE_LIBXSMM_BACKWARD_CONVOLUTIONS">TENSORFLOW_USE_LIBXSMM_BACKWARD_CONVOLUTIONS</span></u></td></tr>
<tr><th id="369">369</th><td>    int64 pad_top, pad_bottom;</td></tr>
<tr><th id="370">370</th><td>    int64 pad_left, pad_right;</td></tr>
<tr><th id="371">371</th><td>    OP_REQUIRES_OK(</td></tr>
<tr><th id="372">372</th><td>        context,</td></tr>
<tr><th id="373">373</th><td>        GetWindowedOutputSizeVerbose(</td></tr>
<tr><th id="374">374</th><td>            dims.spatial_dims[<var>0</var>].input_size, dims.spatial_dims[<var>0</var>].filter_size,</td></tr>
<tr><th id="375">375</th><td>            dims.spatial_dims[<var>0</var>].stride, padding_,</td></tr>
<tr><th id="376">376</th><td>            &amp;dims.spatial_dims[<var>0</var>].output_size, &amp;pad_top, &amp;pad_bottom));</td></tr>
<tr><th id="377">377</th><td>    OP_REQUIRES_OK(</td></tr>
<tr><th id="378">378</th><td>        context,</td></tr>
<tr><th id="379">379</th><td>        GetWindowedOutputSizeVerbose(</td></tr>
<tr><th id="380">380</th><td>            dims.spatial_dims[<var>1</var>].input_size, dims.spatial_dims[<var>1</var>].filter_size,</td></tr>
<tr><th id="381">381</th><td>            dims.spatial_dims[<var>1</var>].stride, padding_,</td></tr>
<tr><th id="382">382</th><td>            &amp;dims.spatial_dims[<var>1</var>].output_size, &amp;pad_left, &amp;pad_right));</td></tr>
<tr><th id="383">383</th><td></td></tr>
<tr><th id="384">384</th><td>    <b>if</b> (pad_left == pad_right &amp;&amp; pad_top == pad_bottom) {</td></tr>
<tr><th id="385">385</th><td>      <b>if</b> (LaunchXsmmBackwardInputConvolution&lt;Device, T&gt;()(</td></tr>
<tr><th id="386">386</th><td>              context, context-&gt;eigen_device&lt;Device&gt;(),</td></tr>
<tr><th id="387">387</th><td>              in_backprop-&gt;tensor&lt;T, <var>4</var>&gt;(), filter.tensor&lt;T, <var>4</var>&gt;(),</td></tr>
<tr><th id="388">388</th><td>              out_backprop.tensor&lt;T, <var>4</var>&gt;(), dims.spatial_dims[<var>0</var>].input_size,</td></tr>
<tr><th id="389">389</th><td>              dims.spatial_dims[<var>1</var>].input_size,</td></tr>
<tr><th id="390">390</th><td>              <b>static_cast</b>&lt;<em>int</em>&gt;(dims.spatial_dims[<var>0</var>].stride),</td></tr>
<tr><th id="391">391</th><td>              <b>static_cast</b>&lt;<em>int</em>&gt;(dims.spatial_dims[<var>1</var>].stride),</td></tr>
<tr><th id="392">392</th><td>              <b>static_cast</b>&lt;<em>int</em>&gt;(pad_top), <b>static_cast</b>&lt;<em>int</em>&gt;(pad_left),</td></tr>
<tr><th id="393">393</th><td>              data_format_)) {</td></tr>
<tr><th id="394">394</th><td>        <b>return</b>;</td></tr>
<tr><th id="395">395</th><td>      }</td></tr>
<tr><th id="396">396</th><td>    }</td></tr>
<tr><th id="397">397</th><td><u>#<span data-ppcond="367">else</span></u></td></tr>
<tr><th id="398">398</th><td>    <a class="typedef" href="../platform/default/integral_types.h.html#tensorflow::int64" title='tensorflow::int64' data-type='long long' data-ref="tensorflow::int64">int64</a> <dfn class="local col3 decl" id="53pad_top" title='pad_top' data-type='int64' data-ref="53pad_top">pad_top</dfn>, <dfn class="local col4 decl" id="54pad_bottom" title='pad_bottom' data-type='int64' data-ref="54pad_bottom">pad_bottom</dfn>;</td></tr>
<tr><th id="399">399</th><td>    <a class="typedef" href="../platform/default/integral_types.h.html#tensorflow::int64" title='tensorflow::int64' data-type='long long' data-ref="tensorflow::int64">int64</a> <dfn class="local col5 decl" id="55pad_left" title='pad_left' data-type='int64' data-ref="55pad_left">pad_left</dfn>, <dfn class="local col6 decl" id="56pad_right" title='pad_right' data-type='int64' data-ref="56pad_right">pad_right</dfn>;</td></tr>
<tr><th id="400">400</th><td><u>#<span data-ppcond="367">endif</span></u></td></tr>
<tr><th id="401">401</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1545" title="do { ::tensorflow::Status _s(GetWindowedOutputSizeVerbose( dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size, dims.spatial_dims[0].stride, padding_, &amp;dims.spatial_dims[0].output_size, &amp;pad_top, &amp;pad_bottom)); if (!(__builtin_expect(!!(_s.ok()), 1))) { (context)-&gt;CtxFailureWithWarning(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 406, _s); return; } } while (0)" data-ref="_M/OP_REQUIRES_OK">OP_REQUIRES_OK</a>(</td></tr>
<tr><th id="402">402</th><td>        <a class="local col6 ref" href="#46context" title='context' data-ref="46context">context</a>,</td></tr>
<tr><th id="403">403</th><td>        GetWindowedOutputSizeVerbose(</td></tr>
<tr><th id="404">404</th><td>            <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a>[<var>0</var>].<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::input_size" title='tensorflow::ConvBackpropSpatialDimension::input_size' data-ref="tensorflow::ConvBackpropSpatialDimension::input_size">input_size</a>, <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a>[<var>0</var>].<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::filter_size" title='tensorflow::ConvBackpropSpatialDimension::filter_size' data-ref="tensorflow::ConvBackpropSpatialDimension::filter_size">filter_size</a>,</td></tr>
<tr><th id="405">405</th><td>            <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a>[<var>0</var>].<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::stride" title='tensorflow::ConvBackpropSpatialDimension::stride' data-ref="tensorflow::ConvBackpropSpatialDimension::stride">stride</a>, <a class="tu member" href="#tensorflow::Conv2DCustomBackpropInputOp::padding_" title='tensorflow::Conv2DCustomBackpropInputOp::padding_' data-ref="tensorflow::Conv2DCustomBackpropInputOp::padding_">padding_</a>,</td></tr>
<tr><th id="406">406</th><td>            &amp;<a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a>[<var>0</var>].<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::output_size" title='tensorflow::ConvBackpropSpatialDimension::output_size' data-ref="tensorflow::ConvBackpropSpatialDimension::output_size">output_size</a>, &amp;<a class="local col3 ref" href="#53pad_top" title='pad_top' data-ref="53pad_top">pad_top</a>, &amp;<a class="local col4 ref" href="#54pad_bottom" title='pad_bottom' data-ref="54pad_bottom">pad_bottom</a>));</td></tr>
<tr><th id="407">407</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1545" title="do { ::tensorflow::Status _s(GetWindowedOutputSizeVerbose( dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size, dims.spatial_dims[1].stride, padding_, &amp;dims.spatial_dims[1].output_size, &amp;pad_left, &amp;pad_right)); if (!(__builtin_expect(!!(_s.ok()), 1))) { (context)-&gt;CtxFailureWithWarning(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 412, _s); return; } } while (0)" data-ref="_M/OP_REQUIRES_OK">OP_REQUIRES_OK</a>(</td></tr>
<tr><th id="408">408</th><td>        <a class="local col6 ref" href="#46context" title='context' data-ref="46context">context</a>,</td></tr>
<tr><th id="409">409</th><td>        GetWindowedOutputSizeVerbose(</td></tr>
<tr><th id="410">410</th><td>            <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a>[<var>1</var>].<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::input_size" title='tensorflow::ConvBackpropSpatialDimension::input_size' data-ref="tensorflow::ConvBackpropSpatialDimension::input_size">input_size</a>, <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a>[<var>1</var>].<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::filter_size" title='tensorflow::ConvBackpropSpatialDimension::filter_size' data-ref="tensorflow::ConvBackpropSpatialDimension::filter_size">filter_size</a>,</td></tr>
<tr><th id="411">411</th><td>            <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a>[<var>1</var>].<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::stride" title='tensorflow::ConvBackpropSpatialDimension::stride' data-ref="tensorflow::ConvBackpropSpatialDimension::stride">stride</a>, <a class="tu member" href="#tensorflow::Conv2DCustomBackpropInputOp::padding_" title='tensorflow::Conv2DCustomBackpropInputOp::padding_' data-ref="tensorflow::Conv2DCustomBackpropInputOp::padding_">padding_</a>,</td></tr>
<tr><th id="412">412</th><td>            &amp;<a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a>[<var>1</var>].<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::output_size" title='tensorflow::ConvBackpropSpatialDimension::output_size' data-ref="tensorflow::ConvBackpropSpatialDimension::output_size">output_size</a>, &amp;<a class="local col5 ref" href="#55pad_left" title='pad_left' data-ref="55pad_left">pad_left</a>, &amp;<a class="local col6 ref" href="#56pad_right" title='pad_right' data-ref="56pad_right">pad_right</a>));</td></tr>
<tr><th id="413">413</th><td></td></tr>
<tr><th id="414">414</th><td>    <i>// The total dimension size of each kernel.</i></td></tr>
<tr><th id="415">415</th><td>    <em>const</em> <em>int</em> <dfn class="local col7 decl" id="57filter_total_size" title='filter_total_size' data-type='const int' data-ref="57filter_total_size">filter_total_size</dfn> = <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a><a class="ref" href="../lib/gtl/inlined_vector.h.html#_ZN10tensorflow3gtl13InlinedVectorixEm" title='tensorflow::gtl::InlinedVector::operator[]' data-ref="_ZN10tensorflow3gtl13InlinedVectorixEm">[<var>0</var>]</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::filter_size" title='tensorflow::ConvBackpropSpatialDimension::filter_size' data-ref="tensorflow::ConvBackpropSpatialDimension::filter_size">filter_size</a> *</td></tr>
<tr><th id="416">416</th><td>                                  <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a><a class="ref" href="../lib/gtl/inlined_vector.h.html#_ZN10tensorflow3gtl13InlinedVectorixEm" title='tensorflow::gtl::InlinedVector::operator[]' data-ref="_ZN10tensorflow3gtl13InlinedVectorixEm">[<var>1</var>]</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::filter_size" title='tensorflow::ConvBackpropSpatialDimension::filter_size' data-ref="tensorflow::ConvBackpropSpatialDimension::filter_size">filter_size</a> *</td></tr>
<tr><th id="417">417</th><td>                                  <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::in_depth" title='tensorflow::ConvBackpropDimensions::in_depth' data-ref="tensorflow::ConvBackpropDimensions::in_depth">in_depth</a>;</td></tr>
<tr><th id="418">418</th><td>    <i>// The output image size is the spatial size of the output.</i></td></tr>
<tr><th id="419">419</th><td>    <em>const</em> <em>int</em> <dfn class="local col8 decl" id="58output_image_size" title='output_image_size' data-type='const int' data-ref="58output_image_size">output_image_size</dfn> =</td></tr>
<tr><th id="420">420</th><td>        <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a><a class="ref" href="../lib/gtl/inlined_vector.h.html#_ZN10tensorflow3gtl13InlinedVectorixEm" title='tensorflow::gtl::InlinedVector::operator[]' data-ref="_ZN10tensorflow3gtl13InlinedVectorixEm">[<var>0</var>]</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::output_size" title='tensorflow::ConvBackpropSpatialDimension::output_size' data-ref="tensorflow::ConvBackpropSpatialDimension::output_size">output_size</a> * <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a><a class="ref" href="../lib/gtl/inlined_vector.h.html#_ZN10tensorflow3gtl13InlinedVectorixEm" title='tensorflow::gtl::InlinedVector::operator[]' data-ref="_ZN10tensorflow3gtl13InlinedVectorixEm">[<var>1</var>]</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::output_size" title='tensorflow::ConvBackpropSpatialDimension::output_size' data-ref="tensorflow::ConvBackpropSpatialDimension::output_size">output_size</a>;</td></tr>
<tr><th id="421">421</th><td></td></tr>
<tr><th id="422">422</th><td>    <i>// TODO(andydavis) Get L2/L3 cache sizes from device.</i></td></tr>
<tr><th id="423">423</th><td>    <em>const</em> <span class='typedef' title='size_t' data-type='unsigned long' data-ref="size_t">size_t</span> <dfn class="local col9 decl" id="59l2_cache_size" title='l2_cache_size' data-type='const size_t' data-ref="59l2_cache_size">l2_cache_size</dfn> = <var>256LL</var> &lt;&lt; <var>10</var>;</td></tr>
<tr><th id="424">424</th><td>    <em>const</em> <span class='typedef' title='size_t' data-type='unsigned long' data-ref="size_t">size_t</span> <dfn class="local col0 decl" id="60l3_cache_size" title='l3_cache_size' data-type='const size_t' data-ref="60l3_cache_size">l3_cache_size</dfn> = <var>30LL</var> &lt;&lt; <var>20</var>;</td></tr>
<tr><th id="425">425</th><td></td></tr>
<tr><th id="426">426</th><td>    <i>// Use L3 cache size as target working set size.</i></td></tr>
<tr><th id="427">427</th><td>    <em>const</em> <span class='typedef' title='size_t' data-type='unsigned long' data-ref="size_t">size_t</span> <dfn class="local col1 decl" id="61target_working_set_size" title='target_working_set_size' data-type='const size_t' data-ref="61target_working_set_size">target_working_set_size</dfn> = <a class="local col0 ref" href="#60l3_cache_size" title='l3_cache_size' data-ref="60l3_cache_size">l3_cache_size</a> / <b>sizeof</b>(T);</td></tr>
<tr><th id="428">428</th><td></td></tr>
<tr><th id="429">429</th><td>    <i>// Calculate size of matrices involved in MatMul: C = A x B.</i></td></tr>
<tr><th id="430">430</th><td>    <em>const</em> <span class='typedef' title='size_t' data-type='unsigned long' data-ref="size_t">size_t</span> <dfn class="local col2 decl" id="62size_A" title='size_A' data-type='const size_t' data-ref="62size_A">size_A</dfn> = <a class="local col8 ref" href="#58output_image_size" title='output_image_size' data-ref="58output_image_size">output_image_size</a> * <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::out_depth" title='tensorflow::ConvBackpropDimensions::out_depth' data-ref="tensorflow::ConvBackpropDimensions::out_depth">out_depth</a>;</td></tr>
<tr><th id="431">431</th><td></td></tr>
<tr><th id="432">432</th><td>    <em>const</em> <span class='typedef' title='size_t' data-type='unsigned long' data-ref="size_t">size_t</span> <dfn class="local col3 decl" id="63size_B" title='size_B' data-type='const size_t' data-ref="63size_B">size_B</dfn> = <a class="local col7 ref" href="#57filter_total_size" title='filter_total_size' data-ref="57filter_total_size">filter_total_size</a> * <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::out_depth" title='tensorflow::ConvBackpropDimensions::out_depth' data-ref="tensorflow::ConvBackpropDimensions::out_depth">out_depth</a>;</td></tr>
<tr><th id="433">433</th><td></td></tr>
<tr><th id="434">434</th><td>    <em>const</em> <span class='typedef' title='size_t' data-type='unsigned long' data-ref="size_t">size_t</span> <dfn class="local col4 decl" id="64size_C" title='size_C' data-type='const size_t' data-ref="64size_C">size_C</dfn> = <a class="local col8 ref" href="#58output_image_size" title='output_image_size' data-ref="58output_image_size">output_image_size</a> * <a class="local col7 ref" href="#57filter_total_size" title='filter_total_size' data-ref="57filter_total_size">filter_total_size</a>;</td></tr>
<tr><th id="435">435</th><td></td></tr>
<tr><th id="436">436</th><td>    <em>const</em> <span class='typedef' title='size_t' data-type='unsigned long' data-ref="size_t">size_t</span> <dfn class="local col5 decl" id="65work_unit_size" title='work_unit_size' data-type='const size_t' data-ref="65work_unit_size">work_unit_size</dfn> = <a class="local col2 ref" href="#62size_A" title='size_A' data-ref="62size_A">size_A</a> + <a class="local col3 ref" href="#63size_B" title='size_B' data-ref="63size_B">size_B</a> + <a class="local col4 ref" href="#64size_C" title='size_C' data-ref="64size_C">size_C</a>;</td></tr>
<tr><th id="437">437</th><td></td></tr>
<tr><th id="438">438</th><td>    <em>auto</em> <dfn class="local col6 decl" id="66worker_threads" title='worker_threads' data-type='tensorflow::DeviceBase::CpuWorkerThreads' data-ref="66worker_threads">worker_threads</dfn> = <a class="ref fake" href="../framework/device_base.h.html#110" title='tensorflow::DeviceBase::CpuWorkerThreads::CpuWorkerThreads' data-ref="_ZN10tensorflow10DeviceBase16CpuWorkerThreadsC1ERKS1_"></a>*(<a class="local col6 ref" href="#46context" title='context' data-ref="46context">context</a>-&gt;<a class="ref" href="../framework/op_kernel.h.html#_ZNK10tensorflow15OpKernelContext6deviceEv" title='tensorflow::OpKernelContext::device' data-ref="_ZNK10tensorflow15OpKernelContext6deviceEv">device</a>()-&gt;<a class="virtual ref" href="../framework/device_base.h.html#_ZNK10tensorflow10DeviceBase29tensorflow_cpu_worker_threadsEv" title='tensorflow::DeviceBase::tensorflow_cpu_worker_threads' data-ref="_ZNK10tensorflow10DeviceBase29tensorflow_cpu_worker_threadsEv">tensorflow_cpu_worker_threads</a>());</td></tr>
<tr><th id="439">439</th><td></td></tr>
<tr><th id="440">440</th><td>    <i>// Calculate per-thread work unit size.</i></td></tr>
<tr><th id="441">441</th><td>    <em>const</em> <span class='typedef' title='size_t' data-type='unsigned long' data-ref="size_t">size_t</span> <dfn class="local col7 decl" id="67thread_work_unit_size" title='thread_work_unit_size' data-type='const size_t' data-ref="67thread_work_unit_size">thread_work_unit_size</dfn> =</td></tr>
<tr><th id="442">442</th><td>        <a class="local col5 ref" href="#65work_unit_size" title='work_unit_size' data-ref="65work_unit_size">work_unit_size</a> / <a class="local col6 ref" href="#66worker_threads" title='worker_threads' data-ref="66worker_threads">worker_threads</a>.<a class="ref" href="../framework/device_base.h.html#tensorflow::DeviceBase::CpuWorkerThreads::num_threads" title='tensorflow::DeviceBase::CpuWorkerThreads::num_threads' data-ref="tensorflow::DeviceBase::CpuWorkerThreads::num_threads">num_threads</a>;</td></tr>
<tr><th id="443">443</th><td></td></tr>
<tr><th id="444">444</th><td>    <i>// Set minimum per-thread work unit size to size of L2 cache.</i></td></tr>
<tr><th id="445">445</th><td>    <em>const</em> <span class='typedef' title='size_t' data-type='unsigned long' data-ref="size_t">size_t</span> <dfn class="local col8 decl" id="68min_thread_work_unit_size" title='min_thread_work_unit_size' data-type='const size_t' data-ref="68min_thread_work_unit_size">min_thread_work_unit_size</dfn> = <a class="local col9 ref" href="#59l2_cache_size" title='l2_cache_size' data-ref="59l2_cache_size">l2_cache_size</a> / <b>sizeof</b>(T);</td></tr>
<tr><th id="446">446</th><td></td></tr>
<tr><th id="447">447</th><td>    <i>// Use parallel tensor contractions if there is no batching, or if the</i></td></tr>
<tr><th id="448">448</th><td><i>    // minimum per-thread work unit size threshold has been exceeded.</i></td></tr>
<tr><th id="449">449</th><td><i>    // Otherwise, revert to multiple single-threaded matmul ops running in</i></td></tr>
<tr><th id="450">450</th><td><i>    // parallel to keep all threads busy.</i></td></tr>
<tr><th id="451">451</th><td><i>    // TODO(andydavis) Explore alternatives to branching the code in this way</i></td></tr>
<tr><th id="452">452</th><td><i>    // (i.e. run multiple, parallel tensor contractions in another thread pool).</i></td></tr>
<tr><th id="453">453</th><td>    <em>const</em> <em>bool</em> <dfn class="local col9 decl" id="69use_parallel_contraction" title='use_parallel_contraction' data-type='const bool' data-ref="69use_parallel_contraction">use_parallel_contraction</dfn> =</td></tr>
<tr><th id="454">454</th><td>        <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::batch_size" title='tensorflow::ConvBackpropDimensions::batch_size' data-ref="tensorflow::ConvBackpropDimensions::batch_size">batch_size</a> == <var>1</var> ||</td></tr>
<tr><th id="455">455</th><td>        <a class="local col7 ref" href="#67thread_work_unit_size" title='thread_work_unit_size' data-ref="67thread_work_unit_size">thread_work_unit_size</a> &gt;= <a class="local col8 ref" href="#68min_thread_work_unit_size" title='min_thread_work_unit_size' data-ref="68min_thread_work_unit_size">min_thread_work_unit_size</a>;</td></tr>
<tr><th id="456">456</th><td></td></tr>
<tr><th id="457">457</th><td>    <em>const</em> <span class='typedef' title='size_t' data-type='unsigned long' data-ref="size_t">size_t</span> <dfn class="local col0 decl" id="70shard_size" title='shard_size' data-type='const size_t' data-ref="70shard_size">shard_size</dfn> =</td></tr>
<tr><th id="458">458</th><td>        <a class="local col9 ref" href="#69use_parallel_contraction" title='use_parallel_contraction' data-ref="69use_parallel_contraction">use_parallel_contraction</a></td></tr>
<tr><th id="459">459</th><td>            ? <var>1</var></td></tr>
<tr><th id="460">460</th><td>            : (<a class="local col1 ref" href="#61target_working_set_size" title='target_working_set_size' data-ref="61target_working_set_size">target_working_set_size</a> + <a class="local col5 ref" href="#65work_unit_size" title='work_unit_size' data-ref="65work_unit_size">work_unit_size</a> - <var>1</var>) / <a class="local col5 ref" href="#65work_unit_size" title='work_unit_size' data-ref="65work_unit_size">work_unit_size</a>;</td></tr>
<tr><th id="461">461</th><td></td></tr>
<tr><th id="462">462</th><td>    <a class="type" href="../framework/tensor.h.html#tensorflow::Tensor" title='tensorflow::Tensor' data-ref="tensorflow::Tensor">Tensor</a> <a class="ref fake" href="../framework/tensor.h.html#_ZN10tensorflow6TensorC1Ev" title='tensorflow::Tensor::Tensor' data-ref="_ZN10tensorflow6TensorC1Ev"></a><dfn class="local col1 decl" id="71col_buffer" title='col_buffer' data-type='tensorflow::Tensor' data-ref="71col_buffer">col_buffer</dfn>;</td></tr>
<tr><th id="463">463</th><td>    <a class="macro" href="../framework/op_kernel.h.html#1545" title="do { ::tensorflow::Status _s(context-&gt;allocate_temp( DataTypeToEnum&lt;T&gt;::value, TensorShape({static_cast&lt;int64&gt;(shard_size), static_cast&lt;int64&gt;(output_image_size), static_cast&lt;int64&gt;(filter_total_size)}), &amp;col_buffer)); if (!(__builtin_expect(!!(_s.ok()), 1))) { (context)-&gt;CtxFailureWithWarning(&quot;/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc&quot;, 469, _s); return; } } while (0)" data-ref="_M/OP_REQUIRES_OK">OP_REQUIRES_OK</a>(<a class="local col6 ref" href="#46context" title='context' data-ref="46context">context</a>,</td></tr>
<tr><th id="464">464</th><td>                   <a class="local col6 ref" href="#46context" title='context' data-ref="46context">context</a>-&gt;allocate_temp(</td></tr>
<tr><th id="465">465</th><td>                       <a class="type" href="../framework/types.h.html#tensorflow::DataTypeToEnum" title='tensorflow::DataTypeToEnum' data-ref="tensorflow::DataTypeToEnum">DataTypeToEnum</a>&lt;T&gt;::value,</td></tr>
<tr><th id="466">466</th><td>                       <a class="type" href="../framework/tensor_shape.h.html#tensorflow::TensorShape" title='tensorflow::TensorShape' data-ref="tensorflow::TensorShape">TensorShape</a>({<b>static_cast</b>&lt;<a class="typedef" href="../platform/default/integral_types.h.html#tensorflow::int64" title='tensorflow::int64' data-type='long long' data-ref="tensorflow::int64">int64</a>&gt;(<a class="local col0 ref" href="#70shard_size" title='shard_size' data-ref="70shard_size">shard_size</a>),</td></tr>
<tr><th id="467">467</th><td>                                    <b>static_cast</b>&lt;<a class="typedef" href="../platform/default/integral_types.h.html#tensorflow::int64" title='tensorflow::int64' data-type='long long' data-ref="tensorflow::int64">int64</a>&gt;(<a class="local col8 ref" href="#58output_image_size" title='output_image_size' data-ref="58output_image_size">output_image_size</a>),</td></tr>
<tr><th id="468">468</th><td>                                    <b>static_cast</b>&lt;<a class="typedef" href="../platform/default/integral_types.h.html#tensorflow::int64" title='tensorflow::int64' data-type='long long' data-ref="tensorflow::int64">int64</a>&gt;(<a class="local col7 ref" href="#57filter_total_size" title='filter_total_size' data-ref="57filter_total_size">filter_total_size</a>)}),</td></tr>
<tr><th id="469">469</th><td>                       &amp;<a class="local col1 ref" href="#71col_buffer" title='col_buffer' data-ref="71col_buffer">col_buffer</a>));</td></tr>
<tr><th id="470">470</th><td></td></tr>
<tr><th id="471">471</th><td>    <i>// The input offset corresponding to a single input image.</i></td></tr>
<tr><th id="472">472</th><td>    <em>const</em> <em>int</em> <dfn class="local col2 decl" id="72input_offset" title='input_offset' data-type='const int' data-ref="72input_offset">input_offset</dfn> = <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a><a class="ref" href="../lib/gtl/inlined_vector.h.html#_ZN10tensorflow3gtl13InlinedVectorixEm" title='tensorflow::gtl::InlinedVector::operator[]' data-ref="_ZN10tensorflow3gtl13InlinedVectorixEm">[<var>0</var>]</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::input_size" title='tensorflow::ConvBackpropSpatialDimension::input_size' data-ref="tensorflow::ConvBackpropSpatialDimension::input_size">input_size</a> *</td></tr>
<tr><th id="473">473</th><td>                             <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a><a class="ref" href="../lib/gtl/inlined_vector.h.html#_ZN10tensorflow3gtl13InlinedVectorixEm" title='tensorflow::gtl::InlinedVector::operator[]' data-ref="_ZN10tensorflow3gtl13InlinedVectorixEm">[<var>1</var>]</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::input_size" title='tensorflow::ConvBackpropSpatialDimension::input_size' data-ref="tensorflow::ConvBackpropSpatialDimension::input_size">input_size</a> * <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::in_depth" title='tensorflow::ConvBackpropDimensions::in_depth' data-ref="tensorflow::ConvBackpropDimensions::in_depth">in_depth</a>;</td></tr>
<tr><th id="474">474</th><td>    <i>// The output offset corresponding to a single output image.</i></td></tr>
<tr><th id="475">475</th><td>    <em>const</em> <em>int</em> <dfn class="local col3 decl" id="73output_offset" title='output_offset' data-type='const int' data-ref="73output_offset">output_offset</dfn> = <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a><a class="ref" href="../lib/gtl/inlined_vector.h.html#_ZN10tensorflow3gtl13InlinedVectorixEm" title='tensorflow::gtl::InlinedVector::operator[]' data-ref="_ZN10tensorflow3gtl13InlinedVectorixEm">[<var>0</var>]</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::output_size" title='tensorflow::ConvBackpropSpatialDimension::output_size' data-ref="tensorflow::ConvBackpropSpatialDimension::output_size">output_size</a> *</td></tr>
<tr><th id="476">476</th><td>                              <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a><a class="ref" href="../lib/gtl/inlined_vector.h.html#_ZN10tensorflow3gtl13InlinedVectorixEm" title='tensorflow::gtl::InlinedVector::operator[]' data-ref="_ZN10tensorflow3gtl13InlinedVectorixEm">[<var>1</var>]</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::output_size" title='tensorflow::ConvBackpropSpatialDimension::output_size' data-ref="tensorflow::ConvBackpropSpatialDimension::output_size">output_size</a> * <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::out_depth" title='tensorflow::ConvBackpropDimensions::out_depth' data-ref="tensorflow::ConvBackpropDimensions::out_depth">out_depth</a>;</td></tr>
<tr><th id="477">477</th><td></td></tr>
<tr><th id="478">478</th><td>    <em>const</em> T* <dfn class="local col4 decl" id="74filter_data" title='filter_data' data-type='const T *' data-ref="74filter_data">filter_data</dfn> = <a class="local col8 ref" href="#48filter" title='filter' data-ref="48filter">filter</a>.<b>template</b> flat&lt;T&gt;().data();</td></tr>
<tr><th id="479">479</th><td>    T* <dfn class="local col5 decl" id="75col_buffer_data" title='col_buffer_data' data-type='T *' data-ref="75col_buffer_data">col_buffer_data</dfn> = <a class="local col1 ref" href="#71col_buffer" title='col_buffer' data-ref="71col_buffer">col_buffer</a>.<b>template</b> flat&lt;T&gt;().data();</td></tr>
<tr><th id="480">480</th><td>    <em>const</em> T* <dfn class="local col6 decl" id="76out_backprop_data" title='out_backprop_data' data-type='const T *' data-ref="76out_backprop_data">out_backprop_data</dfn> = <a class="local col9 ref" href="#49out_backprop" title='out_backprop' data-ref="49out_backprop">out_backprop</a>.<b>template</b> flat&lt;T&gt;().data();</td></tr>
<tr><th id="481">481</th><td></td></tr>
<tr><th id="482">482</th><td>    <em>auto</em> <dfn class="local col7 decl" id="77in_backprop_flat" title='in_backprop_flat' data-type='auto' data-ref="77in_backprop_flat">in_backprop_flat</dfn> = <a class="local col2 ref" href="#52in_backprop" title='in_backprop' data-ref="52in_backprop">in_backprop</a>-&gt;<b>template</b> flat&lt;T&gt;();</td></tr>
<tr><th id="483">483</th><td>    T* <dfn class="local col8 decl" id="78input_backprop_data" title='input_backprop_data' data-type='T *' data-ref="78input_backprop_data">input_backprop_data</dfn> = <a class="local col7 ref" href="#77in_backprop_flat" title='in_backprop_flat' data-ref="77in_backprop_flat">in_backprop_flat</a>.data();</td></tr>
<tr><th id="484">484</th><td>    <a class="local col7 ref" href="#77in_backprop_flat" title='in_backprop_flat' data-ref="77in_backprop_flat">in_backprop_flat</a>.device(<a class="local col6 ref" href="#46context" title='context' data-ref="46context">context</a>-&gt;eigen_device&lt;Device&gt;()) =</td></tr>
<tr><th id="485">485</th><td>        <a class="local col7 ref" href="#77in_backprop_flat" title='in_backprop_flat' data-ref="77in_backprop_flat">in_backprop_flat</a>.constant(T(<var>0</var>));</td></tr>
<tr><th id="486">486</th><td></td></tr>
<tr><th id="487">487</th><td>    <b>if</b> (<a class="local col9 ref" href="#69use_parallel_contraction" title='use_parallel_contraction' data-ref="69use_parallel_contraction">use_parallel_contraction</a>) {</td></tr>
<tr><th id="488">488</th><td>      <b>typedef</b> <span class="namespace">Eigen::</span><span class='type' title='Eigen::TensorMap' data-ref="Eigen::TensorMap">TensorMap</span>&lt;<span class="namespace">Eigen::</span><span class='type' title='Eigen::Tensor' data-ref="Eigen::Tensor">Tensor</span>&lt;T, <var>2</var>, <span class="namespace">Eigen::</span><span class='enum' title='Eigen::StorageOptions::RowMajor' data-ref="Eigen::StorageOptions::RowMajor">RowMajor</span>&gt;,</td></tr>
<tr><th id="489">489</th><td>                               <span class="namespace">Eigen::</span><span class='enum' title='Eigen::AlignmentType::Unaligned' data-ref="Eigen::AlignmentType::Unaligned">Unaligned</span>&gt;</td></tr>
<tr><th id="490">490</th><td>          <dfn class="local col9 typedef" id="79TensorMap" title='TensorMap' data-type='Eigen::TensorMap&lt;Eigen::Tensor&lt;T, 2, Eigen::RowMajor&gt;, Eigen::Unaligned&gt;' data-ref="79TensorMap">TensorMap</dfn>;</td></tr>
<tr><th id="491">491</th><td>      <b>typedef</b> <span class="namespace">Eigen::</span><span class='type' title='Eigen::TensorMap' data-ref="Eigen::TensorMap">TensorMap</span>&lt;<span class="namespace">Eigen::</span><span class='type' title='Eigen::Tensor' data-ref="Eigen::Tensor">Tensor</span>&lt;<em>const</em> T, <var>2</var>, <span class="namespace">Eigen::</span><span class='enum' title='Eigen::StorageOptions::RowMajor' data-ref="Eigen::StorageOptions::RowMajor">RowMajor</span>&gt;,</td></tr>
<tr><th id="492">492</th><td>                               <span class="namespace">Eigen::</span><span class='enum' title='Eigen::AlignmentType::Unaligned' data-ref="Eigen::AlignmentType::Unaligned">Unaligned</span>&gt;</td></tr>
<tr><th id="493">493</th><td>          <dfn class="local col0 typedef" id="80ConstTensorMap" title='ConstTensorMap' data-type='Eigen::TensorMap&lt;Eigen::Tensor&lt;const T, 2, Eigen::RowMajor&gt;, Eigen::Unaligned&gt;' data-ref="80ConstTensorMap">ConstTensorMap</dfn>;</td></tr>
<tr><th id="494">494</th><td></td></tr>
<tr><th id="495">495</th><td>      <i>// Initialize contraction dims (we need to transpose 'B' below).</i></td></tr>
<tr><th id="496">496</th><td>      <span class="namespace">Eigen::</span><span class='type' title='Eigen::array' data-ref="Eigen::array">array</span>&lt;<span class="namespace">Eigen::</span><span class='type' title='Eigen::IndexPair' data-ref="Eigen::IndexPair">IndexPair</span>&lt;<span class="namespace">Eigen::</span><span class='typedef' title='Eigen::DenseIndex' data-type='std::ptrdiff_t' data-ref="Eigen::DenseIndex">DenseIndex</span>&gt;, <var>1</var>&gt; <span class='ref fake' title='Eigen::array::array&lt;T, n&gt;' data-ref="_ZN5Eigen5arrayC1Ev"></span><dfn class="local col1 decl" id="81contract_dims" title='contract_dims' data-type='Eigen::array&lt;Eigen::IndexPair&lt;Eigen::DenseIndex&gt;, 1&gt;' data-ref="81contract_dims">contract_dims</dfn>;</td></tr>
<tr><th id="497">497</th><td>      <a class="local col1 ref" href="#81contract_dims" title='contract_dims' data-ref="81contract_dims">contract_dims</a><span class='ref' title='Eigen::array::operator[]' data-ref="_ZN5Eigen5arrayixEm">[<var>0</var>]</span>.<span class='ref' title='Eigen::IndexPair&lt;long&gt;::first' data-ref="Eigen::IndexPair::first">first</span> = <var>1</var>;</td></tr>
<tr><th id="498">498</th><td>      <a class="local col1 ref" href="#81contract_dims" title='contract_dims' data-ref="81contract_dims">contract_dims</a><span class='ref' title='Eigen::array::operator[]' data-ref="_ZN5Eigen5arrayixEm">[<var>0</var>]</span>.<span class='ref' title='Eigen::IndexPair&lt;long&gt;::second' data-ref="Eigen::IndexPair::second">second</span> = <var>1</var>;</td></tr>
<tr><th id="499">499</th><td></td></tr>
<tr><th id="500">500</th><td>      <b>for</b> (<em>int</em> <dfn class="local col2 decl" id="82image_id" title='image_id' data-type='int' data-ref="82image_id">image_id</dfn> = <var>0</var>; <a class="local col2 ref" href="#82image_id" title='image_id' data-ref="82image_id">image_id</a> &lt; <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::batch_size" title='tensorflow::ConvBackpropDimensions::batch_size' data-ref="tensorflow::ConvBackpropDimensions::batch_size">batch_size</a>; ++<a class="local col2 ref" href="#82image_id" title='image_id' data-ref="82image_id">image_id</a>) {</td></tr>
<tr><th id="501">501</th><td>        <i>// Compute gradient into col_buffer.</i></td></tr>
<tr><th id="502">502</th><td>        <a class="local col9 typedef" href="#79TensorMap" title='TensorMap' data-type='Eigen::TensorMap&lt;Eigen::Tensor&lt;T, 2, Eigen::RowMajor&gt;, Eigen::Unaligned&gt;' data-ref="79TensorMap">TensorMap</a> <dfn class="local col3 decl" id="83C" title='C' data-type='TensorMap' data-ref="83C">C</dfn>(<a class="local col5 ref" href="#75col_buffer_data" title='col_buffer_data' data-ref="75col_buffer_data">col_buffer_data</a>, <a class="local col8 ref" href="#58output_image_size" title='output_image_size' data-ref="58output_image_size">output_image_size</a>, <a class="local col7 ref" href="#57filter_total_size" title='filter_total_size' data-ref="57filter_total_size">filter_total_size</a>);</td></tr>
<tr><th id="503">503</th><td></td></tr>
<tr><th id="504">504</th><td>        <a class="local col0 typedef" href="#80ConstTensorMap" title='ConstTensorMap' data-type='Eigen::TensorMap&lt;Eigen::Tensor&lt;const T, 2, Eigen::RowMajor&gt;, Eigen::Unaligned&gt;' data-ref="80ConstTensorMap">ConstTensorMap</a> <dfn class="local col4 decl" id="84A" title='A' data-type='ConstTensorMap' data-ref="84A">A</dfn>(<a class="local col6 ref" href="#76out_backprop_data" title='out_backprop_data' data-ref="76out_backprop_data">out_backprop_data</a> + <a class="local col3 ref" href="#73output_offset" title='output_offset' data-ref="73output_offset">output_offset</a> * <a class="local col2 ref" href="#82image_id" title='image_id' data-ref="82image_id">image_id</a>,</td></tr>
<tr><th id="505">505</th><td>                         <a class="local col8 ref" href="#58output_image_size" title='output_image_size' data-ref="58output_image_size">output_image_size</a>, <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::out_depth" title='tensorflow::ConvBackpropDimensions::out_depth' data-ref="tensorflow::ConvBackpropDimensions::out_depth">out_depth</a>);</td></tr>
<tr><th id="506">506</th><td>        <a class="local col0 typedef" href="#80ConstTensorMap" title='ConstTensorMap' data-type='Eigen::TensorMap&lt;Eigen::Tensor&lt;const T, 2, Eigen::RowMajor&gt;, Eigen::Unaligned&gt;' data-ref="80ConstTensorMap">ConstTensorMap</a> <dfn class="local col5 decl" id="85B" title='B' data-type='ConstTensorMap' data-ref="85B">B</dfn>(<a class="local col4 ref" href="#74filter_data" title='filter_data' data-ref="74filter_data">filter_data</a>, <a class="local col7 ref" href="#57filter_total_size" title='filter_total_size' data-ref="57filter_total_size">filter_total_size</a>, <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::out_depth" title='tensorflow::ConvBackpropDimensions::out_depth' data-ref="tensorflow::ConvBackpropDimensions::out_depth">out_depth</a>);</td></tr>
<tr><th id="507">507</th><td></td></tr>
<tr><th id="508">508</th><td>        <a class="local col3 ref" href="#83C" title='C' data-ref="83C">C</a>.device(<a class="local col6 ref" href="#46context" title='context' data-ref="46context">context</a>-&gt;<a class="ref" href="../framework/op_kernel.h.html#_ZNK10tensorflow15OpKernelContext16eigen_cpu_deviceEv" title='tensorflow::OpKernelContext::eigen_cpu_device' data-ref="_ZNK10tensorflow15OpKernelContext16eigen_cpu_deviceEv">eigen_cpu_device</a>()) = <a class="local col4 ref" href="#84A" title='A' data-ref="84A">A</a>.contract(<a class="local col5 ref" href="#85B" title='B' data-ref="85B">B</a>, <a class="local col1 ref" href="#81contract_dims" title='contract_dims' data-ref="81contract_dims">contract_dims</a>);</td></tr>
<tr><th id="509">509</th><td></td></tr>
<tr><th id="510">510</th><td>        Col2im&lt;T&gt;(</td></tr>
<tr><th id="511">511</th><td>            <a class="local col5 ref" href="#75col_buffer_data" title='col_buffer_data' data-ref="75col_buffer_data">col_buffer_data</a>, <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::in_depth" title='tensorflow::ConvBackpropDimensions::in_depth' data-ref="tensorflow::ConvBackpropDimensions::in_depth">in_depth</a>, <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a><a class="ref" href="../lib/gtl/inlined_vector.h.html#_ZN10tensorflow3gtl13InlinedVectorixEm" title='tensorflow::gtl::InlinedVector::operator[]' data-ref="_ZN10tensorflow3gtl13InlinedVectorixEm">[<var>0</var>]</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::input_size" title='tensorflow::ConvBackpropSpatialDimension::input_size' data-ref="tensorflow::ConvBackpropSpatialDimension::input_size">input_size</a>,</td></tr>
<tr><th id="512">512</th><td>            <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a><a class="ref" href="../lib/gtl/inlined_vector.h.html#_ZN10tensorflow3gtl13InlinedVectorixEm" title='tensorflow::gtl::InlinedVector::operator[]' data-ref="_ZN10tensorflow3gtl13InlinedVectorixEm">[<var>1</var>]</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::input_size" title='tensorflow::ConvBackpropSpatialDimension::input_size' data-ref="tensorflow::ConvBackpropSpatialDimension::input_size">input_size</a>, <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a><a class="ref" href="../lib/gtl/inlined_vector.h.html#_ZN10tensorflow3gtl13InlinedVectorixEm" title='tensorflow::gtl::InlinedVector::operator[]' data-ref="_ZN10tensorflow3gtl13InlinedVectorixEm">[<var>0</var>]</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::filter_size" title='tensorflow::ConvBackpropSpatialDimension::filter_size' data-ref="tensorflow::ConvBackpropSpatialDimension::filter_size">filter_size</a>,</td></tr>
<tr><th id="513">513</th><td>            <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a><a class="ref" href="../lib/gtl/inlined_vector.h.html#_ZN10tensorflow3gtl13InlinedVectorixEm" title='tensorflow::gtl::InlinedVector::operator[]' data-ref="_ZN10tensorflow3gtl13InlinedVectorixEm">[<var>1</var>]</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::filter_size" title='tensorflow::ConvBackpropSpatialDimension::filter_size' data-ref="tensorflow::ConvBackpropSpatialDimension::filter_size">filter_size</a>, <a class="local col3 ref" href="#53pad_top" title='pad_top' data-ref="53pad_top">pad_top</a>, <a class="local col5 ref" href="#55pad_left" title='pad_left' data-ref="55pad_left">pad_left</a>, <a class="local col4 ref" href="#54pad_bottom" title='pad_bottom' data-ref="54pad_bottom">pad_bottom</a>,</td></tr>
<tr><th id="514">514</th><td>            <a class="local col6 ref" href="#56pad_right" title='pad_right' data-ref="56pad_right">pad_right</a>, <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a><a class="ref" href="../lib/gtl/inlined_vector.h.html#_ZN10tensorflow3gtl13InlinedVectorixEm" title='tensorflow::gtl::InlinedVector::operator[]' data-ref="_ZN10tensorflow3gtl13InlinedVectorixEm">[<var>0</var>]</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::stride" title='tensorflow::ConvBackpropSpatialDimension::stride' data-ref="tensorflow::ConvBackpropSpatialDimension::stride">stride</a>, <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a><a class="ref" href="../lib/gtl/inlined_vector.h.html#_ZN10tensorflow3gtl13InlinedVectorixEm" title='tensorflow::gtl::InlinedVector::operator[]' data-ref="_ZN10tensorflow3gtl13InlinedVectorixEm">[<var>1</var>]</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::stride" title='tensorflow::ConvBackpropSpatialDimension::stride' data-ref="tensorflow::ConvBackpropSpatialDimension::stride">stride</a>,</td></tr>
<tr><th id="515">515</th><td>            <a class="local col8 ref" href="#78input_backprop_data" title='input_backprop_data' data-ref="78input_backprop_data">input_backprop_data</a>);</td></tr>
<tr><th id="516">516</th><td></td></tr>
<tr><th id="517">517</th><td>        <a class="local col8 ref" href="#78input_backprop_data" title='input_backprop_data' data-ref="78input_backprop_data">input_backprop_data</a> += <a class="local col2 ref" href="#72input_offset" title='input_offset' data-ref="72input_offset">input_offset</a>;</td></tr>
<tr><th id="518">518</th><td>      }</td></tr>
<tr><th id="519">519</th><td>    } <b>else</b> {</td></tr>
<tr><th id="520">520</th><td>      <b>typedef</b> <span class="namespace">Eigen::</span><span class='type' title='Eigen::Map' data-ref="Eigen::Map">Map</span>&lt;</td></tr>
<tr><th id="521">521</th><td>          <span class="namespace">Eigen::</span><span class='type' title='Eigen::Matrix' data-ref="Eigen::Matrix">Matrix</span>&lt;T, <span class="namespace">Eigen::</span><span class='ref' title='Eigen::Dynamic' data-ref="Eigen::Dynamic">Dynamic</span>, <span class="namespace">Eigen::</span><span class='ref' title='Eigen::Dynamic' data-ref="Eigen::Dynamic">Dynamic</span>, <span class="namespace">Eigen::</span><span class='enum' title='Eigen::StorageOptions::RowMajor' data-ref="Eigen::StorageOptions::RowMajor">RowMajor</span>&gt;&gt;</td></tr>
<tr><th id="522">522</th><td>          <dfn class="local col6 typedef" id="86MatrixMap" title='MatrixMap' data-type='Eigen::Map&lt;Eigen::Matrix&lt;T, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt; &gt;' data-ref="86MatrixMap">MatrixMap</dfn>;</td></tr>
<tr><th id="523">523</th><td>      <b>typedef</b> <span class="namespace">Eigen::</span><span class='type' title='Eigen::Map' data-ref="Eigen::Map">Map</span>&lt;<em>const</em> <span class="namespace">Eigen::</span><span class='type' title='Eigen::Matrix' data-ref="Eigen::Matrix">Matrix</span>&lt;T, <span class="namespace">Eigen::</span><span class='ref' title='Eigen::Dynamic' data-ref="Eigen::Dynamic">Dynamic</span>, <span class="namespace">Eigen::</span><span class='ref' title='Eigen::Dynamic' data-ref="Eigen::Dynamic">Dynamic</span>,</td></tr>
<tr><th id="524">524</th><td>                                             <span class="namespace">Eigen::</span><span class='enum' title='Eigen::StorageOptions::RowMajor' data-ref="Eigen::StorageOptions::RowMajor">RowMajor</span>&gt;&gt;</td></tr>
<tr><th id="525">525</th><td>          <dfn class="local col7 typedef" id="87ConstMatrixMap" title='ConstMatrixMap' data-type='Eigen::Map&lt;const Eigen::Matrix&lt;T, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt; &gt;' data-ref="87ConstMatrixMap">ConstMatrixMap</dfn>;</td></tr>
<tr><th id="526">526</th><td></td></tr>
<tr><th id="527">527</th><td>      <b>for</b> (<em>int</em> <dfn class="local col8 decl" id="88image_id" title='image_id' data-type='int' data-ref="88image_id">image_id</dfn> = <var>0</var>; <a class="local col8 ref" href="#88image_id" title='image_id' data-ref="88image_id">image_id</a> &lt; <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::batch_size" title='tensorflow::ConvBackpropDimensions::batch_size' data-ref="tensorflow::ConvBackpropDimensions::batch_size">batch_size</a>;</td></tr>
<tr><th id="528">528</th><td>           <a class="local col8 ref" href="#88image_id" title='image_id' data-ref="88image_id">image_id</a> += <a class="local col0 ref" href="#70shard_size" title='shard_size' data-ref="70shard_size">shard_size</a>) {</td></tr>
<tr><th id="529">529</th><td>        <em>const</em> <em>int</em> <dfn class="local col9 decl" id="89shard_limit" title='shard_limit' data-type='const int' data-ref="89shard_limit">shard_limit</dfn> =</td></tr>
<tr><th id="530">530</th><td>            <span class="namespace">std::</span><a class="ref" href="../../../../include/c++/5/bits/algorithmfwd.h.html#_ZSt3minRKT_S1_" title='std::min' data-ref="_ZSt3minRKT_S1_">min</a>(<b>static_cast</b>&lt;<em>int</em>&gt;(<a class="local col0 ref" href="#70shard_size" title='shard_size' data-ref="70shard_size">shard_size</a>),</td></tr>
<tr><th id="531">531</th><td>                     <b>static_cast</b>&lt;<em>int</em>&gt;(<a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::batch_size" title='tensorflow::ConvBackpropDimensions::batch_size' data-ref="tensorflow::ConvBackpropDimensions::batch_size">batch_size</a>) - <a class="local col8 ref" href="#88image_id" title='image_id' data-ref="88image_id">image_id</a>);</td></tr>
<tr><th id="532">532</th><td></td></tr>
<tr><th id="533">533</th><td>        <em>auto</em> <dfn class="local col0 decl" id="90shard" title='shard' data-type='auto' data-ref="90shard">shard</dfn> = [&amp;dims, &amp;pad_top, &amp;pad_left, &amp;pad_bottom, &amp;pad_right,</td></tr>
<tr><th id="534">534</th><td>                      &amp;output_image_size, &amp;filter_total_size,</td></tr>
<tr><th id="535">535</th><td>                      &amp;input_backprop_data, &amp;col_buffer_data,</td></tr>
<tr><th id="536">536</th><td>                      &amp;out_backprop_data, &amp;filter_data, &amp;input_offset,</td></tr>
<tr><th id="537">537</th><td>                      &amp;output_offset, &amp;size_C](<a class="typedef" href="../platform/default/integral_types.h.html#tensorflow::int64" title='tensorflow::int64' data-type='long long' data-ref="tensorflow::int64">int64</a> <dfn class="local col1 decl" id="91start" title='start' data-type='int64' data-ref="91start">start</dfn>, <a class="typedef" href="../platform/default/integral_types.h.html#tensorflow::int64" title='tensorflow::int64' data-type='long long' data-ref="tensorflow::int64">int64</a> <dfn class="local col2 decl" id="92limit" title='limit' data-type='int64' data-ref="92limit">limit</dfn>) {</td></tr>
<tr><th id="538">538</th><td>          <b>for</b> (<em>int</em> <dfn class="local col3 decl" id="93shard_id" title='shard_id' data-type='int' data-ref="93shard_id">shard_id</dfn> = <a class="local col1 ref" href="#91start" title='start' data-ref="91start">start</a>; <a class="local col3 ref" href="#93shard_id" title='shard_id' data-ref="93shard_id">shard_id</a> &lt; <a class="local col2 ref" href="#92limit" title='limit' data-ref="92limit">limit</a>; ++<a class="local col3 ref" href="#93shard_id" title='shard_id' data-ref="93shard_id">shard_id</a>) {</td></tr>
<tr><th id="539">539</th><td>            T* <dfn class="local col4 decl" id="94im2col_buf" title='im2col_buf' data-type='T *' data-ref="94im2col_buf">im2col_buf</dfn> = <a class="local col5 ref" href="#75col_buffer_data" title='col_buffer_data' data-ref="75col_buffer_data">col_buffer_data</a> + <a class="local col3 ref" href="#93shard_id" title='shard_id' data-ref="93shard_id">shard_id</a> * <a class="local col4 ref" href="#64size_C" title='size_C' data-ref="64size_C">size_C</a>;</td></tr>
<tr><th id="540">540</th><td>            T* <dfn class="local col5 decl" id="95input_data" title='input_data' data-type='T *' data-ref="95input_data">input_data</dfn> = <a class="local col8 ref" href="#78input_backprop_data" title='input_backprop_data' data-ref="78input_backprop_data">input_backprop_data</a> + <a class="local col3 ref" href="#93shard_id" title='shard_id' data-ref="93shard_id">shard_id</a> * <a class="local col2 ref" href="#72input_offset" title='input_offset' data-ref="72input_offset">input_offset</a>;</td></tr>
<tr><th id="541">541</th><td>            <em>const</em> T* <dfn class="local col6 decl" id="96out_data" title='out_data' data-type='const T *' data-ref="96out_data">out_data</dfn> = <a class="local col6 ref" href="#76out_backprop_data" title='out_backprop_data' data-ref="76out_backprop_data">out_backprop_data</a> + <a class="local col3 ref" href="#93shard_id" title='shard_id' data-ref="93shard_id">shard_id</a> * <a class="local col3 ref" href="#73output_offset" title='output_offset' data-ref="73output_offset">output_offset</a>;</td></tr>
<tr><th id="542">542</th><td></td></tr>
<tr><th id="543">543</th><td>            <i>// Compute gradient into 'im2col_buf'.</i></td></tr>
<tr><th id="544">544</th><td>            <a class="local col6 typedef" href="#86MatrixMap" title='MatrixMap' data-type='Eigen::Map&lt;Eigen::Matrix&lt;T, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt; &gt;' data-ref="86MatrixMap">MatrixMap</a> <dfn class="local col7 decl" id="97C" title='C' data-type='MatrixMap' data-ref="97C">C</dfn>(<a class="local col4 ref" href="#94im2col_buf" title='im2col_buf' data-ref="94im2col_buf">im2col_buf</a>, <a class="local col8 ref" href="#58output_image_size" title='output_image_size' data-ref="58output_image_size">output_image_size</a>, <a class="local col7 ref" href="#57filter_total_size" title='filter_total_size' data-ref="57filter_total_size">filter_total_size</a>);</td></tr>
<tr><th id="545">545</th><td></td></tr>
<tr><th id="546">546</th><td>            <a class="local col7 typedef" href="#87ConstMatrixMap" title='ConstMatrixMap' data-type='Eigen::Map&lt;const Eigen::Matrix&lt;T, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt; &gt;' data-ref="87ConstMatrixMap">ConstMatrixMap</a> <dfn class="local col8 decl" id="98A" title='A' data-type='ConstMatrixMap' data-ref="98A">A</dfn>(<a class="local col6 ref" href="#96out_data" title='out_data' data-ref="96out_data">out_data</a>, <a class="local col8 ref" href="#58output_image_size" title='output_image_size' data-ref="58output_image_size">output_image_size</a>, <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::out_depth" title='tensorflow::ConvBackpropDimensions::out_depth' data-ref="tensorflow::ConvBackpropDimensions::out_depth">out_depth</a>);</td></tr>
<tr><th id="547">547</th><td>            <a class="local col7 typedef" href="#87ConstMatrixMap" title='ConstMatrixMap' data-type='Eigen::Map&lt;const Eigen::Matrix&lt;T, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt; &gt;' data-ref="87ConstMatrixMap">ConstMatrixMap</a> <dfn class="local col9 decl" id="99B" title='B' data-type='ConstMatrixMap' data-ref="99B">B</dfn>(<a class="local col4 ref" href="#74filter_data" title='filter_data' data-ref="74filter_data">filter_data</a>, <a class="local col7 ref" href="#57filter_total_size" title='filter_total_size' data-ref="57filter_total_size">filter_total_size</a>, <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::out_depth" title='tensorflow::ConvBackpropDimensions::out_depth' data-ref="tensorflow::ConvBackpropDimensions::out_depth">out_depth</a>);</td></tr>
<tr><th id="548">548</th><td></td></tr>
<tr><th id="549">549</th><td>            <a class="local col7 ref" href="#97C" title='C' data-ref="97C">C</a>.noalias() = <a class="local col8 ref" href="#98A" title='A' data-ref="98A">A</a> * <a class="local col9 ref" href="#99B" title='B' data-ref="99B">B</a>.transpose();</td></tr>
<tr><th id="550">550</th><td></td></tr>
<tr><th id="551">551</th><td>            Col2im&lt;T&gt;(<a class="local col4 ref" href="#94im2col_buf" title='im2col_buf' data-ref="94im2col_buf">im2col_buf</a>, <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::in_depth" title='tensorflow::ConvBackpropDimensions::in_depth' data-ref="tensorflow::ConvBackpropDimensions::in_depth">in_depth</a>,</td></tr>
<tr><th id="552">552</th><td>                      <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a><a class="ref" href="../lib/gtl/inlined_vector.h.html#_ZN10tensorflow3gtl13InlinedVectorixEm" title='tensorflow::gtl::InlinedVector::operator[]' data-ref="_ZN10tensorflow3gtl13InlinedVectorixEm">[<var>0</var>]</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::input_size" title='tensorflow::ConvBackpropSpatialDimension::input_size' data-ref="tensorflow::ConvBackpropSpatialDimension::input_size">input_size</a>,</td></tr>
<tr><th id="553">553</th><td>                      <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a><a class="ref" href="../lib/gtl/inlined_vector.h.html#_ZN10tensorflow3gtl13InlinedVectorixEm" title='tensorflow::gtl::InlinedVector::operator[]' data-ref="_ZN10tensorflow3gtl13InlinedVectorixEm">[<var>1</var>]</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::input_size" title='tensorflow::ConvBackpropSpatialDimension::input_size' data-ref="tensorflow::ConvBackpropSpatialDimension::input_size">input_size</a>,</td></tr>
<tr><th id="554">554</th><td>                      <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a><a class="ref" href="../lib/gtl/inlined_vector.h.html#_ZN10tensorflow3gtl13InlinedVectorixEm" title='tensorflow::gtl::InlinedVector::operator[]' data-ref="_ZN10tensorflow3gtl13InlinedVectorixEm">[<var>0</var>]</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::filter_size" title='tensorflow::ConvBackpropSpatialDimension::filter_size' data-ref="tensorflow::ConvBackpropSpatialDimension::filter_size">filter_size</a>,</td></tr>
<tr><th id="555">555</th><td>                      <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a><a class="ref" href="../lib/gtl/inlined_vector.h.html#_ZN10tensorflow3gtl13InlinedVectorixEm" title='tensorflow::gtl::InlinedVector::operator[]' data-ref="_ZN10tensorflow3gtl13InlinedVectorixEm">[<var>1</var>]</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::filter_size" title='tensorflow::ConvBackpropSpatialDimension::filter_size' data-ref="tensorflow::ConvBackpropSpatialDimension::filter_size">filter_size</a>, <a class="local col3 ref" href="#53pad_top" title='pad_top' data-ref="53pad_top">pad_top</a>, <a class="local col5 ref" href="#55pad_left" title='pad_left' data-ref="55pad_left">pad_left</a>,</td></tr>
<tr><th id="556">556</th><td>                      <a class="local col4 ref" href="#54pad_bottom" title='pad_bottom' data-ref="54pad_bottom">pad_bottom</a>, <a class="local col6 ref" href="#56pad_right" title='pad_right' data-ref="56pad_right">pad_right</a>, <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a><a class="ref" href="../lib/gtl/inlined_vector.h.html#_ZN10tensorflow3gtl13InlinedVectorixEm" title='tensorflow::gtl::InlinedVector::operator[]' data-ref="_ZN10tensorflow3gtl13InlinedVectorixEm">[<var>0</var>]</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::stride" title='tensorflow::ConvBackpropSpatialDimension::stride' data-ref="tensorflow::ConvBackpropSpatialDimension::stride">stride</a>,</td></tr>
<tr><th id="557">557</th><td>                      <a class="local col1 ref" href="#51dims" title='dims' data-ref="51dims">dims</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropDimensions::spatial_dims" title='tensorflow::ConvBackpropDimensions::spatial_dims' data-ref="tensorflow::ConvBackpropDimensions::spatial_dims">spatial_dims</a><a class="ref" href="../lib/gtl/inlined_vector.h.html#_ZN10tensorflow3gtl13InlinedVectorixEm" title='tensorflow::gtl::InlinedVector::operator[]' data-ref="_ZN10tensorflow3gtl13InlinedVectorixEm">[<var>1</var>]</a>.<a class="ref" href="conv_grad_ops.h.html#tensorflow::ConvBackpropSpatialDimension::stride" title='tensorflow::ConvBackpropSpatialDimension::stride' data-ref="tensorflow::ConvBackpropSpatialDimension::stride">stride</a>, <a class="local col5 ref" href="#95input_data" title='input_data' data-ref="95input_data">input_data</a>);</td></tr>
<tr><th id="558">558</th><td>          }</td></tr>
<tr><th id="559">559</th><td>        };</td></tr>
<tr><th id="560">560</th><td>        Shard(<a class="local col6 ref" href="#66worker_threads" title='worker_threads' data-ref="66worker_threads">worker_threads</a>.<a class="ref" href="../framework/device_base.h.html#tensorflow::DeviceBase::CpuWorkerThreads::num_threads" title='tensorflow::DeviceBase::CpuWorkerThreads::num_threads' data-ref="tensorflow::DeviceBase::CpuWorkerThreads::num_threads">num_threads</a>, <a class="local col6 ref" href="#66worker_threads" title='worker_threads' data-ref="66worker_threads">worker_threads</a>.<a class="ref" href="../framework/device_base.h.html#tensorflow::DeviceBase::CpuWorkerThreads::workers" title='tensorflow::DeviceBase::CpuWorkerThreads::workers' data-ref="tensorflow::DeviceBase::CpuWorkerThreads::workers">workers</a>, <a class="local col9 ref" href="#89shard_limit" title='shard_limit' data-ref="89shard_limit">shard_limit</a>,</td></tr>
<tr><th id="561">561</th><td>              <a class="local col5 ref" href="#65work_unit_size" title='work_unit_size' data-ref="65work_unit_size">work_unit_size</a>, <a class="local col0 ref" href="#90shard" title='shard' data-ref="90shard">shard</a>);</td></tr>
<tr><th id="562">562</th><td></td></tr>
<tr><th id="563">563</th><td>        <a class="local col8 ref" href="#78input_backprop_data" title='input_backprop_data' data-ref="78input_backprop_data">input_backprop_data</a> += <a class="local col2 ref" href="#72input_offset" title='input_offset' data-ref="72input_offset">input_offset</a> * <a class="local col9 ref" href="#89shard_limit" title='shard_limit' data-ref="89shard_limit">shard_limit</a>;</td></tr>
<tr><th id="564">564</th><td>        <a class="local col6 ref" href="#76out_backprop_data" title='out_backprop_data' data-ref="76out_backprop_data">out_backprop_data</a> += <a class="local col3 ref" href="#73output_offset" title='output_offset' data-ref="73output_offset">output_offset</a> * <a class="local col9 ref" href="#89shard_limit" title='shard_limit' data-ref="89shard_limit">shard_limit</a>;</td></tr>
<tr><th id="565">565</th><td>      }</td></tr>
<tr><th id="566">566</th><td>    }</td></tr>
<tr><th id="567">567</th><td>  }</td></tr>
<tr><th id="568">568</th><td></td></tr>
<tr><th id="569">569</th><td> <b>private</b>:</td></tr>
<tr><th id="570">570</th><td>  <span class="namespace">std::</span><a class="type" href="../../../../include/c++/5/bits/stl_vector.h.html#std::vector" title='std::vector' data-ref="std::vector">vector</a>&lt;<a class="typedef" href="../platform/default/integral_types.h.html#tensorflow::int32" title='tensorflow::int32' data-type='int' data-ref="tensorflow::int32">int32</a>&gt; <dfn class="tu decl" id="tensorflow::Conv2DCustomBackpropInputOp::dilations_" title='tensorflow::Conv2DCustomBackpropInputOp::dilations_' data-type='std::vector&lt;int32&gt;' data-ref="tensorflow::Conv2DCustomBackpropInputOp::dilations_">dilations_</dfn>;</td></tr>
<tr><th id="571">571</th><td>  <span class="namespace">std::</span><a class="type" href="../../../../include/c++/5/bits/stl_vector.h.html#std::vector" title='std::vector' data-ref="std::vector">vector</a>&lt;<a class="typedef" href="../platform/default/integral_types.h.html#tensorflow::int32" title='tensorflow::int32' data-type='int' data-ref="tensorflow::int32">int32</a>&gt; <dfn class="tu decl" id="tensorflow::Conv2DCustomBackpropInputOp::strides_" title='tensorflow::Conv2DCustomBackpropInputOp::strides_' data-type='std::vector&lt;int32&gt;' data-ref="tensorflow::Conv2DCustomBackpropInputOp::strides_">strides_</dfn>;</td></tr>
<tr><th id="572">572</th><td>  <a class="type" href="../util/padding.h.html#tensorflow::Padding" title='tensorflow::Padding' data-ref="tensorflow::Padding">Padding</a> <dfn class="tu decl" id="tensorflow::Conv2DCustomBackpropInputOp::padding_" title='tensorflow::Conv2DCustomBackpropInputOp::padding_' data-type='tensorflow::Padding' data-ref="tensorflow::Conv2DCustomBackpropInputOp::padding_">padding_</dfn>;</td></tr>
<tr><th id="573">573</th><td>  <a class="type" href="../util/tensor_format.h.html#tensorflow::TensorFormat" title='tensorflow::TensorFormat' data-ref="tensorflow::TensorFormat">TensorFormat</a> <dfn class="tu decl" id="tensorflow::Conv2DCustomBackpropInputOp::data_format_" title='tensorflow::Conv2DCustomBackpropInputOp::data_format_' data-type='tensorflow::TensorFormat' data-ref="tensorflow::Conv2DCustomBackpropInputOp::data_format_">data_format_</dfn>;</td></tr>
<tr><th id="574">574</th><td></td></tr>
<tr><th id="575">575</th><td>  <a class="macro" href="../platform/macros.h.html#91" title="Conv2DCustomBackpropInputOp(const Conv2DCustomBackpropInputOp&amp;) = delete; void operator=(const Conv2DCustomBackpropInputOp&amp;) = delete" data-ref="_M/TF_DISALLOW_COPY_AND_ASSIGN">TF_DISALLOW_COPY_AND_ASSIGN</a>(<dfn class="tu decl def" id="_ZN10tensorflow27Conv2DCustomBackpropInputOpC1ERKNS_27Conv2DCustomBackpropInputOpIT_T0_EE" title='tensorflow::Conv2DCustomBackpropInputOp::Conv2DCustomBackpropInputOp&lt;Device, T&gt;' data-type='void tensorflow::Conv2DCustomBackpropInputOp::Conv2DCustomBackpropInputOp&lt;Device, T&gt;(const Conv2DCustomBackpropInputOp&lt;Device, T&gt; &amp; )' data-ref="_ZN10tensorflow27Conv2DCustomBackpropInputOpC1ERKNS_27Conv2DCustomBackpropInputOpIT_T0_EE">Conv2DCustomBackpropInputOp</dfn>);</td></tr>
<tr><th id="576">576</th><td>};</td></tr>
<tr><th id="577">577</th><td></td></tr>
<tr><th id="578">578</th><td><u>#define <dfn class="macro" id="_M/REGISTER_CPU_KERNELS" data-ref="_M/REGISTER_CPU_KERNELS">REGISTER_CPU_KERNELS</dfn>(T)                                              \</u></td></tr>
<tr><th id="579">579</th><td><u>  REGISTER_KERNEL_BUILDER(                                                   \</u></td></tr>
<tr><th id="580">580</th><td><u>      <a class="type" href="../framework/op_kernel.h.html#tensorflow::register_kernel::Name" title='tensorflow::register_kernel::Name' data-ref="tensorflow::register_kernel::Name">Name</a><a class="ref" href="../framework/op_kernel.h.html#_ZN10tensorflow15register_kernel4NameC1EPKc" title='tensorflow::register_kernel::Name::Name' data-ref="_ZN10tensorflow15register_kernel4NameC1EPKc">(</a>"Conv2DBackpropInput").<a class="ref" href="../framework/kernel_def_builder.h.html#_ZN10tensorflow16KernelDefBuilder6DeviceEPKc" title='tensorflow::KernelDefBuilder::Device' data-ref="_ZN10tensorflow16KernelDefBuilder6DeviceEPKc">Device</a>(<a class="ref" href="../framework/types.h.html#tensorflow::DEVICE_CPU" title='tensorflow::DEVICE_CPU' data-ref="tensorflow::DEVICE_CPU">DEVICE_CPU</a>).<a class="ref" href="../framework/kernel_def_builder.h.html#_ZN10tensorflow16KernelDefBuilder14TypeConstraintEPKc" title='tensorflow::KernelDefBuilder::TypeConstraint' data-ref="_ZN10tensorflow16KernelDefBuilder14TypeConstraintEPKc">TypeConstraint</a>&lt;T&gt;("T"), \</u></td></tr>
<tr><th id="581">581</th><td><u>      <a class="type" href="#tensorflow::Conv2DCustomBackpropInputOp" title='tensorflow::Conv2DCustomBackpropInputOp' data-ref="tensorflow::Conv2DCustomBackpropInputOp">Conv2DCustomBackpropInputOp</a>&lt;<a class="typedef" href="#tensorflow::CPUDevice" title='tensorflow::CPUDevice' data-type='Eigen::ThreadPoolDevice' data-ref="tensorflow::CPUDevice">CPUDevice</a>, T&gt;);                            \</u></td></tr>
<tr><th id="582">582</th><td><u>  REGISTER_KERNEL_BUILDER(<a class="type" href="../framework/op_kernel.h.html#tensorflow::register_kernel::Name" title='tensorflow::register_kernel::Name' data-ref="tensorflow::register_kernel::Name">Name</a><a class="ref" href="../framework/op_kernel.h.html#_ZN10tensorflow15register_kernel4NameC1EPKc" title='tensorflow::register_kernel::Name::Name' data-ref="_ZN10tensorflow15register_kernel4NameC1EPKc">(</a>"Conv2DBackpropInput")                        \</u></td></tr>
<tr><th id="583">583</th><td><u>                              .<a class="ref" href="../framework/kernel_def_builder.h.html#_ZN10tensorflow16KernelDefBuilder6DeviceEPKc" title='tensorflow::KernelDefBuilder::Device' data-ref="_ZN10tensorflow16KernelDefBuilder6DeviceEPKc">Device</a>(<a class="ref" href="../framework/types.h.html#tensorflow::DEVICE_CPU" title='tensorflow::DEVICE_CPU' data-ref="tensorflow::DEVICE_CPU">DEVICE_CPU</a>)                            \</u></td></tr>
<tr><th id="584">584</th><td><u>                              .<a class="ref" href="../framework/kernel_def_builder.h.html#_ZN10tensorflow16KernelDefBuilder5LabelEPKc" title='tensorflow::KernelDefBuilder::Label' data-ref="_ZN10tensorflow16KernelDefBuilder5LabelEPKc">Label</a>("custom")                               \</u></td></tr>
<tr><th id="585">585</th><td><u>                              .<a class="ref" href="../framework/kernel_def_builder.h.html#_ZN10tensorflow16KernelDefBuilder14TypeConstraintEPKc" title='tensorflow::KernelDefBuilder::TypeConstraint' data-ref="_ZN10tensorflow16KernelDefBuilder14TypeConstraintEPKc">TypeConstraint</a>&lt;T&gt;("T"),                       \</u></td></tr>
<tr><th id="586">586</th><td><u>                          <a class="type" href="#tensorflow::Conv2DCustomBackpropInputOp" title='tensorflow::Conv2DCustomBackpropInputOp' data-ref="tensorflow::Conv2DCustomBackpropInputOp">Conv2DCustomBackpropInputOp</a>&lt;<a class="typedef" href="#tensorflow::CPUDevice" title='tensorflow::CPUDevice' data-type='Eigen::ThreadPoolDevice' data-ref="tensorflow::CPUDevice">CPUDevice</a>, T&gt;);        \</u></td></tr>
<tr><th id="587">587</th><td><u>  REGISTER_KERNEL_BUILDER(<a class="type" href="../framework/op_kernel.h.html#tensorflow::register_kernel::Name" title='tensorflow::register_kernel::Name' data-ref="tensorflow::register_kernel::Name">Name</a><a class="ref" href="../framework/op_kernel.h.html#_ZN10tensorflow15register_kernel4NameC1EPKc" title='tensorflow::register_kernel::Name::Name' data-ref="_ZN10tensorflow15register_kernel4NameC1EPKc">(</a>"Conv2DBackpropInput")                        \</u></td></tr>
<tr><th id="588">588</th><td><u>                              .<a class="ref" href="../framework/kernel_def_builder.h.html#_ZN10tensorflow16KernelDefBuilder6DeviceEPKc" title='tensorflow::KernelDefBuilder::Device' data-ref="_ZN10tensorflow16KernelDefBuilder6DeviceEPKc">Device</a>(<a class="ref" href="../framework/types.h.html#tensorflow::DEVICE_CPU" title='tensorflow::DEVICE_CPU' data-ref="tensorflow::DEVICE_CPU">DEVICE_CPU</a>)                            \</u></td></tr>
<tr><th id="589">589</th><td><u>                              .<a class="ref" href="../framework/kernel_def_builder.h.html#_ZN10tensorflow16KernelDefBuilder5LabelEPKc" title='tensorflow::KernelDefBuilder::Label' data-ref="_ZN10tensorflow16KernelDefBuilder5LabelEPKc">Label</a>("eigen_tensor")                         \</u></td></tr>
<tr><th id="590">590</th><td><u>                              .<a class="ref" href="../framework/kernel_def_builder.h.html#_ZN10tensorflow16KernelDefBuilder14TypeConstraintEPKc" title='tensorflow::KernelDefBuilder::TypeConstraint' data-ref="_ZN10tensorflow16KernelDefBuilder14TypeConstraintEPKc">TypeConstraint</a>&lt;T&gt;("T"),                       \</u></td></tr>
<tr><th id="591">591</th><td><u>                          <a class="type" href="#tensorflow::Conv2DFastBackpropInputOp" title='tensorflow::Conv2DFastBackpropInputOp' data-ref="tensorflow::Conv2DFastBackpropInputOp">Conv2DFastBackpropInputOp</a>&lt;<a class="typedef" href="#tensorflow::CPUDevice" title='tensorflow::CPUDevice' data-type='Eigen::ThreadPoolDevice' data-ref="tensorflow::CPUDevice">CPUDevice</a>, T&gt;);</u></td></tr>
<tr><th id="592">592</th><td></td></tr>
<tr><th id="593">593</th><td><a class="macro" href="../framework/register_types.h.html#87" title="constexpr bool should_register_0__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__0__object( should_register_0__flag ? ::tensorflow::register_kernel::Name(&quot;Conv2DBackpropInput&quot;).Device(DEVICE_CPU).TypeConstraint&lt;Eigen::half&gt;(&quot;T&quot;).Build() : nullptr, &quot;Conv2DCustomBackpropInputOp&lt;CPUDevice, Eigen::half&gt;&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new Conv2DCustomBackpropInputOp&lt;CPUDevice, Eigen::half&gt;(context); });; constexpr bool should_register_1__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__1__object( should_register_1__flag ? ::tensorflow::register_kernel::Name(&quot;Conv2DBackpropInput&quot;) .Device(DEVICE_CPU) .Label(&quot;custom&quot;) .TypeConstraint&lt;Eigen::half&gt;(&quot;T&quot;).Build() : nullptr, &quot;Conv2DCustomBackpropInputOp&lt;CPUDevice, Eigen::half&gt;&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new Conv2DCustomBackpropInputOp&lt;CPUDevice, Eigen::half&gt;(context); });; constexpr bool should_register_2__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__2__object( should_register_2__flag ? ::tensorflow::register_kernel::Name(&quot;Conv2DBackpropInput&quot;) .Device(DEVICE_CPU) .Label(&quot;eigen_tensor&quot;) .TypeConstraint&lt;Eigen::half&gt;(&quot;T&quot;).Build() : nullptr, &quot;Conv2DFastBackpropInputOp&lt;CPUDevice, Eigen::half&gt;&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new Conv2DFastBackpropInputOp&lt;CPUDevice, Eigen::half&gt;(context); });;" data-ref="_M/TF_CALL_half">TF_CALL_half</a>(REGISTER_CPU_KERNELS);</td></tr>
<tr><th id="594">594</th><td><a class="macro" href="../framework/register_types.h.html#62" title="constexpr bool should_register_6__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__6__object( should_register_6__flag ? ::tensorflow::register_kernel::Name(&quot;Conv2DBackpropInput&quot;).Device(DEVICE_CPU).TypeConstraint&lt;float&gt;(&quot;T&quot;).Build() : nullptr, &quot;Conv2DCustomBackpropInputOp&lt;CPUDevice, float&gt;&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new Conv2DCustomBackpropInputOp&lt;CPUDevice, float&gt;(context); });; constexpr bool should_register_7__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__7__object( should_register_7__flag ? ::tensorflow::register_kernel::Name(&quot;Conv2DBackpropInput&quot;) .Device(DEVICE_CPU) .Label(&quot;custom&quot;) .TypeConstraint&lt;float&gt;(&quot;T&quot;).Build() : nullptr, &quot;Conv2DCustomBackpropInputOp&lt;CPUDevice, float&gt;&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new Conv2DCustomBackpropInputOp&lt;CPUDevice, float&gt;(context); });; constexpr bool should_register_8__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__8__object( should_register_8__flag ? ::tensorflow::register_kernel::Name(&quot;Conv2DBackpropInput&quot;) .Device(DEVICE_CPU) .Label(&quot;eigen_tensor&quot;) .TypeConstraint&lt;float&gt;(&quot;T&quot;).Build() : nullptr, &quot;Conv2DFastBackpropInputOp&lt;CPUDevice, float&gt;&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new Conv2DFastBackpropInputOp&lt;CPUDevice, float&gt;(context); });;" data-ref="_M/TF_CALL_float">TF_CALL_float</a>(REGISTER_CPU_KERNELS);</td></tr>
<tr><th id="595">595</th><td><a class="macro" href="../framework/register_types.h.html#63" title="constexpr bool should_register_12__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__12__object( should_register_12__flag ? ::tensorflow::register_kernel::Name(&quot;Conv2DBackpropInput&quot;).Device(DEVICE_CPU).TypeConstraint&lt;double&gt;(&quot;T&quot;).Build() : nullptr, &quot;Conv2DCustomBackpropInputOp&lt;CPUDevice, double&gt;&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new Conv2DCustomBackpropInputOp&lt;CPUDevice, double&gt;(context); });; constexpr bool should_register_13__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__13__object( should_register_13__flag ? ::tensorflow::register_kernel::Name(&quot;Conv2DBackpropInput&quot;) .Device(DEVICE_CPU) .Label(&quot;custom&quot;) .TypeConstraint&lt;double&gt;(&quot;T&quot;).Build() : nullptr, &quot;Conv2DCustomBackpropInputOp&lt;CPUDevice, double&gt;&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new Conv2DCustomBackpropInputOp&lt;CPUDevice, double&gt;(context); });; constexpr bool should_register_14__flag = true; static ::tensorflow::kernel_factory::OpKernelRegistrar registrar__body__14__object( should_register_14__flag ? ::tensorflow::register_kernel::Name(&quot;Conv2DBackpropInput&quot;) .Device(DEVICE_CPU) .Label(&quot;eigen_tensor&quot;) .TypeConstraint&lt;double&gt;(&quot;T&quot;).Build() : nullptr, &quot;Conv2DFastBackpropInputOp&lt;CPUDevice, double&gt;&quot;, [](::tensorflow::OpKernelConstruction* context) -&gt; ::tensorflow::OpKernel* { return new Conv2DFastBackpropInputOp&lt;CPUDevice, double&gt;(context); });;" data-ref="_M/TF_CALL_double">TF_CALL_double</a>(REGISTER_CPU_KERNELS);</td></tr>
<tr><th id="596">596</th><td><u>#undef <a class="macro" href="#578" data-ref="_M/REGISTER_CPU_KERNELS">REGISTER_CPU_KERNELS</a></u></td></tr>
<tr><th id="597">597</th><td></td></tr>
<tr><th id="598">598</th><td><i>// GPU definitions.</i></td></tr>
<tr><th id="599">599</th><td><u>#<span data-ppcond="599">if</span> GOOGLE_CUDA</u></td></tr>
<tr><th id="600">600</th><td><i>// The slow version (but compiles for GPU)</i></td></tr>
<tr><th id="601">601</th><td><i></i></td></tr>
<tr><th id="602">602</th><td><i>// A dummy type to group forward backward data autotune results together.</i></td></tr>
<tr><th id="603">603</th><td><b>struct</b> ConvBackwardDataAutoTuneGroup {</td></tr>
<tr><th id="604">604</th><td>  <em>static</em> string name() { <b>return</b> <q>"ConvBwdData"</q>; }</td></tr>
<tr><th id="605">605</th><td>};</td></tr>
<tr><th id="606">606</th><td><b>typedef</b> AutoTuneSingleton&lt;ConvBackwardDataAutoTuneGroup, ConvParameters,</td></tr>
<tr><th id="607">607</th><td>                          perftools::gputools::dnn::AlgorithmConfig&gt;</td></tr>
<tr><th id="608">608</th><td>    AutoTuneConvBwdData;</td></tr>
<tr><th id="609">609</th><td></td></tr>
<tr><th id="610">610</th><td><i>// Backprop for input.</i></td></tr>
<tr><th id="611">611</th><td><b>template</b> &lt;<b>typename</b> Device, <b>class</b> T&gt;</td></tr>
<tr><th id="612">612</th><td><b>class</b> Conv2DSlowBackpropInputOp : <b>public</b> OpKernel {</td></tr>
<tr><th id="613">613</th><td> <b>public</b>:</td></tr>
<tr><th id="614">614</th><td>  <b>explicit</b> Conv2DSlowBackpropInputOp(OpKernelConstruction* context)</td></tr>
<tr><th id="615">615</th><td>      : OpKernel(context) {</td></tr>
<tr><th id="616">616</th><td>    string data_format;</td></tr>
<tr><th id="617">617</th><td>    OP_REQUIRES_OK(context, context-&gt;GetAttr(<q>"data_format"</q>, &amp;data_format));</td></tr>
<tr><th id="618">618</th><td>    OP_REQUIRES(context, FormatFromString(data_format, &amp;data_format_),</td></tr>
<tr><th id="619">619</th><td>                errors::InvalidArgument(<q>"Invalid data format"</q>));</td></tr>
<tr><th id="620">620</th><td>    OP_REQUIRES_OK(context, context-&gt;GetAttr(<q>"strides"</q>, &amp;strides_));</td></tr>
<tr><th id="621">621</th><td>    OP_REQUIRES(context, strides_.size() == <var>4</var>,</td></tr>
<tr><th id="622">622</th><td>                errors::InvalidArgument(<q>"Sliding window strides field must "</q></td></tr>
<tr><th id="623">623</th><td>                                        <q>"specify 4 dimensions"</q>));</td></tr>
<tr><th id="624">624</th><td>    <em>int</em> stride_n = GetTensorDim(strides_, data_format_, <kbd>'N'</kbd>);</td></tr>
<tr><th id="625">625</th><td>    <em>int</em> stride_c = GetTensorDim(strides_, data_format_, <kbd>'C'</kbd>);</td></tr>
<tr><th id="626">626</th><td>    <em>int</em> stride_h = GetTensorDim(strides_, data_format_, <kbd>'H'</kbd>);</td></tr>
<tr><th id="627">627</th><td>    <em>int</em> stride_w = GetTensorDim(strides_, data_format_, <kbd>'W'</kbd>);</td></tr>
<tr><th id="628">628</th><td>    OP_REQUIRES(</td></tr>
<tr><th id="629">629</th><td>        context, (stride_n == <var>1</var> &amp;&amp; stride_c == <var>1</var>),</td></tr>
<tr><th id="630">630</th><td>        errors::InvalidArgument(<q>"Current implementation does not yet support "</q></td></tr>
<tr><th id="631">631</th><td>                                <q>"strides in the batch and depth dimensions."</q>));</td></tr>
<tr><th id="632">632</th><td>    OP_REQUIRES(context, stride_h &gt; <var>0</var> &amp;&amp; stride_w &gt; <var>0</var>,</td></tr>
<tr><th id="633">633</th><td>                errors::InvalidArgument(</td></tr>
<tr><th id="634">634</th><td>                    <q>"Row and column strides should be larger than 0."</q>));</td></tr>
<tr><th id="635">635</th><td>    OP_REQUIRES_OK(context, context-&gt;GetAttr(<q>"dilations"</q>, &amp;dilations_));</td></tr>
<tr><th id="636">636</th><td>    OP_REQUIRES(context, dilations_.size() == <var>4</var>,</td></tr>
<tr><th id="637">637</th><td>                errors::InvalidArgument(<q>"Sliding window dilations field must "</q></td></tr>
<tr><th id="638">638</th><td>                                        <q>"specify 4 dimensions"</q>));</td></tr>
<tr><th id="639">639</th><td>    <em>int</em> dilation_n = GetTensorDim(dilations_, data_format_, <kbd>'N'</kbd>);</td></tr>
<tr><th id="640">640</th><td>    <em>int</em> dilation_c = GetTensorDim(dilations_, data_format_, <kbd>'C'</kbd>);</td></tr>
<tr><th id="641">641</th><td>    <em>int</em> dilation_h = GetTensorDim(dilations_, data_format_, <kbd>'H'</kbd>);</td></tr>
<tr><th id="642">642</th><td>    <em>int</em> dilation_w = GetTensorDim(dilations_, data_format_, <kbd>'W'</kbd>);</td></tr>
<tr><th id="643">643</th><td>    OP_REQUIRES(context, (dilation_n == <var>1</var> &amp;&amp; dilation_c == <var>1</var>),</td></tr>
<tr><th id="644">644</th><td>                errors::InvalidArgument(</td></tr>
<tr><th id="645">645</th><td>                    <q>"Current implementation does not yet support "</q></td></tr>
<tr><th id="646">646</th><td>                    <q>"dilations in the batch and depth dimensions."</q>));</td></tr>
<tr><th id="647">647</th><td>    OP_REQUIRES(</td></tr>
<tr><th id="648">648</th><td>        context, dilation_h &gt; <var>0</var> &amp;&amp; dilation_w &gt; <var>0</var>,</td></tr>
<tr><th id="649">649</th><td>        errors::InvalidArgument(<q>"Dilated rates should be larger than 0."</q>));</td></tr>
<tr><th id="650">650</th><td>    OP_REQUIRES_OK(context, context-&gt;GetAttr(<q>"use_cudnn_on_gpu"</q>, &amp;use_cudnn_));</td></tr>
<tr><th id="651">651</th><td>    use_cudnn_ &amp;= CanUseCudnn();</td></tr>
<tr><th id="652">652</th><td>    cudnn_use_autotune_ = CudnnUseAutotune();</td></tr>
<tr><th id="653">653</th><td>    OP_REQUIRES_OK(context, context-&gt;GetAttr(<q>"padding"</q>, &amp;padding_));</td></tr>
<tr><th id="654">654</th><td>  }</td></tr>
<tr><th id="655">655</th><td></td></tr>
<tr><th id="656">656</th><td>  <em>void</em> Compute(OpKernelContext* context) override {</td></tr>
<tr><th id="657">657</th><td>    <em>const</em> Tensor&amp; input_sizes = context-&gt;input(<var>0</var>);</td></tr>
<tr><th id="658">658</th><td>    <em>const</em> Tensor&amp; filter = context-&gt;input(<var>1</var>);</td></tr>
<tr><th id="659">659</th><td>    <em>const</em> Tensor&amp; out_backprop = context-&gt;input(<var>2</var>);</td></tr>
<tr><th id="660">660</th><td>    OP_REQUIRES(</td></tr>
<tr><th id="661">661</th><td>        context, TensorShapeUtils::IsVector(input_sizes.shape()),</td></tr>
<tr><th id="662">662</th><td>        errors::InvalidArgument(</td></tr>
<tr><th id="663">663</th><td>            <q>"Conv2DBackpropInput: input_sizes input must be 1-dim, not "</q>,</td></tr>
<tr><th id="664">664</th><td>            input_sizes.dims()));</td></tr>
<tr><th id="665">665</th><td>    TensorShape input_shape;</td></tr>
<tr><th id="666">666</th><td>    OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(</td></tr>
<tr><th id="667">667</th><td>                                input_sizes.vec&lt;int32&gt;(), &amp;input_shape));</td></tr>
<tr><th id="668">668</th><td></td></tr>
<tr><th id="669">669</th><td>    Tensor* in_backprop = <b>nullptr</b>;</td></tr>
<tr><th id="670">670</th><td>    OP_REQUIRES_OK(context,</td></tr>
<tr><th id="671">671</th><td>                   context-&gt;allocate_output(<var>0</var>, input_shape, &amp;in_backprop));</td></tr>
<tr><th id="672">672</th><td></td></tr>
<tr><th id="673">673</th><td>    <i>// If there is nothing to compute, return.</i></td></tr>
<tr><th id="674">674</th><td>    <b>if</b> (input_shape.num_elements() == <var>0</var>) {</td></tr>
<tr><th id="675">675</th><td>      <b>return</b>;</td></tr>
<tr><th id="676">676</th><td>    }</td></tr>
<tr><th id="677">677</th><td></td></tr>
<tr><th id="678">678</th><td>    <i>// For now we take the stride from the second and third dimensions only (we</i></td></tr>
<tr><th id="679">679</th><td><i>    // do not support striding on the batch or depth dimension).</i></td></tr>
<tr><th id="680">680</th><td>    <em>const</em> <em>int</em> stride_rows = GetTensorDim(strides_, data_format_, <kbd>'H'</kbd>);</td></tr>
<tr><th id="681">681</th><td>    <em>const</em> <em>int</em> stride_cols = GetTensorDim(strides_, data_format_, <kbd>'W'</kbd>);</td></tr>
<tr><th id="682">682</th><td>    <em>const</em> <em>int</em> dilation_rows = GetTensorDim(dilations_, data_format_, <kbd>'H'</kbd>);</td></tr>
<tr><th id="683">683</th><td>    <em>const</em> <em>int</em> dilation_cols = GetTensorDim(dilations_, data_format_, <kbd>'W'</kbd>);</td></tr>
<tr><th id="684">684</th><td></td></tr>
<tr><th id="685">685</th><td>    launcher_(context, use_cudnn_, cudnn_use_autotune_, out_backprop, filter,</td></tr>
<tr><th id="686">686</th><td>              dilation_rows, dilation_cols, stride_rows, stride_cols, padding_,</td></tr>
<tr><th id="687">687</th><td>              in_backprop, data_format_);</td></tr>
<tr><th id="688">688</th><td>  }</td></tr>
<tr><th id="689">689</th><td></td></tr>
<tr><th id="690">690</th><td> <b>private</b>:</td></tr>
<tr><th id="691">691</th><td>  std::vector&lt;int32&gt; dilations_;</td></tr>
<tr><th id="692">692</th><td>  std::vector&lt;int32&gt; strides_;</td></tr>
<tr><th id="693">693</th><td>  Padding padding_;</td></tr>
<tr><th id="694">694</th><td>  <em>bool</em> use_cudnn_;</td></tr>
<tr><th id="695">695</th><td>  TensorFormat data_format_;</td></tr>
<tr><th id="696">696</th><td>  LaunchConv2DBackpropInputOp&lt;Device, T&gt; launcher_;</td></tr>
<tr><th id="697">697</th><td>  <em>bool</em> cudnn_use_autotune_;</td></tr>
<tr><th id="698">698</th><td></td></tr>
<tr><th id="699">699</th><td>  TF_DISALLOW_COPY_AND_ASSIGN(Conv2DSlowBackpropInputOp);</td></tr>
<tr><th id="700">700</th><td>};</td></tr>
<tr><th id="701">701</th><td></td></tr>
<tr><th id="702">702</th><td><b>template</b> &lt;<b>typename</b> T&gt;</td></tr>
<tr><th id="703">703</th><td><em>void</em> LaunchConv2DBackpropInputOp&lt;GPUDevice, T&gt;::<b>operator</b>()(</td></tr>
<tr><th id="704">704</th><td>    OpKernelContext* ctx, <em>bool</em> use_cudnn, <em>bool</em> cudnn_use_autotune,</td></tr>
<tr><th id="705">705</th><td>    <em>const</em> Tensor&amp; out_backprop, <em>const</em> Tensor&amp; filter, <em>int</em> row_dilation,</td></tr>
<tr><th id="706">706</th><td>    <em>int</em> col_dilation, <em>int</em> row_stride, <em>int</em> col_stride, <em>const</em> Padding&amp; padding,</td></tr>
<tr><th id="707">707</th><td>    Tensor* in_backprop, TensorFormat data_format) {</td></tr>
<tr><th id="708">708</th><td>  <b>using</b> perftools::gputools::dnn::AlgorithmConfig;</td></tr>
<tr><th id="709">709</th><td>  <b>using</b> perftools::gputools::dnn::AlgorithmDesc;</td></tr>
<tr><th id="710">710</th><td>  <b>using</b> perftools::gputools::dnn::ProfileResult;</td></tr>
<tr><th id="711">711</th><td></td></tr>
<tr><th id="712">712</th><td>  std::vector&lt;int32&gt; strides(<var>4</var>, <var>1</var>);</td></tr>
<tr><th id="713">713</th><td>  std::vector&lt;int32&gt; dilations(<var>4</var>, <var>1</var>);</td></tr>
<tr><th id="714">714</th><td>  <em>auto</em> input_h = GetTensorDimIndex(data_format, <kbd>'H'</kbd>);</td></tr>
<tr><th id="715">715</th><td>  <em>auto</em> input_w = GetTensorDimIndex(data_format, <kbd>'W'</kbd>);</td></tr>
<tr><th id="716">716</th><td>  strides[input_h] = row_stride;</td></tr>
<tr><th id="717">717</th><td>  strides[input_w] = col_stride;</td></tr>
<tr><th id="718">718</th><td>  dilations[input_h] = row_dilation;</td></tr>
<tr><th id="719">719</th><td>  dilations[input_w] = col_dilation;</td></tr>
<tr><th id="720">720</th><td>  TensorShape input_shape = in_backprop-&gt;shape();</td></tr>
<tr><th id="721">721</th><td></td></tr>
<tr><th id="722">722</th><td>  <em>const</em> TensorShape&amp; filter_shape = filter.shape();</td></tr>
<tr><th id="723">723</th><td>  ConvBackpropDimensions dims;</td></tr>
<tr><th id="724">724</th><td>  OP_REQUIRES_OK(ctx, ConvBackpropComputeDimensionsV2(</td></tr>
<tr><th id="725">725</th><td>                          <q>"Conv2DSlowBackpropInput"</q>, <i>/*num_spatial_dims=*/</i><var>2</var>,</td></tr>
<tr><th id="726">726</th><td>                          input_shape, filter_shape, out_backprop.shape(),</td></tr>
<tr><th id="727">727</th><td>                          dilations, strides, padding, data_format, &amp;dims));</td></tr>
<tr><th id="728">728</th><td></td></tr>
<tr><th id="729">729</th><td>  <i>// TODO(yangzihao): The padding computations should be done in</i></td></tr>
<tr><th id="730">730</th><td><i>  // GetWindowedOutputSize() functions.</i></td></tr>
<tr><th id="731">731</th><td>  <em>const</em> <em>int</em> padding_rows =</td></tr>
<tr><th id="732">732</th><td>      (padding == VALID)</td></tr>
<tr><th id="733">733</th><td>          ? <var>0</var></td></tr>
<tr><th id="734">734</th><td>          : std::max&lt;<em>int</em>&gt;(<var>0</var>, (dims.spatial_dims[<var>0</var>].output_size - <var>1</var>) *</td></tr>
<tr><th id="735">735</th><td>                                     dims.spatial_dims[<var>0</var>].stride +</td></tr>
<tr><th id="736">736</th><td>                                 (dims.spatial_dims[<var>0</var>].filter_size - <var>1</var>) *</td></tr>
<tr><th id="737">737</th><td>                                     dims.spatial_dims[<var>0</var>].dilation +</td></tr>
<tr><th id="738">738</th><td>                                 <var>1</var> - dims.spatial_dims[<var>0</var>].input_size);</td></tr>
<tr><th id="739">739</th><td>  <em>const</em> <em>int</em> padding_cols =</td></tr>
<tr><th id="740">740</th><td>      (padding == VALID)</td></tr>
<tr><th id="741">741</th><td>          ? <var>0</var></td></tr>
<tr><th id="742">742</th><td>          : std::max&lt;<em>int</em>&gt;(<var>0</var>, (dims.spatial_dims[<var>1</var>].output_size - <var>1</var>) *</td></tr>
<tr><th id="743">743</th><td>                                     dims.spatial_dims[<var>1</var>].stride +</td></tr>
<tr><th id="744">744</th><td>                                 (dims.spatial_dims[<var>1</var>].filter_size - <var>1</var>) *</td></tr>
<tr><th id="745">745</th><td>                                     dims.spatial_dims[<var>1</var>].dilation +</td></tr>
<tr><th id="746">746</th><td>                                 <var>1</var> - dims.spatial_dims[<var>1</var>].input_size);</td></tr>
<tr><th id="747">747</th><td></td></tr>
<tr><th id="748">748</th><td>  <i>// TODO(keveman): cuDNN only supports equal padding on both sides, so only</i></td></tr>
<tr><th id="749">749</th><td><i>  // calling it when that is true. Remove this check when (if?) cuDNN starts</i></td></tr>
<tr><th id="750">750</th><td><i>  // supporting different padding.</i></td></tr>
<tr><th id="751">751</th><td>  <em>bool</em> rows_odd = (padding_rows % <var>2</var> != <var>0</var>);</td></tr>
<tr><th id="752">752</th><td>  <em>bool</em> cols_odd = (padding_cols % <var>2</var> != <var>0</var>);</td></tr>
<tr><th id="753">753</th><td></td></tr>
<tr><th id="754">754</th><td>  <em>auto</em>* stream = ctx-&gt;op_device_context()-&gt;stream();</td></tr>
<tr><th id="755">755</th><td>  OP_REQUIRES(ctx, stream, errors::Internal(<q>"No GPU stream available."</q>));</td></tr>
<tr><th id="756">756</th><td></td></tr>
<tr><th id="757">757</th><td>  <b>if</b> (!use_cudnn) {</td></tr>
<tr><th id="758">758</th><td>    ctx-&gt;SetStatus(errors::Unimplemented(</td></tr>
<tr><th id="759">759</th><td>        <q>"Conv2DBackpropInput for GPU is not currently supported "</q></td></tr>
<tr><th id="760">760</th><td>        <q>"without cudnn"</q>));</td></tr>
<tr><th id="761">761</th><td>    <b>return</b>;</td></tr>
<tr><th id="762">762</th><td>  }</td></tr>
<tr><th id="763">763</th><td></td></tr>
<tr><th id="764">764</th><td>  <b>if</b> (dims.spatial_dims[<var>0</var>].filter_size == <var>1</var> &amp;&amp;</td></tr>
<tr><th id="765">765</th><td>      dims.spatial_dims[<var>1</var>].filter_size == <var>1</var> &amp;&amp;</td></tr>
<tr><th id="766">766</th><td>      dims.spatial_dims[<var>0</var>].stride == <var>1</var> &amp;&amp; dims.spatial_dims[<var>1</var>].stride == <var>1</var> &amp;&amp;</td></tr>
<tr><th id="767">767</th><td>      data_format == FORMAT_NHWC) {</td></tr>
<tr><th id="768">768</th><td>    <i>// 1x1 filter, so call cublas directly.</i></td></tr>
<tr><th id="769">769</th><td>    <em>const</em> uint64 m = dims.batch_size * dims.spatial_dims[<var>0</var>].input_size *</td></tr>
<tr><th id="770">770</th><td>                     dims.spatial_dims[<var>1</var>].input_size;</td></tr>
<tr><th id="771">771</th><td>    <em>const</em> uint64 k = dims.out_depth;</td></tr>
<tr><th id="772">772</th><td>    <em>const</em> uint64 n = dims.in_depth;</td></tr>
<tr><th id="773">773</th><td></td></tr>
<tr><th id="774">774</th><td>    <em>auto</em> a_ptr = AsDeviceMemory(out_backprop.<b>template</b> flat&lt;T&gt;().data(),</td></tr>
<tr><th id="775">775</th><td>                                out_backprop.<b>template</b> flat&lt;T&gt;().size());</td></tr>
<tr><th id="776">776</th><td>    <em>auto</em> b_ptr = AsDeviceMemory(filter.<b>template</b> flat&lt;T&gt;().data(),</td></tr>
<tr><th id="777">777</th><td>                                filter.<b>template</b> flat&lt;T&gt;().size());</td></tr>
<tr><th id="778">778</th><td>    <em>auto</em> c_ptr = AsDeviceMemory(in_backprop-&gt;<b>template</b> flat&lt;T&gt;().data(),</td></tr>
<tr><th id="779">779</th><td>                                in_backprop-&gt;<b>template</b> flat&lt;T&gt;().size());</td></tr>
<tr><th id="780">780</th><td></td></tr>
<tr><th id="781">781</th><td>    <em>auto</em> transpose = perftools::gputools::blas::Transpose::kTranspose;</td></tr>
<tr><th id="782">782</th><td>    <em>auto</em> no_transpose = perftools::gputools::blas::Transpose::kNoTranspose;</td></tr>
<tr><th id="783">783</th><td></td></tr>
<tr><th id="784">784</th><td>    <em>bool</em> blas_launch_status =</td></tr>
<tr><th id="785">785</th><td>        stream</td></tr>
<tr><th id="786">786</th><td>            -&gt;ThenBlasGemm(transpose, no_transpose, n, m, k, <var>1.0f</var>, b_ptr, k,</td></tr>
<tr><th id="787">787</th><td>                           a_ptr, k, <var>0.0f</var>, &amp;c_ptr, n)</td></tr>
<tr><th id="788">788</th><td>            .ok();</td></tr>
<tr><th id="789">789</th><td>    <b>if</b> (!blas_launch_status) {</td></tr>
<tr><th id="790">790</th><td>      ctx-&gt;SetStatus(errors::Internal(<q>"Blas SGEMM launch failed : m="</q>, m,</td></tr>
<tr><th id="791">791</th><td>                                      <q>", n="</q>, n, <q>", k="</q>, k));</td></tr>
<tr><th id="792">792</th><td>    }</td></tr>
<tr><th id="793">793</th><td>    <b>return</b>;</td></tr>
<tr><th id="794">794</th><td>  } <b>else</b> <b>if</b> (dims.spatial_dims[<var>0</var>].filter_size ==</td></tr>
<tr><th id="795">795</th><td>                 dims.spatial_dims[<var>0</var>].input_size &amp;&amp;</td></tr>
<tr><th id="796">796</th><td>             dims.spatial_dims[<var>1</var>].filter_size ==</td></tr>
<tr><th id="797">797</th><td>                 dims.spatial_dims[<var>1</var>].input_size &amp;&amp;</td></tr>
<tr><th id="798">798</th><td>             padding == VALID &amp;&amp; data_format == FORMAT_NHWC) {</td></tr>
<tr><th id="799">799</th><td>    <i>// The input data and filter have the same height/width, so call cublas</i></td></tr>
<tr><th id="800">800</th><td><i>    // directly.</i></td></tr>
<tr><th id="801">801</th><td>    <em>const</em> uint64 m = dims.batch_size;</td></tr>
<tr><th id="802">802</th><td>    <em>const</em> uint64 k = dims.out_depth;</td></tr>
<tr><th id="803">803</th><td>    <em>const</em> uint64 n = dims.spatial_dims[<var>0</var>].input_size *</td></tr>
<tr><th id="804">804</th><td>                     dims.spatial_dims[<var>1</var>].input_size * dims.in_depth;</td></tr>
<tr><th id="805">805</th><td></td></tr>
<tr><th id="806">806</th><td>    <em>auto</em> a_ptr = AsDeviceMemory(out_backprop.<b>template</b> flat&lt;T&gt;().data(),</td></tr>
<tr><th id="807">807</th><td>                                out_backprop.<b>template</b> flat&lt;T&gt;().size());</td></tr>
<tr><th id="808">808</th><td>    <em>auto</em> b_ptr = AsDeviceMemory(filter.<b>template</b> flat&lt;T&gt;().data(),</td></tr>
<tr><th id="809">809</th><td>                                filter.<b>template</b> flat&lt;T&gt;().size());</td></tr>
<tr><th id="810">810</th><td>    <em>auto</em> c_ptr = AsDeviceMemory(in_backprop-&gt;<b>template</b> flat&lt;T&gt;().data(),</td></tr>
<tr><th id="811">811</th><td>                                in_backprop-&gt;<b>template</b> flat&lt;T&gt;().size());</td></tr>
<tr><th id="812">812</th><td></td></tr>
<tr><th id="813">813</th><td>    <em>auto</em> transpose = perftools::gputools::blas::Transpose::kTranspose;</td></tr>
<tr><th id="814">814</th><td>    <em>auto</em> no_transpose = perftools::gputools::blas::Transpose::kNoTranspose;</td></tr>
<tr><th id="815">815</th><td></td></tr>
<tr><th id="816">816</th><td>    <em>bool</em> blas_launch_status =</td></tr>
<tr><th id="817">817</th><td>        stream</td></tr>
<tr><th id="818">818</th><td>            -&gt;ThenBlasGemm(transpose, no_transpose, n, m, k, <var>1.0f</var>, b_ptr, k,</td></tr>
<tr><th id="819">819</th><td>                           a_ptr, k, <var>0.0f</var>, &amp;c_ptr, n)</td></tr>
<tr><th id="820">820</th><td>            .ok();</td></tr>
<tr><th id="821">821</th><td>    <b>if</b> (!blas_launch_status) {</td></tr>
<tr><th id="822">822</th><td>      ctx-&gt;SetStatus(errors::Internal(<q>"Blas SGEMM launch failed : m="</q>, m,</td></tr>
<tr><th id="823">823</th><td>                                      <q>", n="</q>, n, <q>", k="</q>, k));</td></tr>
<tr><th id="824">824</th><td>    }</td></tr>
<tr><th id="825">825</th><td>    <b>return</b>;</td></tr>
<tr><th id="826">826</th><td>  }</td></tr>
<tr><th id="827">827</th><td></td></tr>
<tr><th id="828">828</th><td>  TensorShape compatible_input_shape;</td></tr>
<tr><th id="829">829</th><td>  <b>if</b> (rows_odd || cols_odd) {</td></tr>
<tr><th id="830">830</th><td>    <i>// If a padding dimension is odd, we have one more element on the right</i></td></tr>
<tr><th id="831">831</th><td><i>    // side or the bottom side. This is unsupported in cudnn. Therefore,</i></td></tr>
<tr><th id="832">832</th><td><i>    // we pad that extra element and make it compatible.</i></td></tr>
<tr><th id="833">833</th><td>    compatible_input_shape = ShapeFromFormat(</td></tr>
<tr><th id="834">834</th><td>        data_format, dims.batch_size,</td></tr>
<tr><th id="835">835</th><td>        dims.spatial_dims[<var>0</var>].input_size + rows_odd,</td></tr>
<tr><th id="836">836</th><td>        dims.spatial_dims[<var>1</var>].input_size + cols_odd, dims.in_depth);</td></tr>
<tr><th id="837">837</th><td>  } <b>else</b> {</td></tr>
<tr><th id="838">838</th><td>    compatible_input_shape = input_shape;</td></tr>
<tr><th id="839">839</th><td>  }</td></tr>
<tr><th id="840">840</th><td></td></tr>
<tr><th id="841">841</th><td>  CHECK(padding_rows &gt;= <var>0</var> &amp;&amp; padding_cols &gt;= <var>0</var>)</td></tr>
<tr><th id="842">842</th><td>      &lt;&lt; <q>"Negative row or col paddings: ("</q> &lt;&lt; padding_rows &lt;&lt; <q>", "</q></td></tr>
<tr><th id="843">843</th><td>      &lt;&lt; padding_cols &lt;&lt; <q>")"</q>;</td></tr>
<tr><th id="844">844</th><td>  perftools::gputools::dnn::BatchDescriptor input_desc;</td></tr>
<tr><th id="845">845</th><td>  input_desc.set_count(dims.batch_size)</td></tr>
<tr><th id="846">846</th><td>      .set_height(GetTensorDim(compatible_input_shape, data_format, <kbd>'H'</kbd>))</td></tr>
<tr><th id="847">847</th><td>      .set_width(GetTensorDim(compatible_input_shape, data_format, <kbd>'W'</kbd>))</td></tr>
<tr><th id="848">848</th><td>      .set_feature_map_count(dims.in_depth)</td></tr>
<tr><th id="849">849</th><td>      .set_layout(perftools::gputools::dnn::DataLayout::kBatchDepthYX);</td></tr>
<tr><th id="850">850</th><td>  perftools::gputools::dnn::BatchDescriptor output_desc;</td></tr>
<tr><th id="851">851</th><td>  output_desc.set_count(dims.batch_size)</td></tr>
<tr><th id="852">852</th><td>      .set_height(dims.spatial_dims[<var>0</var>].output_size)</td></tr>
<tr><th id="853">853</th><td>      .set_width(dims.spatial_dims[<var>1</var>].output_size)</td></tr>
<tr><th id="854">854</th><td>      .set_feature_map_count(dims.out_depth)</td></tr>
<tr><th id="855">855</th><td>      .set_layout(perftools::gputools::dnn::DataLayout::kBatchDepthYX);</td></tr>
<tr><th id="856">856</th><td>  perftools::gputools::dnn::FilterDescriptor filter_desc;</td></tr>
<tr><th id="857">857</th><td>  filter_desc.set_input_filter_height(dims.spatial_dims[<var>0</var>].filter_size)</td></tr>
<tr><th id="858">858</th><td>      .set_input_filter_width(dims.spatial_dims[<var>1</var>].filter_size)</td></tr>
<tr><th id="859">859</th><td>      .set_input_feature_map_count(dims.in_depth)</td></tr>
<tr><th id="860">860</th><td>      .set_output_feature_map_count(dims.out_depth);</td></tr>
<tr><th id="861">861</th><td>  perftools::gputools::dnn::ConvolutionDescriptor conv_desc;</td></tr>
<tr><th id="862">862</th><td>  conv_desc.set_vertical_dilation_rate(dims.spatial_dims[<var>0</var>].dilation)</td></tr>
<tr><th id="863">863</th><td>      .set_horizontal_dilation_rate(dims.spatial_dims[<var>1</var>].dilation)</td></tr>
<tr><th id="864">864</th><td>      .set_vertical_filter_stride(dims.spatial_dims[<var>0</var>].stride)</td></tr>
<tr><th id="865">865</th><td>      .set_horizontal_filter_stride(dims.spatial_dims[<var>1</var>].stride)</td></tr>
<tr><th id="866">866</th><td>      .set_zero_padding_height(padding_rows / <var>2</var>)</td></tr>
<tr><th id="867">867</th><td>      .set_zero_padding_width(padding_cols / <var>2</var>);</td></tr>
<tr><th id="868">868</th><td></td></tr>
<tr><th id="869">869</th><td>  <i>// NOTE(keveman):</i></td></tr>
<tr><th id="870">870</th><td><i>  // cuDNN only supports the following layouts :</i></td></tr>
<tr><th id="871">871</th><td><i>  // Input  : B x D x R x C</i></td></tr>
<tr><th id="872">872</th><td><i>  // Filter : OD x ID x R x C</i></td></tr>
<tr><th id="873">873</th><td><i>  // Whereas, we have</i></td></tr>
<tr><th id="874">874</th><td><i>  // Input  : B x R x C x D</i></td></tr>
<tr><th id="875">875</th><td><i>  // Filter : R x C x ID x OD</i></td></tr>
<tr><th id="876">876</th><td><i>  // TransformFilter performs (R x C x ID x OD) =&gt; (OD x ID x R x C)</i></td></tr>
<tr><th id="877">877</th><td><i>  // The first TransformDepth performs</i></td></tr>
<tr><th id="878">878</th><td><i>  // (B x R x C x D) =&gt; (B x D x R x C).</i></td></tr>
<tr><th id="879">879</th><td><i>  // Since the tensor returned from cuDNN is B x D x R x C also,</i></td></tr>
<tr><th id="880">880</th><td><i>  // the second TransformDepth performs</i></td></tr>
<tr><th id="881">881</th><td><i>  // (B x D x R x C) =&gt; (B x R x C x D).</i></td></tr>
<tr><th id="882">882</th><td>  Tensor transformed_filter;</td></tr>
<tr><th id="883">883</th><td>  OP_REQUIRES_OK(</td></tr>
<tr><th id="884">884</th><td>      ctx, ctx-&gt;allocate_temp(DataTypeToEnum&lt;T&gt;::value,</td></tr>
<tr><th id="885">885</th><td>                              TensorShape({dims.out_depth, dims.in_depth,</td></tr>
<tr><th id="886">886</th><td>                                           dims.spatial_dims[<var>0</var>].filter_size,</td></tr>
<tr><th id="887">887</th><td>                                           dims.spatial_dims[<var>1</var>].filter_size}),</td></tr>
<tr><th id="888">888</th><td>                              &amp;transformed_filter));</td></tr>
<tr><th id="889">889</th><td></td></tr>
<tr><th id="890">890</th><td>  functor::TransformFilter&lt;GPUDevice, T, <em>int</em>, <var>4</var>&gt;()(</td></tr>
<tr><th id="891">891</th><td>      ctx-&gt;eigen_device&lt;GPUDevice&gt;(), To32Bit(filter.tensor&lt;T, <var>4</var>&gt;()),</td></tr>
<tr><th id="892">892</th><td>      To32Bit(transformed_filter.tensor&lt;T, <var>4</var>&gt;()));</td></tr>
<tr><th id="893">893</th><td></td></tr>
<tr><th id="894">894</th><td>  Tensor transformed_out_backprop;</td></tr>
<tr><th id="895">895</th><td>  <b>if</b> (data_format == FORMAT_NHWC) {</td></tr>
<tr><th id="896">896</th><td>    TensorShape nchw_shape = ShapeFromFormat(</td></tr>
<tr><th id="897">897</th><td>        FORMAT_NCHW, dims.batch_size, dims.spatial_dims[<var>0</var>].output_size,</td></tr>
<tr><th id="898">898</th><td>        dims.spatial_dims[<var>1</var>].output_size, dims.out_depth);</td></tr>
<tr><th id="899">899</th><td>    <b>if</b> (dims.out_depth &gt; <var>1</var>) {</td></tr>
<tr><th id="900">900</th><td>      OP_REQUIRES_OK(ctx,</td></tr>
<tr><th id="901">901</th><td>                     ctx-&gt;allocate_temp(DataTypeToEnum&lt;T&gt;::value, nchw_shape,</td></tr>
<tr><th id="902">902</th><td>                                        &amp;transformed_out_backprop));</td></tr>
<tr><th id="903">903</th><td>      functor::NHWCToNCHW&lt;GPUDevice, T, <var>4</var>&gt;()(</td></tr>
<tr><th id="904">904</th><td>          ctx-&gt;eigen_device&lt;GPUDevice&gt;(), out_backprop.tensor&lt;T, <var>4</var>&gt;(),</td></tr>
<tr><th id="905">905</th><td>          transformed_out_backprop.tensor&lt;T, <var>4</var>&gt;());</td></tr>
<tr><th id="906">906</th><td>    } <b>else</b> {</td></tr>
<tr><th id="907">907</th><td>      <i>// If depth &lt;= 1, then just reshape.</i></td></tr>
<tr><th id="908">908</th><td>      CHECK(transformed_out_backprop.CopyFrom(out_backprop, nchw_shape));</td></tr>
<tr><th id="909">909</th><td>    }</td></tr>
<tr><th id="910">910</th><td>  } <b>else</b> {</td></tr>
<tr><th id="911">911</th><td>    transformed_out_backprop = out_backprop;</td></tr>
<tr><th id="912">912</th><td>  }</td></tr>
<tr><th id="913">913</th><td></td></tr>
<tr><th id="914">914</th><td>  Tensor pre_transformed_in_backprop;</td></tr>
<tr><th id="915">915</th><td>  OP_REQUIRES_OK(</td></tr>
<tr><th id="916">916</th><td>      ctx, ctx-&gt;allocate_temp(</td></tr>
<tr><th id="917">917</th><td>               DataTypeToEnum&lt;T&gt;::value,</td></tr>
<tr><th id="918">918</th><td>               ShapeFromFormat(</td></tr>
<tr><th id="919">919</th><td>                   FORMAT_NCHW,</td></tr>
<tr><th id="920">920</th><td>                   GetTensorDim(compatible_input_shape, data_format, <kbd>'N'</kbd>),</td></tr>
<tr><th id="921">921</th><td>                   GetTensorDim(compatible_input_shape, data_format, <kbd>'H'</kbd>),</td></tr>
<tr><th id="922">922</th><td>                   GetTensorDim(compatible_input_shape, data_format, <kbd>'W'</kbd>),</td></tr>
<tr><th id="923">923</th><td>                   GetTensorDim(compatible_input_shape, data_format, <kbd>'C'</kbd>)),</td></tr>
<tr><th id="924">924</th><td>               &amp;pre_transformed_in_backprop));</td></tr>
<tr><th id="925">925</th><td></td></tr>
<tr><th id="926">926</th><td>  <em>auto</em> out_backprop_ptr =</td></tr>
<tr><th id="927">927</th><td>      AsDeviceMemory(transformed_out_backprop.<b>template</b> flat&lt;T&gt;().data(),</td></tr>
<tr><th id="928">928</th><td>                     transformed_out_backprop.<b>template</b> flat&lt;T&gt;().size());</td></tr>
<tr><th id="929">929</th><td>  <em>auto</em> filter_ptr =</td></tr>
<tr><th id="930">930</th><td>      AsDeviceMemory(transformed_filter.<b>template</b> flat&lt;T&gt;().data(),</td></tr>
<tr><th id="931">931</th><td>                     transformed_filter.<b>template</b> flat&lt;T&gt;().size());</td></tr>
<tr><th id="932">932</th><td>  <em>auto</em> in_backprop_ptr =</td></tr>
<tr><th id="933">933</th><td>      AsDeviceMemory(pre_transformed_in_backprop.<b>template</b> flat&lt;T&gt;().data(),</td></tr>
<tr><th id="934">934</th><td>                     pre_transformed_in_backprop.<b>template</b> flat&lt;T&gt;().size());</td></tr>
<tr><th id="935">935</th><td></td></tr>
<tr><th id="936">936</th><td>  <em>static</em> int64 ConvolveBackwardDataScratchSize = GetCudnnWorkspaceLimit(</td></tr>
<tr><th id="937">937</th><td>      <q>"TF_CUDNN_WORKSPACE_LIMIT_IN_MB"</q>, <var>1LL</var> &lt;&lt; <var>32</var>  <i>// 4GB by default</i></td></tr>
<tr><th id="938">938</th><td>  );</td></tr>
<tr><th id="939">939</th><td>  CudnnScratchAllocator scratch_allocator(ConvolveBackwardDataScratchSize, ctx);</td></tr>
<tr><th id="940">940</th><td>  <em>int</em> device_id = stream-&gt;parent()-&gt;device_ordinal();</td></tr>
<tr><th id="941">941</th><td>  DataType dtype = out_backprop.dtype();</td></tr>
<tr><th id="942">942</th><td>  ConvParameters conv_parameters = {</td></tr>
<tr><th id="943">943</th><td>      dims.batch_size,                       <i>// batch</i></td></tr>
<tr><th id="944">944</th><td>      dims.in_depth,                         <i>// in_depths</i></td></tr>
<tr><th id="945">945</th><td>      {{input_desc.height(),                 <i>// in_rows</i></td></tr>
<tr><th id="946">946</th><td>        input_desc.width()}},                <i>// in_cols</i></td></tr>
<tr><th id="947">947</th><td>      dims.out_depth,                        <i>// out_depths</i></td></tr>
<tr><th id="948">948</th><td>      {{dims.spatial_dims[<var>0</var>].filter_size,    <i>// filter_rows</i></td></tr>
<tr><th id="949">949</th><td>        dims.spatial_dims[<var>1</var>].filter_size}},  <i>// filter_cols</i></td></tr>
<tr><th id="950">950</th><td>      {{dims.spatial_dims[<var>0</var>].dilation,       <i>// dilation_rows</i></td></tr>
<tr><th id="951">951</th><td>        dims.spatial_dims[<var>1</var>].dilation}},     <i>// dilation_cols</i></td></tr>
<tr><th id="952">952</th><td>      {{dims.spatial_dims[<var>0</var>].stride,         <i>// stride_rows</i></td></tr>
<tr><th id="953">953</th><td>        dims.spatial_dims[<var>1</var>].stride}},       <i>// stride_cols</i></td></tr>
<tr><th id="954">954</th><td>      {{padding_rows,                        <i>// padding_rows</i></td></tr>
<tr><th id="955">955</th><td>        padding_cols}},                      <i>// padding_cols</i></td></tr>
<tr><th id="956">956</th><td>      dtype,                                 <i>// tensor data type</i></td></tr>
<tr><th id="957">957</th><td>      device_id,                             <i>// device_id</i></td></tr>
<tr><th id="958">958</th><td>  };</td></tr>
<tr><th id="959">959</th><td>  AlgorithmConfig algorithm_config;</td></tr>
<tr><th id="960">960</th><td>  <b>if</b> (cudnn_use_autotune &amp;&amp; !AutoTuneConvBwdData::GetInstance()-&gt;Find(</td></tr>
<tr><th id="961">961</th><td>                                conv_parameters, &amp;algorithm_config)) {</td></tr>
<tr><th id="962">962</th><td>    std::vector&lt;AlgorithmDesc&gt; algorithms;</td></tr>
<tr><th id="963">963</th><td>    CHECK(stream-&gt;parent()-&gt;GetConvolveBackwardDataAlgorithms(</td></tr>
<tr><th id="964">964</th><td>        conv_parameters.ShouldIncludeWinogradNonfusedAlgo&lt;T&gt;(), &amp;algorithms));</td></tr>
<tr><th id="965">965</th><td>    ProfileResult best_result;</td></tr>
<tr><th id="966">966</th><td>    ProfileResult best_result_no_scratch;</td></tr>
<tr><th id="967">967</th><td>    <b>for</b> (<em>auto</em> profile_algorithm : algorithms) {</td></tr>
<tr><th id="968">968</th><td>      <i>// TODO(zhengxq): profile each algorithm multiple times to better</i></td></tr>
<tr><th id="969">969</th><td><i>      // accuracy.</i></td></tr>
<tr><th id="970">970</th><td>      CudnnScratchAllocator scratch_allocator(ConvolveBackwardDataScratchSize,</td></tr>
<tr><th id="971">971</th><td>                                              ctx);</td></tr>
<tr><th id="972">972</th><td>      ProfileResult profile_result;</td></tr>
<tr><th id="973">973</th><td>      <em>bool</em> cudnn_launch_status =</td></tr>
<tr><th id="974">974</th><td>          stream</td></tr>
<tr><th id="975">975</th><td>              -&gt;ThenConvolveBackwardDataWithAlgorithm(</td></tr>
<tr><th id="976">976</th><td>                  filter_desc, filter_ptr, output_desc, out_backprop_ptr,</td></tr>
<tr><th id="977">977</th><td>                  conv_desc, input_desc, &amp;in_backprop_ptr, &amp;scratch_allocator,</td></tr>
<tr><th id="978">978</th><td>                  AlgorithmConfig(profile_algorithm), &amp;profile_result)</td></tr>
<tr><th id="979">979</th><td>              .ok();</td></tr>
<tr><th id="980">980</th><td>      <b>if</b> (cudnn_launch_status) {</td></tr>
<tr><th id="981">981</th><td>        <b>if</b> (profile_result.is_valid()) {</td></tr>
<tr><th id="982">982</th><td>          <b>if</b> (profile_result.elapsed_time_in_ms() &lt;</td></tr>
<tr><th id="983">983</th><td>              best_result.elapsed_time_in_ms()) {</td></tr>
<tr><th id="984">984</th><td>            best_result = profile_result;</td></tr>
<tr><th id="985">985</th><td>          }</td></tr>
<tr><th id="986">986</th><td>          <b>if</b> (scratch_allocator.TotalByteSize() == <var>0</var> &amp;&amp;</td></tr>
<tr><th id="987">987</th><td>              profile_result.elapsed_time_in_ms() &lt;</td></tr>
<tr><th id="988">988</th><td>                  best_result_no_scratch.elapsed_time_in_ms()) {</td></tr>
<tr><th id="989">989</th><td>            best_result_no_scratch = profile_result;</td></tr>
<tr><th id="990">990</th><td>          }</td></tr>
<tr><th id="991">991</th><td>        }</td></tr>
<tr><th id="992">992</th><td>      }</td></tr>
<tr><th id="993">993</th><td>    }</td></tr>
<tr><th id="994">994</th><td>    OP_REQUIRES(ctx,</td></tr>
<tr><th id="995">995</th><td>                best_result.is_valid() || best_result_no_scratch.is_valid(),</td></tr>
<tr><th id="996">996</th><td>                errors::NotFound(<q>"No algorithm worked!"</q>));</td></tr>
<tr><th id="997">997</th><td>    <b>if</b> (best_result.is_valid()) {</td></tr>
<tr><th id="998">998</th><td>      algorithm_config.set_algorithm(best_result.algorithm());</td></tr>
<tr><th id="999">999</th><td>    }</td></tr>
<tr><th id="1000">1000</th><td>    <b>if</b> (best_result_no_scratch.is_valid()) {</td></tr>
<tr><th id="1001">1001</th><td>      algorithm_config.set_algorithm_no_scratch(</td></tr>
<tr><th id="1002">1002</th><td>          best_result_no_scratch.algorithm());</td></tr>
<tr><th id="1003">1003</th><td>    }</td></tr>
<tr><th id="1004">1004</th><td>    AutoTuneConvBwdData::GetInstance()-&gt;Insert(conv_parameters,</td></tr>
<tr><th id="1005">1005</th><td>                                               algorithm_config);</td></tr>
<tr><th id="1006">1006</th><td>  }</td></tr>
<tr><th id="1007">1007</th><td>  <em>bool</em> cudnn_launch_status =</td></tr>
<tr><th id="1008">1008</th><td>      stream</td></tr>
<tr><th id="1009">1009</th><td>          -&gt;ThenConvolveBackwardDataWithAlgorithm(</td></tr>
<tr><th id="1010">1010</th><td>              filter_desc, filter_ptr, output_desc, out_backprop_ptr, conv_desc,</td></tr>
<tr><th id="1011">1011</th><td>              input_desc, &amp;in_backprop_ptr, &amp;scratch_allocator,</td></tr>
<tr><th id="1012">1012</th><td>              algorithm_config, <b>nullptr</b>)</td></tr>
<tr><th id="1013">1013</th><td>          .ok();</td></tr>
<tr><th id="1014">1014</th><td></td></tr>
<tr><th id="1015">1015</th><td>  <b>if</b> (!cudnn_launch_status) {</td></tr>
<tr><th id="1016">1016</th><td>    ctx-&gt;SetStatus(errors::Internal(</td></tr>
<tr><th id="1017">1017</th><td>        <q>"cuDNN Backward Data function launch failure : input shape("</q>,</td></tr>
<tr><th id="1018">1018</th><td>        input_shape.DebugString(), <q>") filter shape("</q>,</td></tr>
<tr><th id="1019">1019</th><td>        filter_shape.DebugString(), <q>")"</q>));</td></tr>
<tr><th id="1020">1020</th><td>    <b>return</b>;</td></tr>
<tr><th id="1021">1021</th><td>  }</td></tr>
<tr><th id="1022">1022</th><td></td></tr>
<tr><th id="1023">1023</th><td>  <b>if</b> (rows_odd || cols_odd) {</td></tr>
<tr><th id="1024">1024</th><td>    Tensor in_backprop_remove_padding;</td></tr>
<tr><th id="1025">1025</th><td>    OP_REQUIRES_OK(</td></tr>
<tr><th id="1026">1026</th><td>        ctx, ctx-&gt;allocate_temp(</td></tr>
<tr><th id="1027">1027</th><td>                 DataTypeToEnum&lt;T&gt;::value,</td></tr>
<tr><th id="1028">1028</th><td>                 ShapeFromFormat(FORMAT_NCHW,</td></tr>
<tr><th id="1029">1029</th><td>                                 GetTensorDim(input_shape, data_format, <kbd>'N'</kbd>),</td></tr>
<tr><th id="1030">1030</th><td>                                 GetTensorDim(input_shape, data_format, <kbd>'H'</kbd>),</td></tr>
<tr><th id="1031">1031</th><td>                                 GetTensorDim(input_shape, data_format, <kbd>'W'</kbd>),</td></tr>
<tr><th id="1032">1032</th><td>                                 GetTensorDim(input_shape, data_format, <kbd>'C'</kbd>)),</td></tr>
<tr><th id="1033">1033</th><td>                 &amp;in_backprop_remove_padding));</td></tr>
<tr><th id="1034">1034</th><td></td></tr>
<tr><th id="1035">1035</th><td>    <i>// Remove the padding for odd rows or cols.</i></td></tr>
<tr><th id="1036">1036</th><td>    functor::PadInput&lt;GPUDevice, T, <em>int</em>, <var>4</var>&gt;()(</td></tr>
<tr><th id="1037">1037</th><td>        ctx-&gt;<b>template</b> eigen_device&lt;GPUDevice&gt;(),</td></tr>
<tr><th id="1038">1038</th><td>        To32Bit(<b>const_cast</b>&lt;<em>const</em> Tensor&amp;&gt;(pre_transformed_in_backprop)</td></tr>
<tr><th id="1039">1039</th><td>                    .tensor&lt;T, <var>4</var>&gt;()),</td></tr>
<tr><th id="1040">1040</th><td>        {{<var>0</var>, <var>0</var>}}, {{-rows_odd, -cols_odd}},</td></tr>
<tr><th id="1041">1041</th><td>        To32Bit(in_backprop_remove_padding.tensor&lt;T, <var>4</var>&gt;()), FORMAT_NCHW);</td></tr>
<tr><th id="1042">1042</th><td></td></tr>
<tr><th id="1043">1043</th><td>    pre_transformed_in_backprop = in_backprop_remove_padding;</td></tr>
<tr><th id="1044">1044</th><td>  }</td></tr>
<tr><th id="1045">1045</th><td></td></tr>
<tr><th id="1046">1046</th><td>  <b>if</b> (data_format == FORMAT_NHWC) {</td></tr>
<tr><th id="1047">1047</th><td>    <em>auto</em> toConstTensor = [](<em>const</em> Tensor&amp; x) -&gt; <em>const</em> Tensor { <b>return</b> x; };</td></tr>
<tr><th id="1048">1048</th><td>    functor::NCHWToNHWC&lt;GPUDevice, T, <var>4</var>&gt;()(</td></tr>
<tr><th id="1049">1049</th><td>        ctx-&gt;eigen_device&lt;GPUDevice&gt;(),</td></tr>
<tr><th id="1050">1050</th><td>        toConstTensor(pre_transformed_in_backprop).<b>template</b> tensor&lt;T, <var>4</var>&gt;(),</td></tr>
<tr><th id="1051">1051</th><td>        in_backprop-&gt;tensor&lt;T, <var>4</var>&gt;());</td></tr>
<tr><th id="1052">1052</th><td>  } <b>else</b> {</td></tr>
<tr><th id="1053">1053</th><td>    *in_backprop = pre_transformed_in_backprop;</td></tr>
<tr><th id="1054">1054</th><td>  }</td></tr>
<tr><th id="1055">1055</th><td>}</td></tr>
<tr><th id="1056">1056</th><td></td></tr>
<tr><th id="1057">1057</th><td><i>// Forward declarations of the functor specializations for GPU.</i></td></tr>
<tr><th id="1058">1058</th><td><b>namespace</b> functor {</td></tr>
<tr><th id="1059">1059</th><td><u>#define DECLARE_GPU_SPEC(T)                                              \</u></td></tr>
<tr><th id="1060">1060</th><td><u>  template &lt;&gt;                                                            \</u></td></tr>
<tr><th id="1061">1061</th><td><u>  void ShuffleAndReverse&lt;GPUDevice, T, 4, int&gt;::operator()(              \</u></td></tr>
<tr><th id="1062">1062</th><td><u>      const GPUDevice&amp; d, typename TTypes&lt;T, 4, int&gt;::ConstTensor input, \</u></td></tr>
<tr><th id="1063">1063</th><td><u>      const Eigen::DSizes&lt;int, 4&gt;&amp; order,                                \</u></td></tr>
<tr><th id="1064">1064</th><td><u>      const Eigen::array&lt;bool, 4&gt;&amp; reverse_dims,                         \</u></td></tr>
<tr><th id="1065">1065</th><td><u>      typename TTypes&lt;T, 4, int&gt;::Tensor output);                        \</u></td></tr>
<tr><th id="1066">1066</th><td><u>  extern template struct ShuffleAndReverse&lt;GPUDevice, T, 4, int&gt;;        \</u></td></tr>
<tr><th id="1067">1067</th><td><u>  template &lt;&gt;                                                            \</u></td></tr>
<tr><th id="1068">1068</th><td><u>  void InflatePadAndShuffle&lt;GPUDevice, T, 4, int&gt;::operator()(           \</u></td></tr>
<tr><th id="1069">1069</th><td><u>      const GPUDevice&amp; d, typename TTypes&lt;T, 4, int&gt;::ConstTensor input, \</u></td></tr>
<tr><th id="1070">1070</th><td><u>      const Eigen::DSizes&lt;int, 4&gt;&amp; strides,                              \</u></td></tr>
<tr><th id="1071">1071</th><td><u>      const Eigen::array&lt;Eigen::IndexPair&lt;int&gt;, 4&gt;&amp; pad_dims,            \</u></td></tr>
<tr><th id="1072">1072</th><td><u>      const Eigen::DSizes&lt;int, 4&gt;&amp; order,                                \</u></td></tr>
<tr><th id="1073">1073</th><td><u>      typename TTypes&lt;T, 4, int&gt;::Tensor output);                        \</u></td></tr>
<tr><th id="1074">1074</th><td><u>  extern template struct InflatePadAndShuffle&lt;GPUDevice, T, 4, int&gt;;     \</u></td></tr>
<tr><th id="1075">1075</th><td><u>  template &lt;&gt;                                                            \</u></td></tr>
<tr><th id="1076">1076</th><td><u>  void TransformFilter&lt;GPUDevice, T, int, 4&gt;::operator()(                \</u></td></tr>
<tr><th id="1077">1077</th><td><u>      const GPUDevice&amp; d, typename TTypes&lt;T, 4, int&gt;::ConstTensor in,    \</u></td></tr>
<tr><th id="1078">1078</th><td><u>      typename TTypes&lt;T, 4, int&gt;::Tensor out);                           \</u></td></tr>
<tr><th id="1079">1079</th><td><u>  extern template struct TransformFilter&lt;GPUDevice, T, int, 4&gt;;          \</u></td></tr>
<tr><th id="1080">1080</th><td><u>  template &lt;&gt;                                                            \</u></td></tr>
<tr><th id="1081">1081</th><td><u>  void TransformDepth&lt;GPUDevice, T, int&gt;::operator()(                    \</u></td></tr>
<tr><th id="1082">1082</th><td><u>      const GPUDevice&amp; d, typename TTypes&lt;T, 4, int&gt;::ConstTensor in,    \</u></td></tr>
<tr><th id="1083">1083</th><td><u>      const Eigen::DSizes&lt;int, 4&gt;&amp; shuffle,                              \</u></td></tr>
<tr><th id="1084">1084</th><td><u>      typename TTypes&lt;T, 4, int&gt;::Tensor out);                           \</u></td></tr>
<tr><th id="1085">1085</th><td><u>  extern template struct TransformDepth&lt;GPUDevice, T, int&gt;;              \</u></td></tr>
<tr><th id="1086">1086</th><td><u>  template &lt;&gt;                                                            \</u></td></tr>
<tr><th id="1087">1087</th><td><u>  void PadInput&lt;GPUDevice, T, int, 4&gt;::operator()(                       \</u></td></tr>
<tr><th id="1088">1088</th><td><u>      const GPUDevice&amp; d, typename TTypes&lt;T, 4, int&gt;::ConstTensor in,    \</u></td></tr>
<tr><th id="1089">1089</th><td><u>      const std::array&lt;int, 2&gt;&amp; padding_left,                            \</u></td></tr>
<tr><th id="1090">1090</th><td><u>      const std::array&lt;int, 2&gt;&amp; padding_right,                           \</u></td></tr>
<tr><th id="1091">1091</th><td><u>      typename TTypes&lt;T, 4, int&gt;::Tensor out, TensorFormat data_format); \</u></td></tr>
<tr><th id="1092">1092</th><td><u>  extern template struct PadInput&lt;GPUDevice, T, int, 4&gt;;</u></td></tr>
<tr><th id="1093">1093</th><td></td></tr>
<tr><th id="1094">1094</th><td>DECLARE_GPU_SPEC(<em>double</em>);</td></tr>
<tr><th id="1095">1095</th><td>DECLARE_GPU_SPEC(<em>float</em>);</td></tr>
<tr><th id="1096">1096</th><td>DECLARE_GPU_SPEC(Eigen::half);</td></tr>
<tr><th id="1097">1097</th><td><u>#undef DECLARE_GPU_SPEC</u></td></tr>
<tr><th id="1098">1098</th><td>}  <i>// namespace functor</i></td></tr>
<tr><th id="1099">1099</th><td></td></tr>
<tr><th id="1100">1100</th><td>REGISTER_KERNEL_BUILDER(Name(<q>"Conv2DBackpropInput"</q>)</td></tr>
<tr><th id="1101">1101</th><td>                            .Device(DEVICE_GPU)</td></tr>
<tr><th id="1102">1102</th><td>                            .TypeConstraint&lt;<em>double</em>&gt;(<q>"T"</q>)</td></tr>
<tr><th id="1103">1103</th><td>                            .HostMemory(<q>"input_sizes"</q>),</td></tr>
<tr><th id="1104">1104</th><td>                        Conv2DSlowBackpropInputOp&lt;GPUDevice, <em>double</em>&gt;);</td></tr>
<tr><th id="1105">1105</th><td>REGISTER_KERNEL_BUILDER(Name(<q>"Conv2DBackpropInput"</q>)</td></tr>
<tr><th id="1106">1106</th><td>                            .Device(DEVICE_GPU)</td></tr>
<tr><th id="1107">1107</th><td>                            .TypeConstraint&lt;<em>float</em>&gt;(<q>"T"</q>)</td></tr>
<tr><th id="1108">1108</th><td>                            .HostMemory(<q>"input_sizes"</q>),</td></tr>
<tr><th id="1109">1109</th><td>                        Conv2DSlowBackpropInputOp&lt;GPUDevice, <em>float</em>&gt;);</td></tr>
<tr><th id="1110">1110</th><td>REGISTER_KERNEL_BUILDER(Name(<q>"Conv2DBackpropInput"</q>)</td></tr>
<tr><th id="1111">1111</th><td>                            .Device(DEVICE_GPU)</td></tr>
<tr><th id="1112">1112</th><td>                            .TypeConstraint&lt;Eigen::half&gt;(<q>"T"</q>)</td></tr>
<tr><th id="1113">1113</th><td>                            .HostMemory(<q>"input_sizes"</q>),</td></tr>
<tr><th id="1114">1114</th><td>                        Conv2DSlowBackpropInputOp&lt;GPUDevice, Eigen::half&gt;);</td></tr>
<tr><th id="1115">1115</th><td><u>#<span data-ppcond="599">endif</span>  // GOOGLE_CUDA</u></td></tr>
<tr><th id="1116">1116</th><td></td></tr>
<tr><th id="1117">1117</th><td>}  <i>// namespace tensorflow</i></td></tr>
<tr><th id="1118">1118</th><td></td></tr>
</table><hr/><p id='footer'>
Generated on <em>2018-Sep-06</em> from project tensorflow revision <em>v1.8</em><br />Powered by <a href='https://woboq.com'><img alt='Woboq' src='https://code.woboq.org/woboq-16.png' width='41' height='16' /></a> <a href='https://code.woboq.org'>Code Browser</a> 2.1
<br/>Generator usage only permitted with license.</p>
</div></body></html>
