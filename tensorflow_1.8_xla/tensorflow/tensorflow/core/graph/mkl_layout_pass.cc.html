<!doctype html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><title>mkl_layout_pass.cc source code [tensorflow/tensorflow/core/graph/mkl_layout_pass.cc] - Woboq Code Browser</title>
<link rel="stylesheet" href="https://code.woboq.org/data/qtcreator.css" title="QtCreator"/>
<link rel="alternate stylesheet" href="https://code.woboq.org/data/kdevelop.css" title="KDevelop"/>
<script type="text/javascript" src="https://code.woboq.org/data/jquery/jquery.min.js"></script>
<script type="text/javascript" src="https://code.woboq.org/data/jquery/jquery-ui.min.js"></script>
<script>var file = 'tensorflow/tensorflow/core/graph/mkl_layout_pass.cc'; var root_path = '../../../..'; var data_path = 'https://code.woboq.org/data';</script>
<script src='https://code.woboq.org/data/codebrowser.js'></script>
</head>
<body><div id='header'><h1 id='breadcrumb'><span>Browse the source code of </span><a href='../../..'>tensorflow</a>/<a href='../..'>tensorflow</a>/<a href='..'>core</a>/<a href='./'>graph</a>/<a href='mkl_layout_pass.cc.html'>mkl_layout_pass.cc</a></h1></div>
<hr/><div id='content'><table class="code">
<tr><th id="1">1</th><td><i>/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.</i></td></tr>
<tr><th id="2">2</th><td><i></i></td></tr>
<tr><th id="3">3</th><td><i>Licensed under the Apache License, Version 2.0 (the "License");</i></td></tr>
<tr><th id="4">4</th><td><i>you may not use this file except in compliance with the License.</i></td></tr>
<tr><th id="5">5</th><td><i>You may obtain a copy of the License at</i></td></tr>
<tr><th id="6">6</th><td><i></i></td></tr>
<tr><th id="7">7</th><td><i>    <a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></i></td></tr>
<tr><th id="8">8</th><td><i></i></td></tr>
<tr><th id="9">9</th><td><i>Unless required by applicable law or agreed to in writing, software</i></td></tr>
<tr><th id="10">10</th><td><i>distributed under the License is distributed on an "AS IS" BASIS,</i></td></tr>
<tr><th id="11">11</th><td><i>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</i></td></tr>
<tr><th id="12">12</th><td><i>See the License for the specific language governing permissions and</i></td></tr>
<tr><th id="13">13</th><td><i>limitations under the License.</i></td></tr>
<tr><th id="14">14</th><td><i>==============================================================================*/</i></td></tr>
<tr><th id="15">15</th><td><i></i></td></tr>
<tr><th id="16">16</th><td><i>// TODO(intel): Improve error handling in this file; instead of CHECK failing</i></td></tr>
<tr><th id="17">17</th><td><i>// all over the place, we should log an error and execute the original graph.</i></td></tr>
<tr><th id="18">18</th><td><u>#<span data-ppcond="18">ifdef</span> <span class="macro" data-ref="_M/INTEL_MKL">INTEL_MKL</span></u></td></tr>
<tr><th id="19">19</th><td></td></tr>
<tr><th id="20">20</th><td><u>#include &lt;algorithm&gt;</u></td></tr>
<tr><th id="21">21</th><td><u>#include &lt;functional&gt;</u></td></tr>
<tr><th id="22">22</th><td><u>#include &lt;memory&gt;</u></td></tr>
<tr><th id="23">23</th><td><u>#include &lt;queue&gt;</u></td></tr>
<tr><th id="24">24</th><td><u>#include &lt;set&gt;</u></td></tr>
<tr><th id="25">25</th><td><u>#include &lt;string&gt;</u></td></tr>
<tr><th id="26">26</th><td><u>#include &lt;unordered_set&gt;</u></td></tr>
<tr><th id="27">27</th><td><u>#include &lt;utility&gt;</u></td></tr>
<tr><th id="28">28</th><td><u>#include &lt;vector&gt;</u></td></tr>
<tr><th id="29">29</th><td><u>#include "tensorflow/core/common_runtime/function.h"</u></td></tr>
<tr><th id="30">30</th><td><u>#include "tensorflow/core/common_runtime/optimization_registry.h"</u></td></tr>
<tr><th id="31">31</th><td><u>#include "tensorflow/core/framework/node_def_util.h"</u></td></tr>
<tr><th id="32">32</th><td><u>#include "tensorflow/core/graph/algorithm.h"</u></td></tr>
<tr><th id="33">33</th><td><u>#include "tensorflow/core/graph/graph.h"</u></td></tr>
<tr><th id="34">34</th><td><u>#include "tensorflow/core/graph/node_builder.h"</u></td></tr>
<tr><th id="35">35</th><td><u>#include "tensorflow/core/lib/core/status.h"</u></td></tr>
<tr><th id="36">36</th><td><u>#include "tensorflow/core/lib/gtl/array_slice.h"</u></td></tr>
<tr><th id="37">37</th><td><u>#include "tensorflow/core/lib/gtl/map_util.h"</u></td></tr>
<tr><th id="38">38</th><td><u>#include "tensorflow/core/lib/hash/hash.h"</u></td></tr>
<tr><th id="39">39</th><td><u>#include "tensorflow/core/platform/logging.h"</u></td></tr>
<tr><th id="40">40</th><td><u>#include "tensorflow/core/util/tensor_format.h"</u></td></tr>
<tr><th id="41">41</th><td></td></tr>
<tr><th id="42">42</th><td><u>#include "tensorflow/core/graph/mkl_graph_util.h"</u></td></tr>
<tr><th id="43">43</th><td><u>#include "tensorflow/core/graph/mkl_layout_pass.h"</u></td></tr>
<tr><th id="44">44</th><td></td></tr>
<tr><th id="45">45</th><td><b>namespace</b> tensorflow {</td></tr>
<tr><th id="46">46</th><td></td></tr>
<tr><th id="47">47</th><td><u>#ifdef INTEL_MKL_ML</u></td></tr>
<tr><th id="48">48</th><td></td></tr>
<tr><th id="49">49</th><td><i>// This pass implements rewriting of graph to support following scenarios:</i></td></tr>
<tr><th id="50">50</th><td><i>// (A) Merging nodes in the graph</i></td></tr>
<tr><th id="51">51</th><td><i>// (B) Rewriting a node in the graph to a new node</i></td></tr>
<tr><th id="52">52</th><td><i>//     Rewrite happens under following 2 scenarios:</i></td></tr>
<tr><th id="53">53</th><td><i>//     1) Propagating Mkl layout as an additional output tensor</i></td></tr>
<tr><th id="54">54</th><td><i>//        (we will loosely call a tensor that carries Mkl layout as Mkl tensor</i></td></tr>
<tr><th id="55">55</th><td><i>//         henceforth.) from every Mkl supported NN layer.</i></td></tr>
<tr><th id="56">56</th><td><i>//     2) Context-based rewrite: This is needed in order to optimize</i></td></tr>
<tr><th id="57">57</th><td><i>//        gradient ops of Conv2D+AddBias. Gradient op of both the Conv2D and</i></td></tr>
<tr><th id="58">58</th><td><i>//        MatMul is BiasAddGrad, and we need to rewrite BiasAddGrad into</i></td></tr>
<tr><th id="59">59</th><td><i>//        Conv2D-specific BiasAddGrad, and MatMul-specific BiasAddGrad.</i></td></tr>
<tr><th id="60">60</th><td><i>//        This is context-specific optimization, where the context is the</i></td></tr>
<tr><th id="61">61</th><td><i>//        forward operator that the BiasAddGrad corresponds to.</i></td></tr>
<tr><th id="62">62</th><td><i>//</i></td></tr>
<tr><th id="63">63</th><td><i>// Example of A : Merging nodes in the graph</i></td></tr>
<tr><th id="64">64</th><td><i>// -----------------------------------------</i></td></tr>
<tr><th id="65">65</th><td><i>// Currently, we merge Conv2D+AddBias together. Consider Conv2D and BiasAdd as:</i></td></tr>
<tr><th id="66">66</th><td><i>//</i></td></tr>
<tr><th id="67">67</th><td><i>//           O = Conv2D(A, B)</i></td></tr>
<tr><th id="68">68</th><td><i>//           P = BiasAdd(O, C)</i></td></tr>
<tr><th id="69">69</th><td><i>//</i></td></tr>
<tr><th id="70">70</th><td><i>// We merge them into Conv2DWithBias as:</i></td></tr>
<tr><th id="71">71</th><td><i>//           P = _MklConv2DWithBias(A, A_m, B, B_m, C, C_m)</i></td></tr>
<tr><th id="72">72</th><td><i>//</i></td></tr>
<tr><th id="73">73</th><td><i>// The meaning of A_m, B_m and C_m is explained in B.1.</i></td></tr>
<tr><th id="74">74</th><td><i>//</i></td></tr>
<tr><th id="75">75</th><td><i>// Merge rules:</i></td></tr>
<tr><th id="76">76</th><td><i>//  - The merge for Conv2D and BiasAdd happens when the output of Conv2D _only_</i></td></tr>
<tr><th id="77">77</th><td><i>//    goes to BiasAdd.</i></td></tr>
<tr><th id="78">78</th><td><i>//  - Also, the intersection of attributes of both the nodes must have same</i></td></tr>
<tr><th id="79">79</th><td><i>//    values.</i></td></tr>
<tr><th id="80">80</th><td><i>//  - Both the nodes must have been assigned to same device (if any).</i></td></tr>
<tr><th id="81">81</th><td><i>//</i></td></tr>
<tr><th id="82">82</th><td><i>// Example of B.1 : Rewriting nodes to Mkl nodes</i></td></tr>
<tr><th id="83">83</th><td><i>// ---------------------------------------------</i></td></tr>
<tr><th id="84">84</th><td><i>// Consider a Relu node. Current definition of Relu node looks like:</i></td></tr>
<tr><th id="85">85</th><td><i>//</i></td></tr>
<tr><th id="86">86</th><td><i>//           O = Relu(A)</i></td></tr>
<tr><th id="87">87</th><td><i>//</i></td></tr>
<tr><th id="88">88</th><td><i>// Relu has 1 input (A), and 1 output (O).</i></td></tr>
<tr><th id="89">89</th><td><i>//</i></td></tr>
<tr><th id="90">90</th><td><i>// This rewrite pass will generate a new graph node for Relu (new node is</i></td></tr>
<tr><th id="91">91</th><td><i>// called MklRelu) as:</i></td></tr>
<tr><th id="92">92</th><td><i>//</i></td></tr>
<tr><th id="93">93</th><td><i>//          O, O_m = MklRelu(A, A_m)</i></td></tr>
<tr><th id="94">94</th><td><i>//</i></td></tr>
<tr><th id="95">95</th><td><i>// MklRelu has 2 inputs (A and A_m) and 2 outputs (O and O_m). Here input A is</i></td></tr>
<tr><th id="96">96</th><td><i>// same as input A of Relu; output O is same as output O of Relu. O_m is the</i></td></tr>
<tr><th id="97">97</th><td><i>// additional output tensor that will be set by MklRelu, and it represents</i></td></tr>
<tr><th id="98">98</th><td><i>// Mkl tensor corresponding to O -- in other words, O_m is some kind of</i></td></tr>
<tr><th id="99">99</th><td><i>// metadata for O. A_m is additional input of Relu, and it represents metadata</i></td></tr>
<tr><th id="100">100</th><td><i>// for A - as O_m is metadata for O, A_m is metadata for A. MklRelu receives</i></td></tr>
<tr><th id="101">101</th><td><i>// this metadata from previous node in the graph.</i></td></tr>
<tr><th id="102">102</th><td><i>//</i></td></tr>
<tr><th id="103">103</th><td><i>// When a previous node in the graph is an Mkl node, A_m will represent a valid</i></td></tr>
<tr><th id="104">104</th><td><i>// Mkl tensor. But when a previous node is not an Mkl node, A_m will represent</i></td></tr>
<tr><th id="105">105</th><td><i>// a dummy Mkl tensor.</i></td></tr>
<tr><th id="106">106</th><td><i>//</i></td></tr>
<tr><th id="107">107</th><td><i>// Rewriting rules:</i></td></tr>
<tr><th id="108">108</th><td><i>//  - Selection of a node for rewriting happens by registering the op type of</i></td></tr>
<tr><th id="109">109</th><td><i>//    the node with the rewriting pass. If the op type is not registered, then</i></td></tr>
<tr><th id="110">110</th><td><i>//    all nodes of this op type will not be rewritten.</i></td></tr>
<tr><th id="111">111</th><td><i>//  - Number of inputs after rewriting:</i></td></tr>
<tr><th id="112">112</th><td><i>//      Since for every input Tensorflow tensor, the rewritten node gets Mkl</i></td></tr>
<tr><th id="113">113</th><td><i>//      tensor(s), rewritten node gets 2*N inputs, where N is the number of</i></td></tr>
<tr><th id="114">114</th><td><i>//      inputs for the original node.</i></td></tr>
<tr><th id="115">115</th><td><i>//  - Number of outputs after rewriting:</i></td></tr>
<tr><th id="116">116</th><td><i>//      Since for every output Tensorflow tensor, the rewritten node generates</i></td></tr>
<tr><th id="117">117</th><td><i>//      Mkl tensor(s), the rewritten node generates 2*N outputs, where N is the</i></td></tr>
<tr><th id="118">118</th><td><i>//      number of outputs of the original node.</i></td></tr>
<tr><th id="119">119</th><td><i>//  - Ordering of Tensorflow tensors and Mkl tensors:</i></td></tr>
<tr><th id="120">120</th><td><i>//      Since every rewritten node generates twice the number of inputs and</i></td></tr>
<tr><th id="121">121</th><td><i>//      outputs, one could imagine various orderings among Tensorflow tensors</i></td></tr>
<tr><th id="122">122</th><td><i>//      and Mkl tensors. E.g., assume an op 'Conv2D' that takes (A, B) as</i></td></tr>
<tr><th id="123">123</th><td><i>//      inputs, then the new op '_MklConv2D' can take inputs A, B, A_m and B_m</i></td></tr>
<tr><th id="124">124</th><td><i>//      in A, A_m, B, B_m order or it can also take them in A, B, A_m, B_m</i></td></tr>
<tr><th id="125">125</th><td><i>//      order. Among N inputs one can get N! permutations.</i></td></tr>
<tr><th id="126">126</th><td><i>//</i></td></tr>
<tr><th id="127">127</th><td><i>//      So the question is: which order do we follow? We support 2 types of</i></td></tr>
<tr><th id="128">128</th><td><i>//      orderings: (1) interleaved, and (2) contiguous. Interleaved ordering</i></td></tr>
<tr><th id="129">129</th><td><i>//      follows an intuitive order where an Mkl tensor follows the</i></td></tr>
<tr><th id="130">130</th><td><i>//      corresponding Tensorflow tensor immediately. In the context of the</i></td></tr>
<tr><th id="131">131</th><td><i>//      above example, it will be: A, A_m, B, B_m. Note that the ordering rule</i></td></tr>
<tr><th id="132">132</th><td><i>//      applies to both the inputs and outputs. Contiguous ordering means</i></td></tr>
<tr><th id="133">133</th><td><i>//      all the Tensorflow tensors are contiguous followed by all the Mkl</i></td></tr>
<tr><th id="134">134</th><td><i>//      tensors. We use contiguous ordering as default.</i></td></tr>
<tr><th id="135">135</th><td><i>//</i></td></tr>
<tr><th id="136">136</th><td><i>// Graph rewrite algorithm:</i></td></tr>
<tr><th id="137">137</th><td><i>//      Algorithm: Graph Rewrite</i></td></tr>
<tr><th id="138">138</th><td><i>//      Input: Graph G, Names of the nodes to rewrite and their new names</i></td></tr>
<tr><th id="139">139</th><td><i>//      Output: Modified Graph G' if the nodes are modified, G otherwise.</i></td></tr>
<tr><th id="140">140</th><td><i>//      Start:</i></td></tr>
<tr><th id="141">141</th><td><i>//        N = Topological_Sort(G) // N is a set of nodes in toposort order.</i></td></tr>
<tr><th id="142">142</th><td><i>//        foreach node n in N</i></td></tr>
<tr><th id="143">143</th><td><i>//        do</i></td></tr>
<tr><th id="144">144</th><td><i>//          if (Is_MKL_Op(n))  // Can this node accept an Mkl layout as input.</i></td></tr>
<tr><th id="145">145</th><td><i>//          then</i></td></tr>
<tr><th id="146">146</th><td><i>//            E = set of &lt;incoming edge and its src_output slot&gt; of n</i></td></tr>
<tr><th id="147">147</th><td><i>//            E' = {}   // a new set of edges for rewritten node</i></td></tr>
<tr><th id="148">148</th><td><i>//            foreach &lt;e,s&gt; in E</i></td></tr>
<tr><th id="149">149</th><td><i>//            do</i></td></tr>
<tr><th id="150">150</th><td><i>//              E' U {&lt;e,s&gt;}  // First copy edge which generates Tensorflow</i></td></tr>
<tr><th id="151">151</th><td><i>//                            // tensor as it is</i></td></tr>
<tr><th id="152">152</th><td><i>//              m = Source node of edge e</i></td></tr>
<tr><th id="153">153</th><td><i>//              if Is_Rewritten(m)  // Did we rewrite this node in this pass?</i></td></tr>
<tr><th id="154">154</th><td><i>//              then</i></td></tr>
<tr><th id="155">155</th><td><i>//                E' U {&lt;m,s+1&gt;}    // If yes, then m will generate an Mkl</i></td></tr>
<tr><th id="156">156</th><td><i>//                                  // tensor as an additional output.</i></td></tr>
<tr><th id="157">157</th><td><i>//              else</i></td></tr>
<tr><th id="158">158</th><td><i>//                d = Generate_Dummy_Mkl_Tensor()  // If not, generate a dummy</i></td></tr>
<tr><th id="159">159</th><td><i>//                                                 // Mkl tensor.</i></td></tr>
<tr><th id="160">160</th><td><i>//                E' U {&lt;d,0&gt;}  // The dummy Mkl tensor has only 1 output slot.</i></td></tr>
<tr><th id="161">161</th><td><i>//              fi</i></td></tr>
<tr><th id="162">162</th><td><i>//            done</i></td></tr>
<tr><th id="163">163</th><td><i>//            n' = Build_New_Node(G,new_name,E')</i></td></tr>
<tr><th id="164">164</th><td><i>//            Mark_Rewritten(n')  // Mark the new node as being rewritten.</i></td></tr>
<tr><th id="165">165</th><td><i>//          fi</i></td></tr>
<tr><th id="166">166</th><td><i>//        done</i></td></tr>
<tr><th id="167">167</th><td><i>//</i></td></tr>
<tr><th id="168">168</th><td><i>//      Explanation:</i></td></tr>
<tr><th id="169">169</th><td><i>//        For graph rewrite, we visit nodes of the input graph in the</i></td></tr>
<tr><th id="170">170</th><td><i>//        topological sort order. With this ordering, we visit nodes in the</i></td></tr>
<tr><th id="171">171</th><td><i>//        top-to-bottom fashion. We need this order because while visiting a</i></td></tr>
<tr><th id="172">172</th><td><i>//        node we want that all of its input nodes are visited and rewritten if</i></td></tr>
<tr><th id="173">173</th><td><i>//        applicable. This is because if we need to rewrite a given node</i></td></tr>
<tr><th id="174">174</th><td><i>//        then all of its input nodes need to be fixed (in other words they</i></td></tr>
<tr><th id="175">175</th><td><i>//        cannot be deleted later.)</i></td></tr>
<tr><th id="176">176</th><td><i>//</i></td></tr>
<tr><th id="177">177</th><td><i>//        While visiting a node, we first check if the op type of the node is</i></td></tr>
<tr><th id="178">178</th><td><i>//        an Mkl op. If it is, then we rewrite that node after constructing</i></td></tr>
<tr><th id="179">179</th><td><i>//        new inputs to the node. If the op type of the node is not Mkl op,</i></td></tr>
<tr><th id="180">180</th><td><i>//        then we do not rewrite that node.</i></td></tr>
<tr><th id="181">181</th><td><i>//</i></td></tr>
<tr><th id="182">182</th><td><i>// Handling workspace propagation for certain ops:</i></td></tr>
<tr><th id="183">183</th><td><i>//</i></td></tr>
<tr><th id="184">184</th><td><i>//        Certain backward ops in MKL (MaxPool, LRN and BatchNorm) require</i></td></tr>
<tr><th id="185">185</th><td><i>//        passing of a workspace from their respective forward ops. Workspace</i></td></tr>
<tr><th id="186">186</th><td><i>//        tensors provide memory for storing results of intermediate operations</i></td></tr>
<tr><th id="187">187</th><td><i>//        which are helpful in backward propagation. TensorFlow does not have</i></td></tr>
<tr><th id="188">188</th><td><i>//        a notion of a workspace and as a result does not allow producing</i></td></tr>
<tr><th id="189">189</th><td><i>//        additional outputs from these forward ops. For these ops, we need</i></td></tr>
<tr><th id="190">190</th><td><i>//        to add 2 extra edges between forward ops and their corresponding</i></td></tr>
<tr><th id="191">191</th><td><i>//        backward ops - the first extra edge carries a workspace tensor and</i></td></tr>
<tr><th id="192">192</th><td><i>//        the second one carries an Mkl tensor for the workspace tensor.</i></td></tr>
<tr><th id="193">193</th><td><i>//</i></td></tr>
<tr><th id="194">194</th><td><i>//        Example:</i></td></tr>
<tr><th id="195">195</th><td><i>//</i></td></tr>
<tr><th id="196">196</th><td><i>//        Typical graph for MaxPool and its gradient looks like:</i></td></tr>
<tr><th id="197">197</th><td><i>//</i></td></tr>
<tr><th id="198">198</th><td><i>//        A = MaxPool(T)</i></td></tr>
<tr><th id="199">199</th><td><i>//        B = MaxPoolGrad(X, A, Y)</i></td></tr>
<tr><th id="200">200</th><td><i>//</i></td></tr>
<tr><th id="201">201</th><td><i>//        We will transform this graph to propagate the workspace as:</i></td></tr>
<tr><th id="202">202</th><td><i>//        (with the contiguous ordering)</i></td></tr>
<tr><th id="203">203</th><td><i>//</i></td></tr>
<tr><th id="204">204</th><td><i>//        A, W, A_m, W_m = MklMaxPool(T, T_m)</i></td></tr>
<tr><th id="205">205</th><td><i>//        B, B_m = MklMaxPoolGrad(X, A, Y, W, X_m, A_m, Y_m, W_m)</i></td></tr>
<tr><th id="206">206</th><td><i>//</i></td></tr>
<tr><th id="207">207</th><td><i>//        Here W is the workspace tensor. Transformed tensor names with the</i></td></tr>
<tr><th id="208">208</th><td><i>//        suffix _m are Mkl tensors, and this transformation has been done</i></td></tr>
<tr><th id="209">209</th><td><i>//        using the algorithm discussed earlier. The transformation for</i></td></tr>
<tr><th id="210">210</th><td><i>//        workspace propagation only adds extra outputs (W, W_m) for a forward</i></td></tr>
<tr><th id="211">211</th><td><i>//        op and connects them to the corresponding backward ops.</i></td></tr>
<tr><th id="212">212</th><td><i>//</i></td></tr>
<tr><th id="213">213</th><td><i>//        Terms:</i></td></tr>
<tr><th id="214">214</th><td><i>//</i></td></tr>
<tr><th id="215">215</th><td><i>//        Forward op name = name of the op in the forward pass</i></td></tr>
<tr><th id="216">216</th><td><i>//          where a workspace tensor originates (MaxPool in this example)</i></td></tr>
<tr><th id="217">217</th><td><i>//        Backward op name = name of the op in the backward pass that receives</i></td></tr>
<tr><th id="218">218</th><td><i>//          a workspace tensor from the forward op (MaxPoolGrad in the example)</i></td></tr>
<tr><th id="219">219</th><td><i>//        Slot = Position of the output or input slot that will be</i></td></tr>
<tr><th id="220">220</th><td><i>//               used by the workspace tensor (1 for MklMaxPool as W is the 2nd</i></td></tr>
<tr><th id="221">221</th><td><i>//               output of MaxPool (0 is 1st); 3 for MklMaxPoolGrad)</i></td></tr>
<tr><th id="222">222</th><td><i>//</i></td></tr>
<tr><th id="223">223</th><td><i>//        Question:</i></td></tr>
<tr><th id="224">224</th><td><i>//</i></td></tr>
<tr><th id="225">225</th><td><i>//        How do we associate a backward op to a forward op? There can be more</i></td></tr>
<tr><th id="226">226</th><td><i>//        than one op with the exact same name.</i></td></tr>
<tr><th id="227">227</th><td><i>//</i></td></tr>
<tr><th id="228">228</th><td><i>//        In this example, we associate MaxPoolGrad with MaxPool. But there</i></td></tr>
<tr><th id="229">229</th><td><i>//        could be more than one MaxPool ops. To solve this problem, we look</i></td></tr>
<tr><th id="230">230</th><td><i>//        for _direct_ edge between a forward op and a backward op (tensor A is</i></td></tr>
<tr><th id="231">231</th><td><i>//        flowing along this edge in the example).</i></td></tr>
<tr><th id="232">232</th><td><i>//</i></td></tr>
<tr><th id="233">233</th><td><i>//        How do we transform forward and backward ops when there is no direct</i></td></tr>
<tr><th id="234">234</th><td><i>//        edge between them? In such a case, we generate dummy tensors for</i></td></tr>
<tr><th id="235">235</th><td><i>//        workspace tensors. For the example, transformation of MaxPool will</i></td></tr>
<tr><th id="236">236</th><td><i>//        be exactly same as it would be when there is a direct edge between</i></td></tr>
<tr><th id="237">237</th><td><i>//        the forward and the backward op --- it is just that MaxPool won't</i></td></tr>
<tr><th id="238">238</th><td><i>//        generate any workspace tensor. For MaxPoolGrad, the transformation</i></td></tr>
<tr><th id="239">239</th><td><i>//        will also be same, but instead of connecting W and W_m with the</i></td></tr>
<tr><th id="240">240</th><td><i>//        outputs of MaxPool, we will produce dummy tensors for them, and we</i></td></tr>
<tr><th id="241">241</th><td><i>//        will set workspace_enabled attribute to false.</i></td></tr>
<tr><th id="242">242</th><td><i>//</i></td></tr>
<tr><th id="243">243</th><td><i>// Example of B.2 : Context-based node rewrite</i></td></tr>
<tr><th id="244">244</th><td><i>// -------------------------------------------</i></td></tr>
<tr><th id="245">245</th><td><i>// Consider BiasAddGrad op as:</i></td></tr>
<tr><th id="246">246</th><td><i>//</i></td></tr>
<tr><th id="247">247</th><td><i>//           O = _MklConv2D(A, B, C, A_m, B_m, C_m)</i></td></tr>
<tr><th id="248">248</th><td><i>//           P = BiasAddGrad(O)</i></td></tr>
<tr><th id="249">249</th><td><i>//</i></td></tr>
<tr><th id="250">250</th><td><i>// Then we rewrite it as:</i></td></tr>
<tr><th id="251">251</th><td><i>//</i></td></tr>
<tr><th id="252">252</th><td><i>//           P = Conv2DWithBiasBackpropBias(O, O_m)</i></td></tr>
<tr><th id="253">253</th><td><i>//</i></td></tr>
<tr><th id="254">254</th><td><i>// Rewrite of BiasAddGrad into Conv2DWithBiasBackpropBias takes place depending</i></td></tr>
<tr><th id="255">255</th><td><i>// on the matching 'context'. The term context is loosely related to which</i></td></tr>
<tr><th id="256">256</th><td><i>// forward op is _associated_ to BiasAddGrad. If it is _MklConv2DWithBias then</i></td></tr>
<tr><th id="257">257</th><td><i>// we consider it Conv2D context; if it is MatMul, then it is MatMul context.</i></td></tr>
<tr><th id="258">258</th><td></td></tr>
<tr><th id="259">259</th><td><b>class</b> MklLayoutRewritePass : <b>public</b> GraphOptimizationPass {</td></tr>
<tr><th id="260">260</th><td> <b>public</b>:</td></tr>
<tr><th id="261">261</th><td>  MklLayoutRewritePass() {</td></tr>
<tr><th id="262">262</th><td>    <i>// NOTE: names are alphabetically sorted.</i></td></tr>
<tr><th id="263">263</th><td>    csinfo_.addn = <q>"AddN"</q>;</td></tr>
<tr><th id="264">264</th><td>    csinfo_.avg_pool = <q>"AvgPool"</q>;</td></tr>
<tr><th id="265">265</th><td>    csinfo_.avg_pool_grad = <q>"AvgPoolGrad"</q>;</td></tr>
<tr><th id="266">266</th><td>    csinfo_.bias_add = <q>"BiasAdd"</q>;</td></tr>
<tr><th id="267">267</th><td>    csinfo_.bias_add_grad = <q>"BiasAddGrad"</q>;</td></tr>
<tr><th id="268">268</th><td>    csinfo_.concat = <q>"Concat"</q>;</td></tr>
<tr><th id="269">269</th><td>    csinfo_.concatv2 = <q>"ConcatV2"</q>;</td></tr>
<tr><th id="270">270</th><td>    csinfo_.conv2d = <q>"Conv2D"</q>;</td></tr>
<tr><th id="271">271</th><td>    csinfo_.conv2d_grad_input = <q>"Conv2DBackpropInput"</q>;</td></tr>
<tr><th id="272">272</th><td>    csinfo_.conv2d_grad_filter = <q>"Conv2DBackpropFilter"</q>;</td></tr>
<tr><th id="273">273</th><td>    csinfo_.fused_batch_norm = <q>"FusedBatchNorm"</q>;</td></tr>
<tr><th id="274">274</th><td>    csinfo_.fused_batch_norm_grad = <q>"FusedBatchNormGrad"</q>;</td></tr>
<tr><th id="275">275</th><td>    csinfo_.identity = <q>"Identity"</q>;</td></tr>
<tr><th id="276">276</th><td>    csinfo_.lrn = <q>"LRN"</q>;</td></tr>
<tr><th id="277">277</th><td>    csinfo_.lrn_grad = <q>"LRNGrad"</q>;</td></tr>
<tr><th id="278">278</th><td>    csinfo_.matmul = <q>"MatMul"</q>;</td></tr>
<tr><th id="279">279</th><td>    csinfo_.max_pool = <q>"MaxPool"</q>;</td></tr>
<tr><th id="280">280</th><td>    csinfo_.max_pool_grad = <q>"MaxPoolGrad"</q>;</td></tr>
<tr><th id="281">281</th><td>    csinfo_.mkl_conv2d = <q>"_MklConv2D"</q>;</td></tr>
<tr><th id="282">282</th><td>    csinfo_.mkl_conv2d_grad_input = <q>"_MklConv2DBackpropInput"</q>;</td></tr>
<tr><th id="283">283</th><td>    csinfo_.mkl_conv2d_grad_filter = <q>"_MklConv2DBackpropFilter"</q>;</td></tr>
<tr><th id="284">284</th><td>    csinfo_.mkl_conv2d_with_bias = <q>"_MklConv2DWithBias"</q>;</td></tr>
<tr><th id="285">285</th><td>    csinfo_.mkl_conv2d_with_bias_backprop_bias =</td></tr>
<tr><th id="286">286</th><td>        <q>"_MklConv2DWithBiasBackpropBias"</q>;</td></tr>
<tr><th id="287">287</th><td>    csinfo_.relu = <q>"Relu"</q>;</td></tr>
<tr><th id="288">288</th><td>    csinfo_.relu_grad = <q>"ReluGrad"</q>;</td></tr>
<tr><th id="289">289</th><td>    csinfo_.reshape = <q>"Reshape"</q>;</td></tr>
<tr><th id="290">290</th><td>    csinfo_.split = <q>"Split"</q>;</td></tr>
<tr><th id="291">291</th><td>    <i>// Element-wise ops. Ensure you also add any new ops to IsOpElementWise</i></td></tr>
<tr><th id="292">292</th><td><i>    // in the MklUtil.h (IsMklElementWiseOp method) to ensure that the</i></td></tr>
<tr><th id="293">293</th><td><i>    // MklInputConversion op is added before it.</i></td></tr>
<tr><th id="294">294</th><td>    csinfo_.add = <q>"Add"</q>;</td></tr>
<tr><th id="295">295</th><td>    csinfo_.maximum = <q>"Maximum"</q>;</td></tr>
<tr><th id="296">296</th><td>    csinfo_.mul = <q>"Mul"</q>;</td></tr>
<tr><th id="297">297</th><td>    csinfo_.squared_difference = <q>"SquaredDifference"</q>;</td></tr>
<tr><th id="298">298</th><td>    csinfo_.sub = <q>"Sub"</q>;</td></tr>
<tr><th id="299">299</th><td>    <i>// End - element-wise ops. See note above.</i></td></tr>
<tr><th id="300">300</th><td><i></i></td></tr>
<tr><th id="301">301</th><td><i>    // NOTE: names are alphabetically sorted.</i></td></tr>
<tr><th id="302">302</th><td>    rinfo_.push_back({csinfo_.addn, mkl_op_registry::GetMklOpName(csinfo_.addn),</td></tr>
<tr><th id="303">303</th><td>                      CopyAttrsAddN, AddNRewrite, <b>nullptr</b>});</td></tr>
<tr><th id="304">304</th><td>    rinfo_.push_back({csinfo_.add, mkl_op_registry::GetMklOpName(csinfo_.add),</td></tr>
<tr><th id="305">305</th><td>                      CopyAttrsDataType, AlwaysRewrite, <b>nullptr</b>});</td></tr>
<tr><th id="306">306</th><td>    rinfo_.push_back({csinfo_.avg_pool,</td></tr>
<tr><th id="307">307</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.avg_pool),</td></tr>
<tr><th id="308">308</th><td>                      CopyAttrsPooling, AlwaysRewrite, <b>nullptr</b>});</td></tr>
<tr><th id="309">309</th><td>    rinfo_.push_back({csinfo_.avg_pool_grad,</td></tr>
<tr><th id="310">310</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.avg_pool_grad),</td></tr>
<tr><th id="311">311</th><td>                      CopyAttrsPooling, AlwaysRewrite, <b>nullptr</b>});</td></tr>
<tr><th id="312">312</th><td>    <i>// BiasAddGrad gets written into Conv2DWithBiasBackpropBias depending</i></td></tr>
<tr><th id="313">313</th><td><i>    // on if context contains Conv2D.</i></td></tr>
<tr><th id="314">314</th><td>    rinfo_.push_back({csinfo_.bias_add_grad,</td></tr>
<tr><th id="315">315</th><td>                      csinfo_.mkl_conv2d_with_bias_backprop_bias,</td></tr>
<tr><th id="316">316</th><td>                      CopyAttrsBiasAddGrad, ContextMatchRewrite,</td></tr>
<tr><th id="317">317</th><td>                      &amp;biasaddgrad_conv2dwithbias_context_});</td></tr>
<tr><th id="318">318</th><td>    <i>// BiasAddGrad gets written into BiasAddGrad depending on if context</i></td></tr>
<tr><th id="319">319</th><td><i>    // contains MatMul.</i></td></tr>
<tr><th id="320">320</th><td>    rinfo_.push_back({csinfo_.bias_add_grad, csinfo_.matmul,</td></tr>
<tr><th id="321">321</th><td>                      CopyAttrsBiasAddGrad, ContextMatchRewrite,</td></tr>
<tr><th id="322">322</th><td>                      &amp;biasaddgrad_matmul_context_});</td></tr>
<tr><th id="323">323</th><td>    rinfo_.push_back({csinfo_.concat,</td></tr>
<tr><th id="324">324</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.concat),</td></tr>
<tr><th id="325">325</th><td>                      CopyAttrsConcat, AlwaysRewrite, <b>nullptr</b>});</td></tr>
<tr><th id="326">326</th><td>    rinfo_.push_back({csinfo_.concatv2,</td></tr>
<tr><th id="327">327</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.concatv2),</td></tr>
<tr><th id="328">328</th><td>                      CopyAttrsConcatV2, AlwaysRewrite, <b>nullptr</b>});</td></tr>
<tr><th id="329">329</th><td>    rinfo_.push_back({csinfo_.conv2d,</td></tr>
<tr><th id="330">330</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.conv2d),</td></tr>
<tr><th id="331">331</th><td>                      CopyAttrsConv2D, AlwaysRewrite, <b>nullptr</b>});</td></tr>
<tr><th id="332">332</th><td>    rinfo_.push_back({csinfo_.conv2d_grad_filter,</td></tr>
<tr><th id="333">333</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.conv2d_grad_filter),</td></tr>
<tr><th id="334">334</th><td>                      CopyAttrsConv2D, AlwaysRewrite, <b>nullptr</b>});</td></tr>
<tr><th id="335">335</th><td>    rinfo_.push_back({csinfo_.conv2d_grad_input,</td></tr>
<tr><th id="336">336</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.conv2d_grad_input),</td></tr>
<tr><th id="337">337</th><td>                      CopyAttrsConv2D, AlwaysRewrite, <b>nullptr</b>});</td></tr>
<tr><th id="338">338</th><td>    rinfo_.push_back({csinfo_.fused_batch_norm,</td></tr>
<tr><th id="339">339</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.fused_batch_norm),</td></tr>
<tr><th id="340">340</th><td>                      CopyAttrsFusedBatchNorm, AlwaysRewrite, <b>nullptr</b>});</td></tr>
<tr><th id="341">341</th><td>    rinfo_.push_back(</td></tr>
<tr><th id="342">342</th><td>        {csinfo_.fused_batch_norm_grad,</td></tr>
<tr><th id="343">343</th><td>         mkl_op_registry::GetMklOpName(csinfo_.fused_batch_norm_grad),</td></tr>
<tr><th id="344">344</th><td>         CopyAttrsFusedBatchNorm, AlwaysRewrite, <b>nullptr</b>});</td></tr>
<tr><th id="345">345</th><td>    rinfo_.push_back({csinfo_.identity,</td></tr>
<tr><th id="346">346</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.identity),</td></tr>
<tr><th id="347">347</th><td>                      CopyAttrsIdentity, AlwaysRewrite, <b>nullptr</b>});</td></tr>
<tr><th id="348">348</th><td>    rinfo_.push_back({csinfo_.lrn, mkl_op_registry::GetMklOpName(csinfo_.lrn),</td></tr>
<tr><th id="349">349</th><td>                      CopyAttrsLRN, AlwaysRewrite, <b>nullptr</b>});</td></tr>
<tr><th id="350">350</th><td>    rinfo_.push_back({csinfo_.lrn_grad,</td></tr>
<tr><th id="351">351</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.lrn_grad),</td></tr>
<tr><th id="352">352</th><td>                      CopyAttrsLRN, AlwaysRewrite, <b>nullptr</b>});</td></tr>
<tr><th id="353">353</th><td>    rinfo_.push_back({csinfo_.max_pool,</td></tr>
<tr><th id="354">354</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.max_pool),</td></tr>
<tr><th id="355">355</th><td>                      CopyAttrsPooling, NonDepthBatchWisePoolRewrite, <b>nullptr</b>});</td></tr>
<tr><th id="356">356</th><td>    rinfo_.push_back({csinfo_.max_pool_grad,</td></tr>
<tr><th id="357">357</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.max_pool_grad),</td></tr>
<tr><th id="358">358</th><td>                      CopyAttrsPooling, AlwaysRewrite, <b>nullptr</b>});</td></tr>
<tr><th id="359">359</th><td>    rinfo_.push_back({csinfo_.maximum,</td></tr>
<tr><th id="360">360</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.maximum),</td></tr>
<tr><th id="361">361</th><td>                      CopyAttrsDataType, AlwaysRewrite, <b>nullptr</b>});</td></tr>
<tr><th id="362">362</th><td>    rinfo_.push_back({csinfo_.mul, mkl_op_registry::GetMklOpName(csinfo_.mul),</td></tr>
<tr><th id="363">363</th><td>                      CopyAttrsDataType, AlwaysRewrite, <b>nullptr</b>});</td></tr>
<tr><th id="364">364</th><td>    rinfo_.push_back({csinfo_.relu, mkl_op_registry::GetMklOpName(csinfo_.relu),</td></tr>
<tr><th id="365">365</th><td>                      CopyAttrsDataType, AlwaysRewrite, <b>nullptr</b>});</td></tr>
<tr><th id="366">366</th><td>    rinfo_.push_back({csinfo_.relu_grad,</td></tr>
<tr><th id="367">367</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.relu_grad),</td></tr>
<tr><th id="368">368</th><td>                      CopyAttrsDataType, AlwaysRewrite, <b>nullptr</b>});</td></tr>
<tr><th id="369">369</th><td>    rinfo_.push_back({csinfo_.reshape,</td></tr>
<tr><th id="370">370</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.reshape),</td></tr>
<tr><th id="371">371</th><td>                      CopyAttrsReshape, AlwaysRewrite, <b>nullptr</b>});</td></tr>
<tr><th id="372">372</th><td>    rinfo_.push_back({csinfo_.squared_difference,</td></tr>
<tr><th id="373">373</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.squared_difference),</td></tr>
<tr><th id="374">374</th><td>                      CopyAttrsDataType, AlwaysRewrite, <b>nullptr</b>});</td></tr>
<tr><th id="375">375</th><td>    rinfo_.push_back({csinfo_.sub, mkl_op_registry::GetMklOpName(csinfo_.sub),</td></tr>
<tr><th id="376">376</th><td>                      CopyAttrsDataType, AlwaysRewrite, <b>nullptr</b>});</td></tr>
<tr><th id="377">377</th><td></td></tr>
<tr><th id="378">378</th><td>    <i>// Add info about which ops to add workspace edge to and the slots.</i></td></tr>
<tr><th id="379">379</th><td>    wsinfo_.push_back({csinfo_.lrn, csinfo_.lrn_grad, <var>0</var>, <var>2</var>, <var>1</var>, <var>3</var>});</td></tr>
<tr><th id="380">380</th><td>    wsinfo_.push_back({csinfo_.max_pool, csinfo_.max_pool_grad, <var>0</var>, <var>1</var>, <var>1</var>, <var>3</var>});</td></tr>
<tr><th id="381">381</th><td></td></tr>
<tr><th id="382">382</th><td>    <i>// Add a rule for merging nodes</i></td></tr>
<tr><th id="383">383</th><td>    minfo_.push_back({csinfo_.mkl_conv2d, csinfo_.bias_add, <var>0</var>,</td></tr>
<tr><th id="384">384</th><td>                      csinfo_.mkl_conv2d_with_bias});</td></tr>
<tr><th id="385">385</th><td></td></tr>
<tr><th id="386">386</th><td>    biasaddgrad_matmul_context_ = {csinfo_.bias_add_grad, csinfo_.matmul,</td></tr>
<tr><th id="387">387</th><td>                                   IsBiasAddGradInMatMulContext};</td></tr>
<tr><th id="388">388</th><td></td></tr>
<tr><th id="389">389</th><td>    biasaddgrad_conv2dwithbias_context_ = {</td></tr>
<tr><th id="390">390</th><td>        csinfo_.bias_add_grad, csinfo_.mkl_conv2d_with_bias,</td></tr>
<tr><th id="391">391</th><td>        IsBiasAddGradInConv2DWithBiasContext};</td></tr>
<tr><th id="392">392</th><td></td></tr>
<tr><th id="393">393</th><td>    cinfo_.push_back(&amp;biasaddgrad_matmul_context_);</td></tr>
<tr><th id="394">394</th><td>    cinfo_.push_back(&amp;biasaddgrad_conv2dwithbias_context_);</td></tr>
<tr><th id="395">395</th><td>  }</td></tr>
<tr><th id="396">396</th><td></td></tr>
<tr><th id="397">397</th><td>  <i>// Standard interface to run pass</i></td></tr>
<tr><th id="398">398</th><td>  Status Run(<em>const</em> GraphOptimizationPassOptions&amp; options);</td></tr>
<tr><th id="399">399</th><td></td></tr>
<tr><th id="400">400</th><td>  <i>// Helper function which does most of heavy lifting for rewriting</i></td></tr>
<tr><th id="401">401</th><td><i>  // Mkl nodes to propagate Mkl tensor as additional output</i></td></tr>
<tr><th id="402">402</th><td><i>  //</i></td></tr>
<tr><th id="403">403</th><td><i>  // Extracts common functionality between Run public interface and</i></td></tr>
<tr><th id="404">404</th><td><i>  // test interface.</i></td></tr>
<tr><th id="405">405</th><td><i>  //</i></td></tr>
<tr><th id="406">406</th><td><i>  // @return true, if and only if graph is mutated; false otherwise.</i></td></tr>
<tr><th id="407">407</th><td>  <em>bool</em> RunPass(std::unique_ptr&lt;Graph&gt;* g);</td></tr>
<tr><th id="408">408</th><td></td></tr>
<tr><th id="409">409</th><td>  <i class="doc">/// Structure to specify the context information used in a node rewrite rule</i></td></tr>
<tr><th id="410">410</th><td>  <b>typedef</b> <b>struct</b> {</td></tr>
<tr><th id="411">411</th><td>    string node;  <i>// Name of the node to be rewritten</i></td></tr>
<tr><th id="412">412</th><td>    string fwd;   <i>// Name of the node in the forward pass that this node</i></td></tr>
<tr><th id="413">413</th><td>                  <i>// corresponds to</i></td></tr>
<tr><th id="414">414</th><td>    std::function&lt;<em>bool</em>(<em>const</em> Node*, <em>const</em> Node**, <em>void</em>* c)&gt; context_match_fn;</td></tr>
<tr><th id="415">415</th><td>  } ContextInfo;</td></tr>
<tr><th id="416">416</th><td></td></tr>
<tr><th id="417">417</th><td>  <i class="doc">/// Structure to specify the name of an original node, its new name after</i></td></tr>
<tr><th id="418">418</th><td><i class="doc">  /// rewrite, the number of inputs to the original node, the function to</i></td></tr>
<tr><th id="419">419</th><td><i class="doc">  /// be used to copy attributes for the op, and the rule (if any) which</i></td></tr>
<tr><th id="420">420</th><td><i class="doc">  /// must hold for rewriting the node</i></td></tr>
<tr><th id="421">421</th><td>  <b>typedef</b> <b>struct</b> {</td></tr>
<tr><th id="422">422</th><td>    string name;      <i>// Original name of op of the node in the graph</i></td></tr>
<tr><th id="423">423</th><td>    string new_name;  <i>// New name of the op of the node in the graph</i></td></tr>
<tr><th id="424">424</th><td>    <i>// A function handler to copy attributes from an old node to a new node.</i></td></tr>
<tr><th id="425">425</th><td>    std::function&lt;<em>void</em>(<em>const</em> Node*, NodeBuilder*)&gt; copy_attrs;</td></tr>
<tr><th id="426">426</th><td>    <i>// A rule under which to rewrite this node</i></td></tr>
<tr><th id="427">427</th><td>    std::function&lt;<em>bool</em>(<em>const</em> Node*, <em>const</em> ContextInfo* c)&gt; rewrite_rule;</td></tr>
<tr><th id="428">428</th><td>    <i>// ContextInfo, if any, to be used for rewrite</i></td></tr>
<tr><th id="429">429</th><td>    ContextInfo* context;</td></tr>
<tr><th id="430">430</th><td>  } RewriteInfo;</td></tr>
<tr><th id="431">431</th><td></td></tr>
<tr><th id="432">432</th><td>  <i class="doc">/// Structure to specify a forward op, a backward op, and the slot numbers</i></td></tr>
<tr><th id="433">433</th><td><i class="doc">  /// in the forward and backward ops where we will add a workspace edge.</i></td></tr>
<tr><th id="434">434</th><td>  <b>typedef</b> <b>struct</b> {</td></tr>
<tr><th id="435">435</th><td>    string fwd_op;    <i>// Name of a forward op in the graph</i></td></tr>
<tr><th id="436">436</th><td>    string bwd_op;    <i>// Name of a backward op in the graph</i></td></tr>
<tr><th id="437">437</th><td>    <em>int</em> fwd_slot;     <i>// Output slot in the forward op node where actual</i></td></tr>
<tr><th id="438">438</th><td>                      <i>// output tensor resides</i></td></tr>
<tr><th id="439">439</th><td>    <em>int</em> bwd_slot;     <i>// Input slot in the backward op node where actual</i></td></tr>
<tr><th id="440">440</th><td>                      <i>// input tensor resides</i></td></tr>
<tr><th id="441">441</th><td>    <em>int</em> ws_fwd_slot;  <i>// Output slot in the forward op node where workspace</i></td></tr>
<tr><th id="442">442</th><td>                      <i>// edge is added</i></td></tr>
<tr><th id="443">443</th><td>    <em>int</em> ws_bwd_slot;  <i>// Input slot in the backward op node where workspace</i></td></tr>
<tr><th id="444">444</th><td>                      <i>// edge is added</i></td></tr>
<tr><th id="445">445</th><td>  } WorkSpaceInfo;</td></tr>
<tr><th id="446">446</th><td></td></tr>
<tr><th id="447">447</th><td>  <i class="doc">/// Structure to specify information used in node merge</i></td></tr>
<tr><th id="448">448</th><td>  <b>typedef</b> <b>struct</b> {</td></tr>
<tr><th id="449">449</th><td>    string pred;      <i>// Predecessor node string</i></td></tr>
<tr><th id="450">450</th><td>    string succ;      <i>// Successor node string</i></td></tr>
<tr><th id="451">451</th><td>    <em>int</em> op;           <i>// The operand no the predecessor node corresponds</i></td></tr>
<tr><th id="452">452</th><td>                      <i>// to the successor node</i></td></tr>
<tr><th id="453">453</th><td>    string new_node;  <i>// Name of the node after merge</i></td></tr>
<tr><th id="454">454</th><td>  } MergeInfo;</td></tr>
<tr><th id="455">455</th><td></td></tr>
<tr><th id="456">456</th><td>  <i class="doc">/// Structure to store all constant strings</i></td></tr>
<tr><th id="457">457</th><td><i class="doc">  /// NOTE: names are alphabetically sorted.</i></td></tr>
<tr><th id="458">458</th><td>  <b>typedef</b> <b>struct</b> {</td></tr>
<tr><th id="459">459</th><td>    string addn;</td></tr>
<tr><th id="460">460</th><td>    string add;</td></tr>
<tr><th id="461">461</th><td>    string avg_pool;</td></tr>
<tr><th id="462">462</th><td>    string avg_pool_grad;</td></tr>
<tr><th id="463">463</th><td>    string bias_add;</td></tr>
<tr><th id="464">464</th><td>    string bias_add_grad;</td></tr>
<tr><th id="465">465</th><td>    string concat;</td></tr>
<tr><th id="466">466</th><td>    string concatv2;</td></tr>
<tr><th id="467">467</th><td>    string conv2d;</td></tr>
<tr><th id="468">468</th><td>    string conv2d_grad_input;</td></tr>
<tr><th id="469">469</th><td>    string conv2d_grad_filter;</td></tr>
<tr><th id="470">470</th><td>    string fused_batch_norm;</td></tr>
<tr><th id="471">471</th><td>    string fused_batch_norm_grad;</td></tr>
<tr><th id="472">472</th><td>    string identity;</td></tr>
<tr><th id="473">473</th><td>    string lrn;</td></tr>
<tr><th id="474">474</th><td>    string lrn_grad;</td></tr>
<tr><th id="475">475</th><td>    string matmul;</td></tr>
<tr><th id="476">476</th><td>    string max_pool;</td></tr>
<tr><th id="477">477</th><td>    string max_pool_grad;</td></tr>
<tr><th id="478">478</th><td>    string maximum;</td></tr>
<tr><th id="479">479</th><td>    string mkl_conv2d;</td></tr>
<tr><th id="480">480</th><td>    string mkl_conv2d_grad_input;</td></tr>
<tr><th id="481">481</th><td>    string mkl_conv2d_grad_filter;</td></tr>
<tr><th id="482">482</th><td>    string mkl_conv2d_with_bias;</td></tr>
<tr><th id="483">483</th><td>    string mkl_conv2d_with_bias_backprop_bias;</td></tr>
<tr><th id="484">484</th><td>    string mul;</td></tr>
<tr><th id="485">485</th><td>    string relu;</td></tr>
<tr><th id="486">486</th><td>    string relu_grad;</td></tr>
<tr><th id="487">487</th><td>    string reshape;</td></tr>
<tr><th id="488">488</th><td>    string split;</td></tr>
<tr><th id="489">489</th><td>    string squared_difference;</td></tr>
<tr><th id="490">490</th><td>    string sub;</td></tr>
<tr><th id="491">491</th><td>  } ConstStringsInfo;</td></tr>
<tr><th id="492">492</th><td></td></tr>
<tr><th id="493">493</th><td> <b>private</b>:</td></tr>
<tr><th id="494">494</th><td>  <i class="doc">/// Maintain info about nodes to rewrite</i></td></tr>
<tr><th id="495">495</th><td>  std::vector&lt;RewriteInfo&gt; rinfo_;</td></tr>
<tr><th id="496">496</th><td></td></tr>
<tr><th id="497">497</th><td>  <i class="doc">/// Maintain info about nodes to add workspace edge</i></td></tr>
<tr><th id="498">498</th><td>  std::vector&lt;WorkSpaceInfo&gt; wsinfo_;</td></tr>
<tr><th id="499">499</th><td></td></tr>
<tr><th id="500">500</th><td>  <i class="doc">/// Maintain info about nodes to be merged</i></td></tr>
<tr><th id="501">501</th><td>  std::vector&lt;MergeInfo&gt; minfo_;</td></tr>
<tr><th id="502">502</th><td></td></tr>
<tr><th id="503">503</th><td>  <i class="doc">/// Maintain info about nodes to rewrite</i></td></tr>
<tr><th id="504">504</th><td>  <em>static</em> std::vector&lt;ContextInfo*&gt; cinfo_;</td></tr>
<tr><th id="505">505</th><td></td></tr>
<tr><th id="506">506</th><td>  <i class="doc">/// Maintain structure of constant strings</i></td></tr>
<tr><th id="507">507</th><td>  <em>static</em> ConstStringsInfo csinfo_;</td></tr>
<tr><th id="508">508</th><td></td></tr>
<tr><th id="509">509</th><td>  <i class="doc">/// Context variables used in referencing rules</i></td></tr>
<tr><th id="510">510</th><td>  <em>static</em> ContextInfo biasaddgrad_matmul_context_;</td></tr>
<tr><th id="511">511</th><td>  <em>static</em> ContextInfo biasaddgrad_conv2dwithbias_context_;</td></tr>
<tr><th id="512">512</th><td></td></tr>
<tr><th id="513">513</th><td> <b>private</b>:</td></tr>
<tr><th id="514">514</th><td>  <i>// Is OpDef::ArgDef a list type? It could be N * T or list(type).</i></td></tr>
<tr><th id="515">515</th><td><i>  // Refer to opdef.proto for details of list type.</i></td></tr>
<tr><th id="516">516</th><td>  <b>inline</b> <em>bool</em> ArgIsList(<em>const</em> OpDef::ArgDef&amp; arg) <em>const</em> {</td></tr>
<tr><th id="517">517</th><td>    <b>return</b> !arg.type_list_attr().empty() || !arg.number_attr().empty();</td></tr>
<tr><th id="518">518</th><td>  }</td></tr>
<tr><th id="519">519</th><td></td></tr>
<tr><th id="520">520</th><td>  <i>// Get length of a list in 'n' if 'arg' is of list type. Refer to</i></td></tr>
<tr><th id="521">521</th><td><i>  // description of ArgIsList for definition of list type.</i></td></tr>
<tr><th id="522">522</th><td>  <b>inline</b> <em>int</em> GetTensorListLength(<em>const</em> OpDef::ArgDef&amp; arg, Node* n) {</td></tr>
<tr><th id="523">523</th><td>    CHECK_EQ(ArgIsList(arg), <b>true</b>);</td></tr>
<tr><th id="524">524</th><td>    <em>int</em> N = <var>0</var>;</td></tr>
<tr><th id="525">525</th><td>    <em>const</em> string attr_name = !arg.type_list_attr().empty()</td></tr>
<tr><th id="526">526</th><td>                                 ? arg.type_list_attr()</td></tr>
<tr><th id="527">527</th><td>                                 : arg.number_attr();</td></tr>
<tr><th id="528">528</th><td>    <b>if</b> (!arg.type_list_attr().empty()) {</td></tr>
<tr><th id="529">529</th><td>      std::vector&lt;DataType&gt; value;</td></tr>
<tr><th id="530">530</th><td>      TF_CHECK_OK(GetNodeAttr(n-&gt;def(), attr_name, &amp;value));</td></tr>
<tr><th id="531">531</th><td>      N = value.size();</td></tr>
<tr><th id="532">532</th><td>    } <b>else</b> {</td></tr>
<tr><th id="533">533</th><td>      TF_CHECK_OK(GetNodeAttr(n-&gt;def(), attr_name, &amp;N));</td></tr>
<tr><th id="534">534</th><td>    }</td></tr>
<tr><th id="535">535</th><td>    <b>return</b> N;</td></tr>
<tr><th id="536">536</th><td>  }</td></tr>
<tr><th id="537">537</th><td></td></tr>
<tr><th id="538">538</th><td>  <i>// Can op represented by node 'n' run on DEVICE_CPU?</i></td></tr>
<tr><th id="539">539</th><td><i>  // Op can run on CPU with MKL if the runtime assigned device or the</i></td></tr>
<tr><th id="540">540</th><td><i>  // user requested device contains device CPU, or both are empty.</i></td></tr>
<tr><th id="541">541</th><td>  <em>bool</em> CanOpRunOnCPUDevice(<em>const</em> Node* n) {</td></tr>
<tr><th id="542">542</th><td>    <em>bool</em> result = <b>true</b>;</td></tr>
<tr><th id="543">543</th><td>    string reason;</td></tr>
<tr><th id="544">544</th><td></td></tr>
<tr><th id="545">545</th><td>    <i>// Substring that should be checked for in device name for CPU device.</i></td></tr>
<tr><th id="546">546</th><td>    <em>const</em> <em>char</em>* <em>const</em> kCPUDeviceSubStr = <q>"CPU"</q>;</td></tr>
<tr><th id="547">547</th><td></td></tr>
<tr><th id="548">548</th><td>    <i>// If Op has been specifically assigned to a non-CPU device, then No.</i></td></tr>
<tr><th id="549">549</th><td>    <b>if</b> (!n-&gt;assigned_device_name().empty() &amp;&amp;</td></tr>
<tr><th id="550">550</th><td>        !StringPiece(n-&gt;assigned_device_name()).contains(kCPUDeviceSubStr)) {</td></tr>
<tr><th id="551">551</th><td>      result = <b>false</b>;</td></tr>
<tr><th id="552">552</th><td>      reason = <q>"Op has been assigned a runtime device that is not CPU."</q>;</td></tr>
<tr><th id="553">553</th><td>    }</td></tr>
<tr><th id="554">554</th><td></td></tr>
<tr><th id="555">555</th><td>    <i>// If user has specifically assigned this op to a non-CPU device, then No.</i></td></tr>
<tr><th id="556">556</th><td>    <b>if</b> (!n-&gt;def().device().empty() &amp;&amp;</td></tr>
<tr><th id="557">557</th><td>        !StringPiece(n-&gt;def().device()).contains(kCPUDeviceSubStr)) {</td></tr>
<tr><th id="558">558</th><td>      result = <b>false</b>;</td></tr>
<tr><th id="559">559</th><td>      reason = <q>"User has assigned a device that is not CPU."</q>;</td></tr>
<tr><th id="560">560</th><td>    }</td></tr>
<tr><th id="561">561</th><td></td></tr>
<tr><th id="562">562</th><td>    <b>if</b> (result == <b>false</b>) {</td></tr>
<tr><th id="563">563</th><td>      VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: Skipping rewriting of the node "</q></td></tr>
<tr><th id="564">564</th><td>              &lt;&lt; n-&gt;type_string() &lt;&lt; <q>", reason: "</q> &lt;&lt; reason;</td></tr>
<tr><th id="565">565</th><td>    }</td></tr>
<tr><th id="566">566</th><td></td></tr>
<tr><th id="567">567</th><td>    <i>// Otherwise Yes.</i></td></tr>
<tr><th id="568">568</th><td>    <b>return</b> result;</td></tr>
<tr><th id="569">569</th><td>  }</td></tr>
<tr><th id="570">570</th><td></td></tr>
<tr><th id="571">571</th><td>  <i>// Return a node that can be merged with input node 'n'</i></td></tr>
<tr><th id="572">572</th><td><i>  //</i></td></tr>
<tr><th id="573">573</th><td><i>  // @return pointer to the node if we can find such a</i></td></tr>
<tr><th id="574">574</th><td><i>  // node. Otherwise, it returns nullptr.</i></td></tr>
<tr><th id="575">575</th><td>  Node* CheckForNodeMerge(<em>const</em> Node* n) <em>const</em>;</td></tr>
<tr><th id="576">576</th><td></td></tr>
<tr><th id="577">577</th><td>  <i>// Merge predecessor node with its successor.</i></td></tr>
<tr><th id="578">578</th><td><i>  // Currently, we merge Conv2D with BiasAdd only.</i></td></tr>
<tr><th id="579">579</th><td><i>  //</i></td></tr>
<tr><th id="580">580</th><td><i>  // Input nodes succ and pred may be deleted if the call to</i></td></tr>
<tr><th id="581">581</th><td><i>  // this function is successful. Attempt to use the pointers</i></td></tr>
<tr><th id="582">582</th><td><i>  // after the call to function may result in undefined behaviors.</i></td></tr>
<tr><th id="583">583</th><td><i>  //</i></td></tr>
<tr><th id="584">584</th><td><i>  // @input g - input graph, succ - successor node, pred - predecessor node</i></td></tr>
<tr><th id="585">585</th><td><i>  // @return Status::OK(), if merging is successful and supported.</i></td></tr>
<tr><th id="586">586</th><td><i>  //         Returns appropriate Status error code otherwise.</i></td></tr>
<tr><th id="587">587</th><td><i>  //         Graph is updated in case nodes are merged. Otherwise, it is</i></td></tr>
<tr><th id="588">588</th><td><i>  //         not updated.</i></td></tr>
<tr><th id="589">589</th><td>  Status MergeNode(std::unique_ptr&lt;Graph&gt;* g, Node* succ, Node* pred);</td></tr>
<tr><th id="590">590</th><td></td></tr>
<tr><th id="591">591</th><td>  <i>// Check if the node 'n' has any applicable rewrite rule</i></td></tr>
<tr><th id="592">592</th><td><i>  // We check for 2 scenarios for rewrite.</i></td></tr>
<tr><th id="593">593</th><td><i>  //</i></td></tr>
<tr><th id="594">594</th><td><i>  // @return RewriteInfo* for the applicable rewrite rule</i></td></tr>
<tr><th id="595">595</th><td>  <em>const</em> RewriteInfo* CheckForNodeRewrite(<em>const</em> Node* n) <em>const</em>;</td></tr>
<tr><th id="596">596</th><td></td></tr>
<tr><th id="597">597</th><td>  <i>// Default rewrite rule to be used in scenario 1 for rewrite.</i></td></tr>
<tr><th id="598">598</th><td><i>  // @return - true (since we want to always rewrite)</i></td></tr>
<tr><th id="599">599</th><td>  <em>static</em> <em>bool</em> AlwaysRewrite(<em>const</em> Node* n, <em>const</em> ContextInfo* c = <b>nullptr</b>) {</td></tr>
<tr><th id="600">600</th><td>    <b>return</b> <b>true</b>;</td></tr>
<tr><th id="601">601</th><td>  }</td></tr>
<tr><th id="602">602</th><td></td></tr>
<tr><th id="603">603</th><td>  <i>// Check if we are performing pooling on depth or batch. If it is, then we</i></td></tr>
<tr><th id="604">604</th><td><i>  // do not rewrite MaxPool node to Mkl version.</i></td></tr>
<tr><th id="605">605</th><td><i>  // @return - true (if it is not a depth/batch wise pooling case);</i></td></tr>
<tr><th id="606">606</th><td><i>  //           false otherwise.</i></td></tr>
<tr><th id="607">607</th><td>  <em>static</em> <em>bool</em> NonDepthBatchWisePoolRewrite(<em>const</em> Node* n,</td></tr>
<tr><th id="608">608</th><td>                                           <em>const</em> ContextInfo* c) {</td></tr>
<tr><th id="609">609</th><td>    CHECK_NOTNULL(n);</td></tr>
<tr><th id="610">610</th><td></td></tr>
<tr><th id="611">611</th><td>    string data_format_str;</td></tr>
<tr><th id="612">612</th><td>    TensorFormat data_format;</td></tr>
<tr><th id="613">613</th><td>    std::vector&lt;int32&gt; ksize, strides;</td></tr>
<tr><th id="614">614</th><td>    CHECK_EQ(GetNodeAttr(n-&gt;def(), <q>"ksize"</q>, &amp;ksize).ok(), <b>true</b>);</td></tr>
<tr><th id="615">615</th><td>    CHECK_EQ(GetNodeAttr(n-&gt;def(), <q>"strides"</q>, &amp;strides).ok(), <b>true</b>);</td></tr>
<tr><th id="616">616</th><td>    CHECK_EQ(GetNodeAttr(n-&gt;def(), <q>"data_format"</q>, &amp;data_format_str).ok(), <b>true</b>);</td></tr>
<tr><th id="617">617</th><td>    CHECK_EQ(FormatFromString(data_format_str, &amp;data_format), <b>true</b>);</td></tr>
<tr><th id="618">618</th><td></td></tr>
<tr><th id="619">619</th><td>    <i>// Condition that specifies non-batch-wise and non-depth-wise pooling.</i></td></tr>
<tr><th id="620">620</th><td>    <b>if</b> (GetTensorDim(ksize, data_format, <kbd>'N'</kbd>) == <var>1</var> &amp;&amp;</td></tr>
<tr><th id="621">621</th><td>        GetTensorDim(strides, data_format, <kbd>'N'</kbd>) == <var>1</var> &amp;&amp;</td></tr>
<tr><th id="622">622</th><td>        GetTensorDim(ksize, data_format, <kbd>'C'</kbd>) == <var>1</var> &amp;&amp;</td></tr>
<tr><th id="623">623</th><td>        GetTensorDim(strides, data_format, <kbd>'C'</kbd>) == <var>1</var>) {</td></tr>
<tr><th id="624">624</th><td>      <b>return</b> <b>true</b>;</td></tr>
<tr><th id="625">625</th><td>    }</td></tr>
<tr><th id="626">626</th><td></td></tr>
<tr><th id="627">627</th><td>    <b>return</b> <b>false</b>;</td></tr>
<tr><th id="628">628</th><td>  }</td></tr>
<tr><th id="629">629</th><td></td></tr>
<tr><th id="630">630</th><td>  <em>static</em> <em>bool</em> AddNRewrite(<em>const</em> Node* n, <em>const</em> ContextInfo* c) {</td></tr>
<tr><th id="631">631</th><td>    CHECK_NOTNULL(n);</td></tr>
<tr><th id="632">632</th><td></td></tr>
<tr><th id="633">633</th><td>    <em>int</em> num;</td></tr>
<tr><th id="634">634</th><td>    CHECK_EQ(GetNodeAttr(n-&gt;def(), <q>"N"</q>, &amp;num).ok(), <b>true</b>);</td></tr>
<tr><th id="635">635</th><td></td></tr>
<tr><th id="636">636</th><td>    <i>// Condition that specifies non-batch-wise and non-depth-wise pooling.</i></td></tr>
<tr><th id="637">637</th><td>    <b>if</b> (num == <var>2</var>) {</td></tr>
<tr><th id="638">638</th><td>      <b>return</b> <b>true</b>;</td></tr>
<tr><th id="639">639</th><td>    }</td></tr>
<tr><th id="640">640</th><td></td></tr>
<tr><th id="641">641</th><td>    <b>return</b> <b>false</b>;</td></tr>
<tr><th id="642">642</th><td>  }</td></tr>
<tr><th id="643">643</th><td>  <i>// Is BiasAddGrad node in 'n' is associated with Conv2DWithBias node</i></td></tr>
<tr><th id="644">644</th><td><i>  // specified in contextinfo 'ci'. Function updates fwd_node to point</i></td></tr>
<tr><th id="645">645</th><td><i>  // to Conv2DWithBias node if 'n' is associated with Conv2DWithBias.</i></td></tr>
<tr><th id="646">646</th><td><i>  //</i></td></tr>
<tr><th id="647">647</th><td><i>  // Association checks for one of the following graphs:</i></td></tr>
<tr><th id="648">648</th><td><i>  //</i></td></tr>
<tr><th id="649">649</th><td><i>  // Graph A:</i></td></tr>
<tr><th id="650">650</th><td><i>  //</i></td></tr>
<tr><th id="651">651</th><td><i>  // _ = Conv2DWithBias(F, I, _)</i></td></tr>
<tr><th id="652">652</th><td><i>  // ..</i></td></tr>
<tr><th id="653">653</th><td><i>  // _ = Conv2DBackpropFilter(F, _, G)</i></td></tr>
<tr><th id="654">654</th><td><i>  // _ = Conv2DBackpropInput(_, I, G)</i></td></tr>
<tr><th id="655">655</th><td><i>  // _ = BiasAddGrad(G)</i></td></tr>
<tr><th id="656">656</th><td><i>  //</i></td></tr>
<tr><th id="657">657</th><td><i>  // OR</i></td></tr>
<tr><th id="658">658</th><td><i>  //</i></td></tr>
<tr><th id="659">659</th><td><i>  // Graph B:</i></td></tr>
<tr><th id="660">660</th><td><i>  //</i></td></tr>
<tr><th id="661">661</th><td><i>  // _ = Conv2DWithBias(F, _, _)</i></td></tr>
<tr><th id="662">662</th><td><i>  // ..</i></td></tr>
<tr><th id="663">663</th><td><i>  // _ = Conv2DBackpropFilter(F, _, G)</i></td></tr>
<tr><th id="664">664</th><td><i>  // _ = BiasAddGrad(G)</i></td></tr>
<tr><th id="665">665</th><td><i>  //</i></td></tr>
<tr><th id="666">666</th><td><i>  // Here F, G, and I are graph nodes; _ represents graph nodes that we</i></td></tr>
<tr><th id="667">667</th><td><i>  // don't care here.</i></td></tr>
<tr><th id="668">668</th><td><i>  //</i></td></tr>
<tr><th id="669">669</th><td><i>  // @return - true (if BiasAddGrad is associated with Conv2DWithBias);</i></td></tr>
<tr><th id="670">670</th><td><i>  //           false otherwise.</i></td></tr>
<tr><th id="671">671</th><td>  <em>static</em> <em>bool</em> IsBiasAddGradInConv2DWithBiasContext(<em>const</em> Node* n,</td></tr>
<tr><th id="672">672</th><td>                                                   <em>const</em> Node** fwd_node,</td></tr>
<tr><th id="673">673</th><td>                                                   <em>void</em>* ci) {</td></tr>
<tr><th id="674">674</th><td>    CHECK_NOTNULL(n);</td></tr>
<tr><th id="675">675</th><td>    CHECK_NOTNULL(fwd_node);</td></tr>
<tr><th id="676">676</th><td>    CHECK_NOTNULL(ci);</td></tr>
<tr><th id="677">677</th><td>    *fwd_node = <b>nullptr</b>;</td></tr>
<tr><th id="678">678</th><td></td></tr>
<tr><th id="679">679</th><td>    CHECK_EQ(n-&gt;type_string(), csinfo_.bias_add_grad);</td></tr>
<tr><th id="680">680</th><td></td></tr>
<tr><th id="681">681</th><td>    <i>// Get the only 1 input of BiasAddGrad.</i></td></tr>
<tr><th id="682">682</th><td>    CHECK_EQ(n-&gt;num_inputs(), <var>1</var>);</td></tr>
<tr><th id="683">683</th><td>    <em>const</em> Node* bias_add_grad_inp = <b>nullptr</b>;</td></tr>
<tr><th id="684">684</th><td>    TF_CHECK_OK(n-&gt;input_node(<var>0</var>, &amp;bias_add_grad_inp));</td></tr>
<tr><th id="685">685</th><td>    CHECK_NOTNULL(bias_add_grad_inp);</td></tr>
<tr><th id="686">686</th><td></td></tr>
<tr><th id="687">687</th><td>    <i>// Check if this input also goes to BackpropFilter and BackpropInput</i></td></tr>
<tr><th id="688">688</th><td><i>    // as 3rd input.</i></td></tr>
<tr><th id="689">689</th><td>    <em>bool</em> found_backprop_input = <b>false</b>;</td></tr>
<tr><th id="690">690</th><td>    <em>bool</em> found_backprop_filter = <b>false</b>;</td></tr>
<tr><th id="691">691</th><td>    Node* backprop_filter_node = <b>nullptr</b>;</td></tr>
<tr><th id="692">692</th><td>    Node* backprop_input_node = <b>nullptr</b>;</td></tr>
<tr><th id="693">693</th><td></td></tr>
<tr><th id="694">694</th><td>    <b>for</b> (<em>const</em> Edge* e : bias_add_grad_inp-&gt;out_edges()) {</td></tr>
<tr><th id="695">695</th><td>      Node* third_input = <b>nullptr</b>;</td></tr>
<tr><th id="696">696</th><td>      <b>if</b> (e-&gt;dst()-&gt;type_string() == csinfo_.conv2d_grad_input ||</td></tr>
<tr><th id="697">697</th><td>          e-&gt;dst()-&gt;type_string() == csinfo_.mkl_conv2d_grad_input) {</td></tr>
<tr><th id="698">698</th><td>        <i>// Third input (index 2) of BackpropInput</i></td></tr>
<tr><th id="699">699</th><td>        TF_CHECK_OK(e-&gt;dst()-&gt;input_node(<var>2</var>, &amp;third_input));</td></tr>
<tr><th id="700">700</th><td>        <i>// Third input (index 2) of BackpropInput must be same as the input</i></td></tr>
<tr><th id="701">701</th><td><i>        // of BiasAddGrad.</i></td></tr>
<tr><th id="702">702</th><td>        <b>if</b> (third_input == bias_add_grad_inp) {</td></tr>
<tr><th id="703">703</th><td>          found_backprop_input = <b>true</b>;</td></tr>
<tr><th id="704">704</th><td>          backprop_input_node = e-&gt;dst();</td></tr>
<tr><th id="705">705</th><td>        }</td></tr>
<tr><th id="706">706</th><td>      }</td></tr>
<tr><th id="707">707</th><td></td></tr>
<tr><th id="708">708</th><td>      <b>if</b> (e-&gt;dst()-&gt;type_string() == csinfo_.conv2d_grad_filter ||</td></tr>
<tr><th id="709">709</th><td>          e-&gt;dst()-&gt;type_string() == csinfo_.mkl_conv2d_grad_filter) {</td></tr>
<tr><th id="710">710</th><td>        <i>// Third input (index 2) of BackpropFilter</i></td></tr>
<tr><th id="711">711</th><td>        TF_CHECK_OK(e-&gt;dst()-&gt;input_node(<var>2</var>, &amp;third_input));</td></tr>
<tr><th id="712">712</th><td>        <i>// Third input (index 2) of BackpropFilter must be same as the input</i></td></tr>
<tr><th id="713">713</th><td><i>        // of BiasAddGrad.</i></td></tr>
<tr><th id="714">714</th><td>        <b>if</b> (third_input == bias_add_grad_inp) {</td></tr>
<tr><th id="715">715</th><td>          found_backprop_filter = <b>true</b>;</td></tr>
<tr><th id="716">716</th><td>          backprop_filter_node = e-&gt;dst();</td></tr>
<tr><th id="717">717</th><td>        }</td></tr>
<tr><th id="718">718</th><td>      }</td></tr>
<tr><th id="719">719</th><td></td></tr>
<tr><th id="720">720</th><td>      <i>// If we found both the nodes, then we can stop the search.</i></td></tr>
<tr><th id="721">721</th><td>      <b>if</b> (found_backprop_input &amp;&amp; found_backprop_filter) {</td></tr>
<tr><th id="722">722</th><td>        <b>break</b>;</td></tr>
<tr><th id="723">723</th><td>      }</td></tr>
<tr><th id="724">724</th><td>    }</td></tr>
<tr><th id="725">725</th><td></td></tr>
<tr><th id="726">726</th><td>    <i>// If BackpropFilter node is not found, then this is not</i></td></tr>
<tr><th id="727">727</th><td><i>    // Conv2DWithBias context. For 2nd graph in the example above, only</i></td></tr>
<tr><th id="728">728</th><td><i>    // BackpropFilter would be present.</i></td></tr>
<tr><th id="729">729</th><td>    <b>if</b> (!found_backprop_filter) {</td></tr>
<tr><th id="730">730</th><td>      <b>return</b> <b>false</b>;</td></tr>
<tr><th id="731">731</th><td>    }</td></tr>
<tr><th id="732">732</th><td></td></tr>
<tr><th id="733">733</th><td>    <i>// Otherwise, we found the nodes.</i></td></tr>
<tr><th id="734">734</th><td>    CHECK_NOTNULL(backprop_filter_node);</td></tr>
<tr><th id="735">735</th><td>    <b>if</b> (found_backprop_input) {</td></tr>
<tr><th id="736">736</th><td>      CHECK_NOTNULL(backprop_input_node);</td></tr>
<tr><th id="737">737</th><td>    }</td></tr>
<tr><th id="738">738</th><td></td></tr>
<tr><th id="739">739</th><td>    <i>// Now that we confirmed that this is Conv2DWithBias context, we need to</i></td></tr>
<tr><th id="740">740</th><td><i>    // get access to the forward node (Conv2DWithBias). 2nd input of</i></td></tr>
<tr><th id="741">741</th><td><i>    // Conv2DWithBias is same as the 2nd input of Conv2DBackpropInput; 1st</i></td></tr>
<tr><th id="742">742</th><td><i>    // input of Conv2DWithBias is same as the 1st input of Conv2DBackpropFilter</i></td></tr>
<tr><th id="743">743</th><td><i>    // (This comes from definition of gradient computation for Conv2D).</i></td></tr>
<tr><th id="744">744</th><td>    <b>if</b> (found_backprop_input) {</td></tr>
<tr><th id="745">745</th><td>      <i>// Graph A in the example.</i></td></tr>
<tr><th id="746">746</th><td>      Node* second_inp_of_input = <b>nullptr</b>;</td></tr>
<tr><th id="747">747</th><td>      Node* first_inp_of_filter = <b>nullptr</b>;</td></tr>
<tr><th id="748">748</th><td>      TF_CHECK_OK(backprop_input_node-&gt;input_node(<var>1</var>, &amp;second_inp_of_input));</td></tr>
<tr><th id="749">749</th><td>      TF_CHECK_OK(backprop_filter_node-&gt;input_node(<var>0</var>, &amp;first_inp_of_filter));</td></tr>
<tr><th id="750">750</th><td>      CHECK_NOTNULL(second_inp_of_input);</td></tr>
<tr><th id="751">751</th><td>      CHECK_NOTNULL(first_inp_of_filter);</td></tr>
<tr><th id="752">752</th><td></td></tr>
<tr><th id="753">753</th><td>      <i>// Now we need to find out Conv2DWithBias node from these input nodes.</i></td></tr>
<tr><th id="754">754</th><td><i>      // Conv2DWithBias node is the node that accepts both the nodes</i></td></tr>
<tr><th id="755">755</th><td><i>      // second_inp_of_input and first_inp_of_filter in 2nd and 1st input slots.</i></td></tr>
<tr><th id="756">756</th><td>      <b>for</b> (<em>const</em> Edge* fe : first_inp_of_filter-&gt;out_edges()) {</td></tr>
<tr><th id="757">757</th><td>        <b>if</b> (fe-&gt;dst()-&gt;type_string() == csinfo_.mkl_conv2d_with_bias &amp;&amp;</td></tr>
<tr><th id="758">758</th><td>            fe-&gt;dst_input() == <var>0</var>) {</td></tr>
<tr><th id="759">759</th><td>          <b>for</b> (<em>const</em> Edge* ie : second_inp_of_input-&gt;out_edges()) {</td></tr>
<tr><th id="760">760</th><td>            <b>if</b> (ie-&gt;dst()-&gt;type_string() == csinfo_.mkl_conv2d_with_bias &amp;&amp;</td></tr>
<tr><th id="761">761</th><td>                ie-&gt;dst_input() == <var>1</var> &amp;&amp; fe-&gt;dst() == ie-&gt;dst()) {</td></tr>
<tr><th id="762">762</th><td>              VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: found "</q></td></tr>
<tr><th id="763">763</th><td>                      &lt;&lt; fe-&gt;dst()-&gt;DebugString()</td></tr>
<tr><th id="764">764</th><td>                      &lt;&lt; <q>" as the forward node for matching context, backward"</q></td></tr>
<tr><th id="765">765</th><td>                      &lt;&lt; <q>" node is: "</q> &lt;&lt; n-&gt;DebugString();</td></tr>
<tr><th id="766">766</th><td>              *fwd_node = fe-&gt;dst();</td></tr>
<tr><th id="767">767</th><td>              <b>return</b> <b>true</b>;</td></tr>
<tr><th id="768">768</th><td>            }</td></tr>
<tr><th id="769">769</th><td>          }</td></tr>
<tr><th id="770">770</th><td>        }</td></tr>
<tr><th id="771">771</th><td>      }</td></tr>
<tr><th id="772">772</th><td>    } <b>else</b> {</td></tr>
<tr><th id="773">773</th><td>      <i>// We did not find BackpropInput, so we work with BackpropFilter only.</i></td></tr>
<tr><th id="774">774</th><td><i>      // Graph B in the example.</i></td></tr>
<tr><th id="775">775</th><td>      Node* first_inp_of_filter = <b>nullptr</b>;</td></tr>
<tr><th id="776">776</th><td>      TF_CHECK_OK(backprop_filter_node-&gt;input_node(<var>0</var>, &amp;first_inp_of_filter));</td></tr>
<tr><th id="777">777</th><td>      CHECK_NOTNULL(first_inp_of_filter);</td></tr>
<tr><th id="778">778</th><td></td></tr>
<tr><th id="779">779</th><td>      <i>// Now we need to find out Conv2DWithBias node from first input of</i></td></tr>
<tr><th id="780">780</th><td><i>      // BackpropFIlter. Conv2DWithBias node is the node that accepts</i></td></tr>
<tr><th id="781">781</th><td><i>      // first_inp_of_filter in 1st input slot.</i></td></tr>
<tr><th id="782">782</th><td>      <b>for</b> (<em>const</em> Edge* fe : first_inp_of_filter-&gt;out_edges()) {</td></tr>
<tr><th id="783">783</th><td>        <b>if</b> (fe-&gt;dst()-&gt;type_string() == csinfo_.mkl_conv2d_with_bias &amp;&amp;</td></tr>
<tr><th id="784">784</th><td>            fe-&gt;dst_input() == <var>0</var>) {</td></tr>
<tr><th id="785">785</th><td>          VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: found "</q> &lt;&lt; fe-&gt;dst()-&gt;DebugString()</td></tr>
<tr><th id="786">786</th><td>                  &lt;&lt; <q>" as the forward node for matching context, backward"</q></td></tr>
<tr><th id="787">787</th><td>                  &lt;&lt; <q>" node is: "</q> &lt;&lt; n-&gt;DebugString();</td></tr>
<tr><th id="788">788</th><td>          *fwd_node = fe-&gt;dst();</td></tr>
<tr><th id="789">789</th><td>          <b>return</b> <b>true</b>;</td></tr>
<tr><th id="790">790</th><td>        }</td></tr>
<tr><th id="791">791</th><td>      }</td></tr>
<tr><th id="792">792</th><td>    }</td></tr>
<tr><th id="793">793</th><td></td></tr>
<tr><th id="794">794</th><td>    <b>return</b> <b>false</b>;</td></tr>
<tr><th id="795">795</th><td>  }</td></tr>
<tr><th id="796">796</th><td></td></tr>
<tr><th id="797">797</th><td>  <i>// Is BiasAddGrad node in 'n' is associated with MatMul node</i></td></tr>
<tr><th id="798">798</th><td><i>  // specified in contextinfo 'ci'. Function does not update fwd_node.</i></td></tr>
<tr><th id="799">799</th><td><i>  //</i></td></tr>
<tr><th id="800">800</th><td><i>  // @return - true (if BiasAddGrad is associated with MatMul);</i></td></tr>
<tr><th id="801">801</th><td><i>  //           false otherwise.</i></td></tr>
<tr><th id="802">802</th><td>  <em>static</em> <em>bool</em> IsBiasAddGradInMatMulContext(<em>const</em> Node* n, <em>const</em> Node** fwd_node,</td></tr>
<tr><th id="803">803</th><td>                                           <em>void</em>* ci) {</td></tr>
<tr><th id="804">804</th><td>    <b>return</b> (!IsBiasAddGradInConv2DWithBiasContext(n, fwd_node, ci));</td></tr>
<tr><th id="805">805</th><td>  }</td></tr>
<tr><th id="806">806</th><td></td></tr>
<tr><th id="807">807</th><td>  <i>// Rewrite rule that uses context-information for matching,</i></td></tr>
<tr><th id="808">808</th><td><i>  // used in scenario 2.</i></td></tr>
<tr><th id="809">809</th><td><i>  //</i></td></tr>
<tr><th id="810">810</th><td><i>  // @input - Node 'n' for which to search for matching context</i></td></tr>
<tr><th id="811">811</th><td><i>  // @input - The context 'c' under which to rewrite</i></td></tr>
<tr><th id="812">812</th><td><i>  // @return - true if we can rewrite node under context 'c';</i></td></tr>
<tr><th id="813">813</th><td><i>  //           false otherwise.</i></td></tr>
<tr><th id="814">814</th><td>  <em>static</em> <em>bool</em> ContextMatchRewrite(<em>const</em> Node* n, <em>const</em> ContextInfo* c);</td></tr>
<tr><th id="815">815</th><td></td></tr>
<tr><th id="816">816</th><td>  <i>// Helper function that searches the matching contextinfo for the node.</i></td></tr>
<tr><th id="817">817</th><td><i>  //</i></td></tr>
<tr><th id="818">818</th><td><i>  // @input n - Node (gradient op) whose contextinfo is to be searched,</i></td></tr>
<tr><th id="819">819</th><td><i>  //        fwd_node - pointer to node from the forward pass that this node</i></td></tr>
<tr><th id="820">820</th><td><i>  //        belongs to. fwd_node cannot be NULL.</i></td></tr>
<tr><th id="821">821</th><td><i>  // @return Matching contextinfo in case a match is found; null otherwise.</i></td></tr>
<tr><th id="822">822</th><td><i>  //         Also updates *fwd_node with pointer to forward node that this</i></td></tr>
<tr><th id="823">823</th><td><i>  //         context matches.</i></td></tr>
<tr><th id="824">824</th><td>  <em>static</em> <em>const</em> ContextInfo* SearchMatchingContext(<em>const</em> Node* n,</td></tr>
<tr><th id="825">825</th><td>                                                  <em>const</em> Node** fwd_node);</td></tr>
<tr><th id="826">826</th><td></td></tr>
<tr><th id="827">827</th><td>  <i>// Rewrites input node to a new node specified by its matching rewrite info.</i></td></tr>
<tr><th id="828">828</th><td><i>  //</i></td></tr>
<tr><th id="829">829</th><td><i>  // Method first searches matching rewrite info for input node and then</i></td></tr>
<tr><th id="830">830</th><td><i>  // uses that info to rewrite.</i></td></tr>
<tr><th id="831">831</th><td><i>  //</i></td></tr>
<tr><th id="832">832</th><td><i>  // Input node may be deleted in case of rewrite. Attempt to use the node</i></td></tr>
<tr><th id="833">833</th><td><i>  // after the call can result in undefined behaviors.</i></td></tr>
<tr><th id="834">834</th><td><i>  //</i></td></tr>
<tr><th id="835">835</th><td><i>  // @input  g - input graph, n - Node to be rewritten,</i></td></tr>
<tr><th id="836">836</th><td><i>  //         ri - matching rewriteinfo</i></td></tr>
<tr><th id="837">837</th><td><i>  // @return Status::OK(), if the input node is rewritten;</i></td></tr>
<tr><th id="838">838</th><td><i>  //         Returns appropriate Status error code otherwise.</i></td></tr>
<tr><th id="839">839</th><td><i>  //         Graph is updated in case the input node is rewritten.</i></td></tr>
<tr><th id="840">840</th><td><i>  //         Otherwise, it is not updated.</i></td></tr>
<tr><th id="841">841</th><td>  Status RewriteNode(std::unique_ptr&lt;Graph&gt;* g, Node* n, <em>const</em> RewriteInfo* ri);</td></tr>
<tr><th id="842">842</th><td></td></tr>
<tr><th id="843">843</th><td>  <i>// Get nodes that will feed a list of TF tensors to the new</i></td></tr>
<tr><th id="844">844</th><td><i>  // node that we are constructing.</i></td></tr>
<tr><th id="845">845</th><td><i>  //</i></td></tr>
<tr><th id="846">846</th><td><i>  // @input g - input graph,</i></td></tr>
<tr><th id="847">847</th><td><i>  // @input inputs - inputs to old node that we are using for constructing</i></td></tr>
<tr><th id="848">848</th><td><i>  //                 new inputs,</i></td></tr>
<tr><th id="849">849</th><td><i>  // @input input_idx - the index in the 'inputs' vector pointing to the</i></td></tr>
<tr><th id="850">850</th><td><i>  //                    current input that we have processed so far</i></td></tr>
<tr><th id="851">851</th><td><i>  // @output input_idx - index will be incremented by the number of nodes</i></td></tr>
<tr><th id="852">852</th><td><i>  //                     from 'inputs' that are processed</i></td></tr>
<tr><th id="853">853</th><td><i>  // @input list_length - The expected length of list of TF tensors</i></td></tr>
<tr><th id="854">854</th><td><i>  // @output output_nodes - the list of new nodes creating TF tensors</i></td></tr>
<tr><th id="855">855</th><td><i>  //</i></td></tr>
<tr><th id="856">856</th><td><i>  // @return None</i></td></tr>
<tr><th id="857">857</th><td>  <em>void</em> GetNodesProducingTFTensorList(</td></tr>
<tr><th id="858">858</th><td>      <em>const</em> gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt;&amp; inputs,</td></tr>
<tr><th id="859">859</th><td>      <em>int</em>* input_idx, <em>int</em> list_length,</td></tr>
<tr><th id="860">860</th><td>      std::vector&lt;NodeBuilder::NodeOut&gt;* output_nodes);</td></tr>
<tr><th id="861">861</th><td></td></tr>
<tr><th id="862">862</th><td>  <i>// Get nodes that will feed a list of Mkl tensors to the new</i></td></tr>
<tr><th id="863">863</th><td><i>  // node that we are constructing.</i></td></tr>
<tr><th id="864">864</th><td><i>  //</i></td></tr>
<tr><th id="865">865</th><td><i>  // @input g - input graph,</i></td></tr>
<tr><th id="866">866</th><td><i>  // @input orig_node - Original node that we are rewriting</i></td></tr>
<tr><th id="867">867</th><td><i>  // @input inputs - inputs to old node that we are using for constructing</i></td></tr>
<tr><th id="868">868</th><td><i>  //                 new inputs,</i></td></tr>
<tr><th id="869">869</th><td><i>  // @input input_idx - the index in the 'inputs' vector pointing to the</i></td></tr>
<tr><th id="870">870</th><td><i>  //                    current input that we have processed so far</i></td></tr>
<tr><th id="871">871</th><td><i>  // @output input_idx - index will be incremented by the number of nodes</i></td></tr>
<tr><th id="872">872</th><td><i>  //                     from 'inputs' that are processed</i></td></tr>
<tr><th id="873">873</th><td><i>  // @input list_length - The expected length of list of Mkl tensors</i></td></tr>
<tr><th id="874">874</th><td><i>  // @output output_nodes - the list of new nodes creating Mkl tensors</i></td></tr>
<tr><th id="875">875</th><td><i>  //</i></td></tr>
<tr><th id="876">876</th><td><i>  // @return None</i></td></tr>
<tr><th id="877">877</th><td>  <em>void</em> GetNodesProducingMklTensorList(</td></tr>
<tr><th id="878">878</th><td>      std::unique_ptr&lt;Graph&gt;* g, Node* orig_node,</td></tr>
<tr><th id="879">879</th><td>      <em>const</em> gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt;&amp; inputs,</td></tr>
<tr><th id="880">880</th><td>      <em>int</em>* input_idx, <em>int</em> list_length,</td></tr>
<tr><th id="881">881</th><td>      std::vector&lt;NodeBuilder::NodeOut&gt;* output_nodes);</td></tr>
<tr><th id="882">882</th><td></td></tr>
<tr><th id="883">883</th><td>  <i>// Get a node that will feed an Mkl tensor to the new</i></td></tr>
<tr><th id="884">884</th><td><i>  // node that we are constructing. The output node could be (1) 'n'</i></td></tr>
<tr><th id="885">885</th><td><i>  // if it is Mkl layer, or (2) a dummy node producing dummy Mkl tensor</i></td></tr>
<tr><th id="886">886</th><td><i>  // if 'n' is not an Mkl layer.</i></td></tr>
<tr><th id="887">887</th><td><i>  //</i></td></tr>
<tr><th id="888">888</th><td><i>  // @input g - input graph,</i></td></tr>
<tr><th id="889">889</th><td><i>  // @input orig_node - Original node that we are rewriting,</i></td></tr>
<tr><th id="890">890</th><td><i>  // @input n - Node based on which we are creating Mkl node,</i></td></tr>
<tr><th id="891">891</th><td><i>  // @input n_output_slot - the output slot of node 'n'</i></td></tr>
<tr><th id="892">892</th><td><i>  //            which is feeding to the node that we are constructing</i></td></tr>
<tr><th id="893">893</th><td><i>  // @output mkl_node - the new node that will feed Mkl tensor</i></td></tr>
<tr><th id="894">894</th><td><i>  // @output mkl_node_output_slot - the slot number of mkl_node that</i></td></tr>
<tr><th id="895">895</th><td><i>  //                                will feed the tensor</i></td></tr>
<tr><th id="896">896</th><td><i>  // @return None</i></td></tr>
<tr><th id="897">897</th><td>  <em>void</em> GetNodeProducingMklTensor(std::unique_ptr&lt;Graph&gt;* g, Node* orig_node,</td></tr>
<tr><th id="898">898</th><td>                                 Node* n, <em>int</em> n_output_slot, Node** mkl_node,</td></tr>
<tr><th id="899">899</th><td>                                 <em>int</em>* mkl_node_output_slot);</td></tr>
<tr><th id="900">900</th><td></td></tr>
<tr><th id="901">901</th><td>  <i>// Setup new inputs using old inputs 'inputs' for the rewritten node in 'nb'</i></td></tr>
<tr><th id="902">902</th><td><i>  // in graph 'g'. Original node is input in 'old_node'. Inputs to 'nb' are</i></td></tr>
<tr><th id="903">903</th><td><i>  // set up in contiguous fashion. 'workspace_tensors' carry graph nodes</i></td></tr>
<tr><th id="904">904</th><td><i>  // producing workspace edges if 'are_workspace_tensors_available' is true.</i></td></tr>
<tr><th id="905">905</th><td><i>  // Otherwise, 'workspace_tensors' is empty vector.</i></td></tr>
<tr><th id="906">906</th><td><i>  //</i></td></tr>
<tr><th id="907">907</th><td><i>  // For details, refer to 'Ordering of inputs after rewriting' section in the</i></td></tr>
<tr><th id="908">908</th><td><i>  // documentation above.</i></td></tr>
<tr><th id="909">909</th><td><i>  //</i></td></tr>
<tr><th id="910">910</th><td><i>  // Returns Status::OK() if setting up inputs is successful, otherwise</i></td></tr>
<tr><th id="911">911</th><td><i>  // returns appropriate status code.</i></td></tr>
<tr><th id="912">912</th><td>  <em>int</em> SetUpContiguousInputs(</td></tr>
<tr><th id="913">913</th><td>      std::unique_ptr&lt;Graph&gt;* g,</td></tr>
<tr><th id="914">914</th><td>      <em>const</em> gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt;&amp; old_node_inputs,</td></tr>
<tr><th id="915">915</th><td>      NodeBuilder* nb, Node* old_node,</td></tr>
<tr><th id="916">916</th><td>      std::vector&lt;NodeBuilder::NodeOut&gt;* workspace_tensors,</td></tr>
<tr><th id="917">917</th><td>      <em>bool</em> are_workspace_tensors_available);</td></tr>
<tr><th id="918">918</th><td></td></tr>
<tr><th id="919">919</th><td>  <i>// Setup new inputs using old inputs 'inputs' for the rewritten node in 'nb'</i></td></tr>
<tr><th id="920">920</th><td><i>  // in graph 'g'. Original node is input in 'orig_node'.</i></td></tr>
<tr><th id="921">921</th><td><i>  //</i></td></tr>
<tr><th id="922">922</th><td><i>  // For details, refer to 'Ordering of Tensorflow tensors and Mkl tensors'</i></td></tr>
<tr><th id="923">923</th><td><i>  // section in the documentation above.</i></td></tr>
<tr><th id="924">924</th><td><i>  //</i></td></tr>
<tr><th id="925">925</th><td><i>  // Returns Status::OK() if setting up inputs is successful, otherwise</i></td></tr>
<tr><th id="926">926</th><td><i>  // returns appropriate status code.</i></td></tr>
<tr><th id="927">927</th><td>  Status SetUpInputs(std::unique_ptr&lt;Graph&gt;* g,</td></tr>
<tr><th id="928">928</th><td>                     <em>const</em> gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt;&amp; inputs,</td></tr>
<tr><th id="929">929</th><td>                     NodeBuilder* nb, Node* orig_node);</td></tr>
<tr><th id="930">930</th><td></td></tr>
<tr><th id="931">931</th><td>  <i>// Add workspace edge on the input or output side of Node 'orig_node' by using</i></td></tr>
<tr><th id="932">932</th><td><i>  // NodeBuilder 'nb' for the new node provided. If 'orig_node' does not dictate</i></td></tr>
<tr><th id="933">933</th><td><i>  // adding workspace edge then do not add it. Workspace Tensorflow and Mkl</i></td></tr>
<tr><th id="934">934</th><td><i>  // tensors, if they need to be added, will be set into these tensors.</i></td></tr>
<tr><th id="935">935</th><td><i>  // If we set workspace tensors, then are_ws_tensors_added should be true.</i></td></tr>
<tr><th id="936">936</th><td>  <em>void</em> AddWorkSpaceEdgeIfNeeded(std::unique_ptr&lt;Graph&gt;* g, Node* orig_node,</td></tr>
<tr><th id="937">937</th><td>                                NodeBuilder* nb,</td></tr>
<tr><th id="938">938</th><td>                                std::vector&lt;NodeBuilder::NodeOut&gt;* ws_tensors,</td></tr>
<tr><th id="939">939</th><td>                                <em>bool</em>* are_ws_tensors_added);</td></tr>
<tr><th id="940">940</th><td></td></tr>
<tr><th id="941">941</th><td>  <i>// Functions specific to operators to copy attributes</i></td></tr>
<tr><th id="942">942</th><td><i>  // We need operator-specific function to copy attributes because the framework</i></td></tr>
<tr><th id="943">943</th><td><i>  // does not provide any generic function for it.</i></td></tr>
<tr><th id="944">944</th><td><i>  // NOTE: names are alphabetically sorted.</i></td></tr>
<tr><th id="945">945</th><td>  <em>static</em> <em>void</em> CopyAttrsAddN(<em>const</em> Node* orig_node, NodeBuilder* nb);</td></tr>
<tr><th id="946">946</th><td>  <em>static</em> <em>void</em> CopyAttrsBiasAddGrad(<em>const</em> Node* orig_node, NodeBuilder* nb);</td></tr>
<tr><th id="947">947</th><td>  <em>static</em> <em>void</em> CopyAttrsConcat(<em>const</em> Node* orig_node, NodeBuilder* nb);</td></tr>
<tr><th id="948">948</th><td>  <em>static</em> <em>void</em> CopyAttrsConcatV2(<em>const</em> Node* orig_node, NodeBuilder* nb);</td></tr>
<tr><th id="949">949</th><td>  <em>static</em> <em>void</em> CopyAttrsConv2D(<em>const</em> Node* orig_node, NodeBuilder* nb);</td></tr>
<tr><th id="950">950</th><td>  <em>static</em> <em>void</em> CopyAttrsDataType(<em>const</em> Node* orig_node, NodeBuilder* nb);</td></tr>
<tr><th id="951">951</th><td>  <em>static</em> <em>void</em> CopyAttrsFusedBatchNorm(<em>const</em> Node* orig_node, NodeBuilder* nb);</td></tr>
<tr><th id="952">952</th><td>  <em>static</em> <em>void</em> CopyAttrsIdentity(<em>const</em> Node* orig_node, NodeBuilder* nb);</td></tr>
<tr><th id="953">953</th><td>  <em>static</em> <em>void</em> CopyAttrsLRN(<em>const</em> Node* orig_node, NodeBuilder* nb);</td></tr>
<tr><th id="954">954</th><td>  <em>static</em> <em>void</em> CopyAttrsPooling(<em>const</em> Node* orig_node, NodeBuilder* nb);</td></tr>
<tr><th id="955">955</th><td>  <em>static</em> <em>void</em> CopyAttrsReshape(<em>const</em> Node* orig_node, NodeBuilder* nb);</td></tr>
<tr><th id="956">956</th><td>  <em>static</em> <em>void</em> CopyAttrsSplit(<em>const</em> Node* orig_node, NodeBuilder* nb);</td></tr>
<tr><th id="957">957</th><td></td></tr>
<tr><th id="958">958</th><td>  <i>// Generate a graph node in graph 'g' representing a dummy Mkl tensor node,</i></td></tr>
<tr><th id="959">959</th><td><i>  // using node for original node 'orig_node' and return it in '*out'.</i></td></tr>
<tr><th id="960">960</th><td><i>  // TODO(nhasabni) We should move this to mkl_util.h</i></td></tr>
<tr><th id="961">961</th><td>  <em>void</em> GetDummyMklTensorNode(std::unique_ptr&lt;Graph&gt;* g, Node** out,</td></tr>
<tr><th id="962">962</th><td>                             Node* orig_node);</td></tr>
<tr><th id="963">963</th><td>  <em>void</em> GetDummyWorkspaceTensorNode(std::unique_ptr&lt;Graph&gt;* g, Node** out,</td></tr>
<tr><th id="964">964</th><td>                                   Node* orig_node);</td></tr>
<tr><th id="965">965</th><td>};</td></tr>
<tr><th id="966">966</th><td></td></tr>
<tr><th id="967">967</th><td>MklLayoutRewritePass::ConstStringsInfo MklLayoutRewritePass::csinfo_;</td></tr>
<tr><th id="968">968</th><td>MklLayoutRewritePass::ContextInfo</td></tr>
<tr><th id="969">969</th><td>    MklLayoutRewritePass::biasaddgrad_conv2dwithbias_context_;</td></tr>
<tr><th id="970">970</th><td>MklLayoutRewritePass::ContextInfo</td></tr>
<tr><th id="971">971</th><td>    MklLayoutRewritePass::biasaddgrad_matmul_context_;</td></tr>
<tr><th id="972">972</th><td>std::vector&lt;MklLayoutRewritePass::ContextInfo*&gt; MklLayoutRewritePass::cinfo_;</td></tr>
<tr><th id="973">973</th><td></td></tr>
<tr><th id="974">974</th><td><i>// We register Mkl rewrite pass for phase 1 in post partitioning group.</i></td></tr>
<tr><th id="975">975</th><td><i>// We register it here so that we get a complete picture of all users of Mkl</i></td></tr>
<tr><th id="976">976</th><td><i>// nodes. Do not change the ordering of the Mkl passes.</i></td></tr>
<tr><th id="977">977</th><td><em>const</em> OptimizationPassRegistry::Grouping kMklLayoutRewritePassGroup =</td></tr>
<tr><th id="978">978</th><td>    OptimizationPassRegistry::POST_PARTITIONING;</td></tr>
<tr><th id="979">979</th><td>REGISTER_OPTIMIZATION(kMklLayoutRewritePassGroup, <var>1</var>, MklLayoutRewritePass);</td></tr>
<tr><th id="980">980</th><td></td></tr>
<tr><th id="981">981</th><td><i>//////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="982">982</th><td><i>//           Helper functions for creating new node</i></td></tr>
<tr><th id="983">983</th><td><i>//////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="984">984</th><td></td></tr>
<tr><th id="985">985</th><td><em>static</em> <em>void</em> FillInputs(<em>const</em> Node* n,</td></tr>
<tr><th id="986">986</th><td>                       gtl::InlinedVector&lt;Node*, <var>4</var>&gt;* control_edges,</td></tr>
<tr><th id="987">987</th><td>                       gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt;* in) {</td></tr>
<tr><th id="988">988</th><td>  control_edges-&gt;clear();</td></tr>
<tr><th id="989">989</th><td>  <b>for</b> (<em>const</em> Edge* e : n-&gt;in_edges()) {</td></tr>
<tr><th id="990">990</th><td>    <b>if</b> (e-&gt;IsControlEdge()) {</td></tr>
<tr><th id="991">991</th><td>      control_edges-&gt;push_back(e-&gt;src());</td></tr>
<tr><th id="992">992</th><td>    } <b>else</b> {</td></tr>
<tr><th id="993">993</th><td>      (*in)[e-&gt;dst_input()] = std::make_pair(e-&gt;src(), e-&gt;src_output());</td></tr>
<tr><th id="994">994</th><td>    }</td></tr>
<tr><th id="995">995</th><td>  }</td></tr>
<tr><th id="996">996</th><td>  std::sort(control_edges-&gt;begin(), control_edges-&gt;end());</td></tr>
<tr><th id="997">997</th><td>  <b>if</b> (n-&gt;op_def().is_commutative()) {</td></tr>
<tr><th id="998">998</th><td>    <i>// For commutative inputs, we sort the input by the input Node*</i></td></tr>
<tr><th id="999">999</th><td><i>    // to get a canonical ordering (so that add(a,b) and add(b, a) will</i></td></tr>
<tr><th id="1000">1000</th><td><i>    // hash to the same value if is_commutative is true for 'add').</i></td></tr>
<tr><th id="1001">1001</th><td>    std::sort(in-&gt;begin(), in-&gt;end());</td></tr>
<tr><th id="1002">1002</th><td>  }</td></tr>
<tr><th id="1003">1003</th><td>}</td></tr>
<tr><th id="1004">1004</th><td></td></tr>
<tr><th id="1005">1005</th><td><em>void</em> MklLayoutRewritePass::GetNodesProducingTFTensorList(</td></tr>
<tr><th id="1006">1006</th><td>    <em>const</em> gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt;&amp; inputs, <em>int</em>* input_idx,</td></tr>
<tr><th id="1007">1007</th><td>    <em>int</em> list_length, std::vector&lt;NodeBuilder::NodeOut&gt;* output_nodes) {</td></tr>
<tr><th id="1008">1008</th><td>  CHECK_LT(*input_idx, inputs.size());</td></tr>
<tr><th id="1009">1009</th><td>  CHECK_GT(list_length, <var>0</var>);</td></tr>
<tr><th id="1010">1010</th><td>  CHECK_NOTNULL(output_nodes);</td></tr>
<tr><th id="1011">1011</th><td>  output_nodes-&gt;reserve(list_length);</td></tr>
<tr><th id="1012">1012</th><td></td></tr>
<tr><th id="1013">1013</th><td>  <b>while</b> (list_length != <var>0</var>) {</td></tr>
<tr><th id="1014">1014</th><td>    CHECK_GT(list_length, <var>0</var>);</td></tr>
<tr><th id="1015">1015</th><td>    CHECK_LT(*input_idx, inputs.size());</td></tr>
<tr><th id="1016">1016</th><td>    Node* n = inputs[*input_idx].first;</td></tr>
<tr><th id="1017">1017</th><td>    <em>int</em> slot = inputs[*input_idx].second;</td></tr>
<tr><th id="1018">1018</th><td>    <i>// If input node 'n' is just producing a single tensor at</i></td></tr>
<tr><th id="1019">1019</th><td><i>    // output slot 'slot' then we just add that single node.</i></td></tr>
<tr><th id="1020">1020</th><td>    output_nodes-&gt;push_back(NodeBuilder::NodeOut(n, slot));</td></tr>
<tr><th id="1021">1021</th><td>    (*input_idx)++;</td></tr>
<tr><th id="1022">1022</th><td>    list_length--;</td></tr>
<tr><th id="1023">1023</th><td>  }</td></tr>
<tr><th id="1024">1024</th><td>}</td></tr>
<tr><th id="1025">1025</th><td></td></tr>
<tr><th id="1026">1026</th><td><i>// TODO(nhasabni) We should move this to mkl_util.h.</i></td></tr>
<tr><th id="1027">1027</th><td><em>void</em> MklLayoutRewritePass::GetDummyMklTensorNode(std::unique_ptr&lt;Graph&gt;* g,</td></tr>
<tr><th id="1028">1028</th><td>                                                 Node** out, Node* orig_node) {</td></tr>
<tr><th id="1029">1029</th><td>  <i>// We use a tensor of shape {8} and value 0,0,0,0,0,0,0,0 to represent</i></td></tr>
<tr><th id="1030">1030</th><td><i>  // dummy Mkl tensor. 8 = 2*size_t.</i></td></tr>
<tr><th id="1031">1031</th><td>  <em>const</em> DataType dt = DataTypeToEnum&lt;uint8&gt;::v();</td></tr>
<tr><th id="1032">1032</th><td>  TensorProto proto;</td></tr>
<tr><th id="1033">1033</th><td>  proto.set_dtype(dt);</td></tr>
<tr><th id="1034">1034</th><td>  uint8 zero[<var>8</var>] = {<var>0</var>, <var>0</var>, <var>0</var>, <var>0</var>, <var>0</var>, <var>0</var>, <var>0</var>, <var>0</var>};</td></tr>
<tr><th id="1035">1035</th><td>  proto.set_tensor_content(string(<b>reinterpret_cast</b>&lt;<em>const</em> <em>char</em>*&gt;(zero), <var>8</var>));</td></tr>
<tr><th id="1036">1036</th><td>  TensorShape dummy_shape({<var>8</var>});</td></tr>
<tr><th id="1037">1037</th><td>  dummy_shape.AsProto(proto.mutable_tensor_shape());</td></tr>
<tr><th id="1038">1038</th><td>  TF_CHECK_OK(NodeBuilder((*g)-&gt;NewName(<q>"DMT"</q>), <q>"Const"</q>)</td></tr>
<tr><th id="1039">1039</th><td>                  .Attr(<q>"value"</q>, proto)</td></tr>
<tr><th id="1040">1040</th><td>                  .Attr(<q>"dtype"</q>, dt)</td></tr>
<tr><th id="1041">1041</th><td>                  .Device(orig_node-&gt;def().device())  <i>// We place this node on</i></td></tr>
<tr><th id="1042">1042</th><td>                                                      <i>// the same device as the</i></td></tr>
<tr><th id="1043">1043</th><td><i>                                                      // device of the original</i></td></tr>
<tr><th id="1044">1044</th><td><i>                                                      // node.</i></td></tr>
<tr><th id="1045">1045</th><td>                  .Finalize(&amp;**g, out));</td></tr>
<tr><th id="1046">1046</th><td></td></tr>
<tr><th id="1047">1047</th><td>  <i>// If number of inputs to the original node is &gt; 0, then we add</i></td></tr>
<tr><th id="1048">1048</th><td><i>  // control dependency between 1st input (index 0) of the original node and</i></td></tr>
<tr><th id="1049">1049</th><td><i>  // the dummy Mkl node. This is needed because control-flow ops such as Enter,</i></td></tr>
<tr><th id="1050">1050</th><td><i>  // Merge, etc, require frame_name of the dummy Mkl node to be same as the</i></td></tr>
<tr><th id="1051">1051</th><td><i>  // rewritten node. Adding control edge between 1st input of the original node</i></td></tr>
<tr><th id="1052">1052</th><td><i>  // and the dummy Mkl node ensures that the dummy node is in the same frame</i></td></tr>
<tr><th id="1053">1053</th><td><i>  // as the original node. Choosing 1st input is not necessary - any input of</i></td></tr>
<tr><th id="1054">1054</th><td><i>  // the original node is fine because all the inputs of a node are always in</i></td></tr>
<tr><th id="1055">1055</th><td><i>  // the same frame.</i></td></tr>
<tr><th id="1056">1056</th><td>  <b>if</b> (orig_node-&gt;num_inputs() &gt; <var>0</var>) {</td></tr>
<tr><th id="1057">1057</th><td>    Node* orig_input0 = <b>nullptr</b>;</td></tr>
<tr><th id="1058">1058</th><td>    TF_CHECK_OK(</td></tr>
<tr><th id="1059">1059</th><td>        orig_node-&gt;input_node(<var>0</var>, <b>const_cast</b>&lt;<em>const</em> Node**&gt;(&amp;orig_input0)));</td></tr>
<tr><th id="1060">1060</th><td>    CHECK_NOTNULL((*g)-&gt;AddControlEdge(orig_input0, *out));</td></tr>
<tr><th id="1061">1061</th><td>  }</td></tr>
<tr><th id="1062">1062</th><td></td></tr>
<tr><th id="1063">1063</th><td>  (*out)-&gt;set_assigned_device_name(orig_node-&gt;assigned_device_name());</td></tr>
<tr><th id="1064">1064</th><td>}</td></tr>
<tr><th id="1065">1065</th><td></td></tr>
<tr><th id="1066">1066</th><td><em>void</em> MklLayoutRewritePass::GetNodesProducingMklTensorList(</td></tr>
<tr><th id="1067">1067</th><td>    std::unique_ptr&lt;Graph&gt;* g, Node* orig_node,</td></tr>
<tr><th id="1068">1068</th><td>    <em>const</em> gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt;&amp; inputs, <em>int</em>* input_idx,</td></tr>
<tr><th id="1069">1069</th><td>    <em>int</em> list_length, std::vector&lt;NodeBuilder::NodeOut&gt;* output_nodes) {</td></tr>
<tr><th id="1070">1070</th><td>  CHECK_LT(*input_idx, inputs.size());</td></tr>
<tr><th id="1071">1071</th><td>  CHECK_GT(list_length, <var>0</var>);</td></tr>
<tr><th id="1072">1072</th><td>  CHECK_NOTNULL(output_nodes);</td></tr>
<tr><th id="1073">1073</th><td>  output_nodes-&gt;reserve(list_length);</td></tr>
<tr><th id="1074">1074</th><td></td></tr>
<tr><th id="1075">1075</th><td>  <b>while</b> (list_length != <var>0</var>) {</td></tr>
<tr><th id="1076">1076</th><td>    CHECK_GT(list_length, <var>0</var>);</td></tr>
<tr><th id="1077">1077</th><td>    CHECK_LT(*input_idx, inputs.size());</td></tr>
<tr><th id="1078">1078</th><td>    Node* n = inputs[*input_idx].first;</td></tr>
<tr><th id="1079">1079</th><td>    <em>int</em> slot = inputs[*input_idx].second;</td></tr>
<tr><th id="1080">1080</th><td>    <i>// If 'n' is producing a single tensor, then create a single Mkl tensor</i></td></tr>
<tr><th id="1081">1081</th><td><i>    // node.</i></td></tr>
<tr><th id="1082">1082</th><td>    Node* mkl_node = <b>nullptr</b>;</td></tr>
<tr><th id="1083">1083</th><td>    <em>int</em> mkl_node_output_slot = <var>0</var>;</td></tr>
<tr><th id="1084">1084</th><td>    GetNodeProducingMklTensor(g, orig_node, n, slot, &amp;mkl_node,</td></tr>
<tr><th id="1085">1085</th><td>                              &amp;mkl_node_output_slot);</td></tr>
<tr><th id="1086">1086</th><td>    output_nodes-&gt;push_back(</td></tr>
<tr><th id="1087">1087</th><td>        NodeBuilder::NodeOut(mkl_node, mkl_node_output_slot));</td></tr>
<tr><th id="1088">1088</th><td>    (*input_idx)++;</td></tr>
<tr><th id="1089">1089</th><td>    list_length--;</td></tr>
<tr><th id="1090">1090</th><td>  }</td></tr>
<tr><th id="1091">1091</th><td>}</td></tr>
<tr><th id="1092">1092</th><td></td></tr>
<tr><th id="1093">1093</th><td><i>// Get an input node that will feed Mkl tensor to the new</i></td></tr>
<tr><th id="1094">1094</th><td><i>// node that we are constructing. An input node could be (1) 'n'</i></td></tr>
<tr><th id="1095">1095</th><td><i>// if it is Mkl layer, or (2) a dummy node producing dummy Mkl tensor</i></td></tr>
<tr><th id="1096">1096</th><td><i>// if 'n' is not an Mkl layer.</i></td></tr>
<tr><th id="1097">1097</th><td><em>void</em> MklLayoutRewritePass::GetNodeProducingMklTensor(</td></tr>
<tr><th id="1098">1098</th><td>    std::unique_ptr&lt;Graph&gt;* g, Node* orig_node, Node* n, <em>int</em> n_output_slot,</td></tr>
<tr><th id="1099">1099</th><td>    Node** mkl_node, <em>int</em>* mkl_node_output_slot) {</td></tr>
<tr><th id="1100">1100</th><td>  CHECK_NOTNULL(n);</td></tr>
<tr><th id="1101">1101</th><td>  CHECK_NOTNULL(mkl_node);</td></tr>
<tr><th id="1102">1102</th><td>  CHECK_NOTNULL(mkl_node_output_slot);</td></tr>
<tr><th id="1103">1103</th><td></td></tr>
<tr><th id="1104">1104</th><td>  <i>// If this is an MKL op, then it will create extra output for MKL layout.</i></td></tr>
<tr><th id="1105">1105</th><td>  DataType T;</td></tr>
<tr><th id="1106">1106</th><td>  <b>if</b> (GetNodeAttr(n-&gt;def(), <q>"T"</q>, &amp;T).ok() &amp;&amp;</td></tr>
<tr><th id="1107">1107</th><td>      mkl_op_registry::IsMklOp(n-&gt;type_string(), T)) {</td></tr>
<tr><th id="1108">1108</th><td>    <i>// If this is an MKL op, then it will generate an edge that will receive</i></td></tr>
<tr><th id="1109">1109</th><td><i>    // Mkl tensor from a node.</i></td></tr>
<tr><th id="1110">1110</th><td><i>    // output slot number for Mkl tensor would be N+slot number of TensorFlow</i></td></tr>
<tr><th id="1111">1111</th><td><i>    // tensor, where N is total number of TensorFlow tensors.</i></td></tr>
<tr><th id="1112">1112</th><td>    *mkl_node = n;</td></tr>
<tr><th id="1113">1113</th><td>    *mkl_node_output_slot =</td></tr>
<tr><th id="1114">1114</th><td>        GetTensorMetaDataIndex(n_output_slot, n-&gt;num_outputs());</td></tr>
<tr><th id="1115">1115</th><td>  } <b>else</b> {</td></tr>
<tr><th id="1116">1116</th><td>    <i>// If we have not visited the node and rewritten it, then we need</i></td></tr>
<tr><th id="1117">1117</th><td><i>    // to create a dummy node that will feed a dummy Mkl tensor to this node.</i></td></tr>
<tr><th id="1118">1118</th><td><i>    // DummyMklTensor node has no input and generates only 1 output</i></td></tr>
<tr><th id="1119">1119</th><td><i>    // (dummy Mkl tensor) as output slot number 0.</i></td></tr>
<tr><th id="1120">1120</th><td>    GetDummyMklTensorNode(g, mkl_node, orig_node);</td></tr>
<tr><th id="1121">1121</th><td>    CHECK_NOTNULL(*mkl_node);</td></tr>
<tr><th id="1122">1122</th><td>    *mkl_node_output_slot = <var>0</var>;</td></tr>
<tr><th id="1123">1123</th><td>  }</td></tr>
<tr><th id="1124">1124</th><td>}</td></tr>
<tr><th id="1125">1125</th><td></td></tr>
<tr><th id="1126">1126</th><td><em>int</em> MklLayoutRewritePass::SetUpContiguousInputs(</td></tr>
<tr><th id="1127">1127</th><td>    std::unique_ptr&lt;Graph&gt;* g,</td></tr>
<tr><th id="1128">1128</th><td>    <em>const</em> gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt;&amp; old_node_inputs,</td></tr>
<tr><th id="1129">1129</th><td>    NodeBuilder* nb, Node* old_node,</td></tr>
<tr><th id="1130">1130</th><td>    std::vector&lt;NodeBuilder::NodeOut&gt;* workspace_tensors,</td></tr>
<tr><th id="1131">1131</th><td>    <em>bool</em> are_workspace_tensors_available) {</td></tr>
<tr><th id="1132">1132</th><td>  CHECK_NOTNULL(workspace_tensors);</td></tr>
<tr><th id="1133">1133</th><td>  CHECK_EQ(kTensorOrdering, MklTfTensorOrdering::TENSORS_CONTIGUOUS);</td></tr>
<tr><th id="1134">1134</th><td></td></tr>
<tr><th id="1135">1135</th><td>  <i>// TODO(nhasabni): Temporary solution to connect filter input of</i></td></tr>
<tr><th id="1136">1136</th><td><i>  // BackpropInput with the converted filter from Conv2D.</i></td></tr>
<tr><th id="1137">1137</th><td>  <em>bool</em> do_connect_conv2d_backprop_input_filter = <b>false</b>;</td></tr>
<tr><th id="1138">1138</th><td>  Node* conv2d_node = <b>nullptr</b>;</td></tr>
<tr><th id="1139">1139</th><td>  <i>// Filter node is 2nd input (slot index 1) of Conv2D.</i></td></tr>
<tr><th id="1140">1140</th><td>  <em>int</em> kConv2DFilterInputSlotIdx = <var>1</var>;</td></tr>
<tr><th id="1141">1141</th><td>  <em>int</em> kConv2DBackpropInputFilterInputSlotIdx = <var>1</var>;</td></tr>
<tr><th id="1142">1142</th><td>  <em>int</em> kConv2DFilterOutputSlotIdx = <var>1</var>;</td></tr>
<tr><th id="1143">1143</th><td>  <b>if</b> (old_node-&gt;type_string() == csinfo_.conv2d_grad_input) {</td></tr>
<tr><th id="1144">1144</th><td>    <i>// We need to find Conv2D node from Conv2DBackpropInput.</i></td></tr>
<tr><th id="1145">1145</th><td><i>    // For that let's first find filter node that is 2nd input (slot 1)</i></td></tr>
<tr><th id="1146">1146</th><td><i>    // of BackpropInput.</i></td></tr>
<tr><th id="1147">1147</th><td>    Node* filter_node = <b>nullptr</b>;</td></tr>
<tr><th id="1148">1148</th><td>    TF_CHECK_OK(old_node-&gt;input_node(kConv2DBackpropInputFilterInputSlotIdx,</td></tr>
<tr><th id="1149">1149</th><td>                                     &amp;filter_node));</td></tr>
<tr><th id="1150">1150</th><td>    CHECK_NOTNULL(filter_node);</td></tr>
<tr><th id="1151">1151</th><td></td></tr>
<tr><th id="1152">1152</th><td>    <i>// Now check which nodes receive from filter_node. Filter feeds as</i></td></tr>
<tr><th id="1153">1153</th><td><i>    // 2nd input (slot 1) of _MklConv2D and _MklConv2DWithBias.</i></td></tr>
<tr><th id="1154">1154</th><td>    <b>for</b> (<em>const</em> Edge* e : filter_node-&gt;out_edges()) {</td></tr>
<tr><th id="1155">1155</th><td>      <b>if</b> (e-&gt;dst()-&gt;type_string() == csinfo_.mkl_conv2d &amp;&amp;</td></tr>
<tr><th id="1156">1156</th><td>          e-&gt;dst_input() == kConv2DFilterInputSlotIdx</td></tr>
<tr><th id="1157">1157</th><td>          <i>/* filter is 2nd input of Conv2D and _MklConv2D. */</i>) {</td></tr>
<tr><th id="1158">1158</th><td>        <b>if</b> (conv2d_node != <b>nullptr</b>) {</td></tr>
<tr><th id="1159">1159</th><td>          VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: unusual case of same filter"</q></td></tr>
<tr><th id="1160">1160</th><td>                  &lt;&lt; <q>" feeding multiple Conv2D nodes: "</q></td></tr>
<tr><th id="1161">1161</th><td>                  &lt;&lt; filter_node-&gt;DebugString();</td></tr>
<tr><th id="1162">1162</th><td>          <i>// We will not connect filter input of Conv2DBackpropInput</i></td></tr>
<tr><th id="1163">1163</th><td><i>          // to be safe here.</i></td></tr>
<tr><th id="1164">1164</th><td>          do_connect_conv2d_backprop_input_filter = <b>false</b>;</td></tr>
<tr><th id="1165">1165</th><td>          <b>break</b>;</td></tr>
<tr><th id="1166">1166</th><td>        } <b>else</b> {</td></tr>
<tr><th id="1167">1167</th><td>          conv2d_node = e-&gt;dst();</td></tr>
<tr><th id="1168">1168</th><td>          do_connect_conv2d_backprop_input_filter = <b>true</b>;</td></tr>
<tr><th id="1169">1169</th><td>        }</td></tr>
<tr><th id="1170">1170</th><td>      }</td></tr>
<tr><th id="1171">1171</th><td>    }</td></tr>
<tr><th id="1172">1172</th><td>  }</td></tr>
<tr><th id="1173">1173</th><td></td></tr>
<tr><th id="1174">1174</th><td>  <i>// Number of input slots to original op</i></td></tr>
<tr><th id="1175">1175</th><td><i>  // Input slots are represented by .Input() calls in REGISTER_OP.</i></td></tr>
<tr><th id="1176">1176</th><td>  <em>int</em> old_node_input_slots = old_node-&gt;op_def().input_arg_size();</td></tr>
<tr><th id="1177">1177</th><td>  <i>// Actual number of inputs can be greater than or equal to number</i></td></tr>
<tr><th id="1178">1178</th><td><i>  // of Input slots because inputs of type list could be unfolded.</i></td></tr>
<tr><th id="1179">1179</th><td>  CHECK_GE(old_node_inputs.size(), old_node_input_slots);</td></tr>
<tr><th id="1180">1180</th><td>  <em>int</em> nn_slot_idx = <var>0</var>;  <i>// slot index for inputs of new node</i></td></tr>
<tr><th id="1181">1181</th><td></td></tr>
<tr><th id="1182">1182</th><td>  <i>// Let's copy all inputs (TF tensors) of original node to new node.</i></td></tr>
<tr><th id="1183">1183</th><td>  <em>int</em> iidx = <var>0</var>;</td></tr>
<tr><th id="1184">1184</th><td>  <b>for</b> (<em>int</em> on_slot_idx = <var>0</var>; on_slot_idx &lt; old_node_input_slots; on_slot_idx++) {</td></tr>
<tr><th id="1185">1185</th><td>    <i>// An input slot could be a single tensor or a list. We need</i></td></tr>
<tr><th id="1186">1186</th><td><i>    // to handle this case accordingly.</i></td></tr>
<tr><th id="1187">1187</th><td>    CHECK_LT(iidx, old_node_inputs.size());</td></tr>
<tr><th id="1188">1188</th><td>    <em>const</em> OpDef::ArgDef&amp; arg = old_node-&gt;op_def().input_arg(on_slot_idx);</td></tr>
<tr><th id="1189">1189</th><td>    <b>if</b> (ArgIsList(arg)) {</td></tr>
<tr><th id="1190">1190</th><td>      std::vector&lt;NodeBuilder::NodeOut&gt; new_node_inputs;</td></tr>
<tr><th id="1191">1191</th><td>      <em>int</em> N = GetTensorListLength(arg, old_node);</td></tr>
<tr><th id="1192">1192</th><td>      GetNodesProducingTFTensorList(old_node_inputs, &amp;iidx, N,</td></tr>
<tr><th id="1193">1193</th><td>                                    &amp;new_node_inputs);</td></tr>
<tr><th id="1194">1194</th><td>      nb-&gt;Input(new_node_inputs);</td></tr>
<tr><th id="1195">1195</th><td>      nn_slot_idx++;</td></tr>
<tr><th id="1196">1196</th><td>    } <b>else</b> {</td></tr>
<tr><th id="1197">1197</th><td>      <i>// Special case for connecting filter input of Conv2DBackpropInput</i></td></tr>
<tr><th id="1198">1198</th><td>      <b>if</b> (do_connect_conv2d_backprop_input_filter &amp;&amp;</td></tr>
<tr><th id="1199">1199</th><td>          iidx == kConv2DBackpropInputFilterInputSlotIdx) {</td></tr>
<tr><th id="1200">1200</th><td>        nb-&gt;Input(conv2d_node, kConv2DFilterOutputSlotIdx);</td></tr>
<tr><th id="1201">1201</th><td>      } <b>else</b> {</td></tr>
<tr><th id="1202">1202</th><td>        nb-&gt;Input(old_node_inputs[iidx].first, old_node_inputs[iidx].second);</td></tr>
<tr><th id="1203">1203</th><td>      }</td></tr>
<tr><th id="1204">1204</th><td>      iidx++;</td></tr>
<tr><th id="1205">1205</th><td>      nn_slot_idx++;</td></tr>
<tr><th id="1206">1206</th><td>    }</td></tr>
<tr><th id="1207">1207</th><td>  }</td></tr>
<tr><th id="1208">1208</th><td></td></tr>
<tr><th id="1209">1209</th><td>  <i>// If workspace tensors are available for this op and we are using</i></td></tr>
<tr><th id="1210">1210</th><td><i>  // contiguous ordering then we need to add Tensorflow tensor for</i></td></tr>
<tr><th id="1211">1211</th><td><i>  // workspace here because Tensorflow tensor for workspace is the</i></td></tr>
<tr><th id="1212">1212</th><td><i>  // last tensor in the list of Tensorflow tensors.</i></td></tr>
<tr><th id="1213">1213</th><td>  <b>if</b> (are_workspace_tensors_available) {</td></tr>
<tr><th id="1214">1214</th><td>    CHECK_EQ(workspace_tensors-&gt;size(), <var>2</var>);</td></tr>
<tr><th id="1215">1215</th><td>    <i>// Tensorflow tensor</i></td></tr>
<tr><th id="1216">1216</th><td>    nb-&gt;Input((*workspace_tensors)[<var>0</var>].node, (*workspace_tensors)[<var>0</var>].index);</td></tr>
<tr><th id="1217">1217</th><td>    nn_slot_idx++;</td></tr>
<tr><th id="1218">1218</th><td>  }</td></tr>
<tr><th id="1219">1219</th><td></td></tr>
<tr><th id="1220">1220</th><td>  <i>// Let's now setup all Mkl inputs to new node.</i></td></tr>
<tr><th id="1221">1221</th><td><i>  // Number of Mkl inputs must be same as number of TF inputs.</i></td></tr>
<tr><th id="1222">1222</th><td>  iidx = <var>0</var>;</td></tr>
<tr><th id="1223">1223</th><td>  <b>for</b> (<em>int</em> on_slot_idx = <var>0</var>; on_slot_idx &lt; old_node_input_slots; on_slot_idx++) {</td></tr>
<tr><th id="1224">1224</th><td>    <i>// An input slot could be a single tensor or a list. We need</i></td></tr>
<tr><th id="1225">1225</th><td><i>    // to handle this case accordingly.</i></td></tr>
<tr><th id="1226">1226</th><td>    CHECK_LT(iidx, old_node_inputs.size());</td></tr>
<tr><th id="1227">1227</th><td>    <em>const</em> OpDef::ArgDef&amp; arg = old_node-&gt;op_def().input_arg(on_slot_idx);</td></tr>
<tr><th id="1228">1228</th><td>    <b>if</b> (ArgIsList(arg)) {</td></tr>
<tr><th id="1229">1229</th><td>      std::vector&lt;NodeBuilder::NodeOut&gt; new_node_inputs;</td></tr>
<tr><th id="1230">1230</th><td>      <em>int</em> N = GetTensorListLength(arg, old_node);</td></tr>
<tr><th id="1231">1231</th><td>      GetNodesProducingMklTensorList(g, old_node, old_node_inputs, &amp;iidx, N,</td></tr>
<tr><th id="1232">1232</th><td>                                     &amp;new_node_inputs);</td></tr>
<tr><th id="1233">1233</th><td>      nb-&gt;Input(new_node_inputs);</td></tr>
<tr><th id="1234">1234</th><td>      nn_slot_idx++;</td></tr>
<tr><th id="1235">1235</th><td>    } <b>else</b> {</td></tr>
<tr><th id="1236">1236</th><td>      Node* mkl_node = <b>nullptr</b>;</td></tr>
<tr><th id="1237">1237</th><td>      <em>int</em> mkl_node_output_slot = <var>0</var>;</td></tr>
<tr><th id="1238">1238</th><td>      <i>// Special case for connecting filter input of Conv2DBackpropInput</i></td></tr>
<tr><th id="1239">1239</th><td>      <b>if</b> (do_connect_conv2d_backprop_input_filter &amp;&amp;</td></tr>
<tr><th id="1240">1240</th><td>          iidx == kConv2DBackpropInputFilterInputSlotIdx) {</td></tr>
<tr><th id="1241">1241</th><td>        GetNodeProducingMklTensor(g, old_node, conv2d_node,</td></tr>
<tr><th id="1242">1242</th><td>                                  kConv2DFilterOutputSlotIdx, &amp;mkl_node,</td></tr>
<tr><th id="1243">1243</th><td>                                  &amp;mkl_node_output_slot);</td></tr>
<tr><th id="1244">1244</th><td>      } <b>else</b> {</td></tr>
<tr><th id="1245">1245</th><td>        GetNodeProducingMklTensor(g, old_node, old_node_inputs[iidx].first,</td></tr>
<tr><th id="1246">1246</th><td>                                  old_node_inputs[iidx].second, &amp;mkl_node,</td></tr>
<tr><th id="1247">1247</th><td>                                  &amp;mkl_node_output_slot);</td></tr>
<tr><th id="1248">1248</th><td>      }</td></tr>
<tr><th id="1249">1249</th><td>      nb-&gt;Input(mkl_node, mkl_node_output_slot);</td></tr>
<tr><th id="1250">1250</th><td>      iidx++;</td></tr>
<tr><th id="1251">1251</th><td>      nn_slot_idx++;</td></tr>
<tr><th id="1252">1252</th><td>    }</td></tr>
<tr><th id="1253">1253</th><td>  }</td></tr>
<tr><th id="1254">1254</th><td></td></tr>
<tr><th id="1255">1255</th><td>  <i>// If workspace tensors are available for this op and we are using</i></td></tr>
<tr><th id="1256">1256</th><td><i>  // contiguous ordering then we need to add Mkl tensor for</i></td></tr>
<tr><th id="1257">1257</th><td><i>  // workspace here because Mkl tensor for workspace is the</i></td></tr>
<tr><th id="1258">1258</th><td><i>  // last tensor in the list of Mkl tensors.</i></td></tr>
<tr><th id="1259">1259</th><td>  <b>if</b> (are_workspace_tensors_available) {</td></tr>
<tr><th id="1260">1260</th><td>    CHECK_EQ(workspace_tensors-&gt;size(), <var>2</var>);</td></tr>
<tr><th id="1261">1261</th><td>    <i>// Mkl tensor</i></td></tr>
<tr><th id="1262">1262</th><td>    nb-&gt;Input((*workspace_tensors)[<var>1</var>].node, (*workspace_tensors)[<var>1</var>].index);</td></tr>
<tr><th id="1263">1263</th><td>    nn_slot_idx++;</td></tr>
<tr><th id="1264">1264</th><td>  }</td></tr>
<tr><th id="1265">1265</th><td></td></tr>
<tr><th id="1266">1266</th><td>  <b>return</b> nn_slot_idx;</td></tr>
<tr><th id="1267">1267</th><td>}</td></tr>
<tr><th id="1268">1268</th><td></td></tr>
<tr><th id="1269">1269</th><td>Status MklLayoutRewritePass::SetUpInputs(</td></tr>
<tr><th id="1270">1270</th><td>    std::unique_ptr&lt;Graph&gt;* g,</td></tr>
<tr><th id="1271">1271</th><td>    <em>const</em> gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt;&amp; old_node_inputs,</td></tr>
<tr><th id="1272">1272</th><td>    NodeBuilder* nb, Node* old_node) {</td></tr>
<tr><th id="1273">1273</th><td>  <i>// Let's check if we need to add workspace tensors for this node.</i></td></tr>
<tr><th id="1274">1274</th><td><i>  // We add workspace edge only for MaxPool, LRN and BatchNorm.</i></td></tr>
<tr><th id="1275">1275</th><td>  std::vector&lt;NodeBuilder::NodeOut&gt; workspace_tensors;</td></tr>
<tr><th id="1276">1276</th><td>  <em>bool</em> are_workspace_tensors_available = <b>false</b>;</td></tr>
<tr><th id="1277">1277</th><td>  AddWorkSpaceEdgeIfNeeded(g, old_node, nb, &amp;workspace_tensors,</td></tr>
<tr><th id="1278">1278</th><td>                           &amp;are_workspace_tensors_available);</td></tr>
<tr><th id="1279">1279</th><td></td></tr>
<tr><th id="1280">1280</th><td>  <em>int</em> new_node_input_slots = <var>0</var>;</td></tr>
<tr><th id="1281">1281</th><td>  <b>if</b> (kTensorOrdering == MklTfTensorOrdering::TENSORS_INTERLEAVED) {</td></tr>
<tr><th id="1282">1282</th><td>    <i>// TODO(nhasabni): implement this function just for same of completion.</i></td></tr>
<tr><th id="1283">1283</th><td><i>    // We do not use interleaved ordering right now.</i></td></tr>
<tr><th id="1284">1284</th><td>    <b>return</b> Status(</td></tr>
<tr><th id="1285">1285</th><td>        error::Code::UNIMPLEMENTED,</td></tr>
<tr><th id="1286">1286</th><td>        <q>"Interleaved ordering of tensors is currently not supported."</q>);</td></tr>
<tr><th id="1287">1287</th><td>  } <b>else</b> {</td></tr>
<tr><th id="1288">1288</th><td>    CHECK_EQ(kTensorOrdering, MklTfTensorOrdering::TENSORS_CONTIGUOUS);</td></tr>
<tr><th id="1289">1289</th><td>    new_node_input_slots = SetUpContiguousInputs(</td></tr>
<tr><th id="1290">1290</th><td>        g, old_node_inputs, nb, old_node, &amp;workspace_tensors,</td></tr>
<tr><th id="1291">1291</th><td>        are_workspace_tensors_available);</td></tr>
<tr><th id="1292">1292</th><td>  }</td></tr>
<tr><th id="1293">1293</th><td></td></tr>
<tr><th id="1294">1294</th><td>  <i>// Sanity check</i></td></tr>
<tr><th id="1295">1295</th><td>  <em>int</em> old_node_input_slots = old_node-&gt;op_def().input_arg_size();</td></tr>
<tr><th id="1296">1296</th><td>  <b>if</b> (!are_workspace_tensors_available) {</td></tr>
<tr><th id="1297">1297</th><td>    <i>// If we are not adding workspace tensors for this op, then the total</i></td></tr>
<tr><th id="1298">1298</th><td><i>    // number of input slots to the new node _must_ be 2 times the number</i></td></tr>
<tr><th id="1299">1299</th><td><i>    // of input slots to the original node: N original Tensorflow tensors and</i></td></tr>
<tr><th id="1300">1300</th><td><i>    // N for Mkl tensors corresponding to each Tensorflow tensors.</i></td></tr>
<tr><th id="1301">1301</th><td>    CHECK_EQ(new_node_input_slots, old_node_input_slots * <var>2</var>);</td></tr>
<tr><th id="1302">1302</th><td>  } <b>else</b> {</td></tr>
<tr><th id="1303">1303</th><td>    <i>// If we are adding workspace tensors for this op, then the total</i></td></tr>
<tr><th id="1304">1304</th><td><i>    // The total number of input slots to new node _must_ be 2 times the number</i></td></tr>
<tr><th id="1305">1305</th><td><i>    // of input slots to the original node: N original Tensorflow tensors and</i></td></tr>
<tr><th id="1306">1306</th><td><i>    // N for Mkl tensors corresponding to each Tensorflow tensors plus 2</i></td></tr>
<tr><th id="1307">1307</th><td><i>    // (for workspace Tensorflow tensor and workspace Mkl tensor).</i></td></tr>
<tr><th id="1308">1308</th><td>    CHECK_EQ(new_node_input_slots, old_node_input_slots * <var>2</var> + <var>2</var>);</td></tr>
<tr><th id="1309">1309</th><td>  }</td></tr>
<tr><th id="1310">1310</th><td></td></tr>
<tr><th id="1311">1311</th><td>  <b>return</b> Status::OK();</td></tr>
<tr><th id="1312">1312</th><td>}</td></tr>
<tr><th id="1313">1313</th><td></td></tr>
<tr><th id="1314">1314</th><td><i>//////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="1315">1315</th><td><i>//           Helper functions related to workspace pass</i></td></tr>
<tr><th id="1316">1316</th><td><i>//////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="1317">1317</th><td><i></i></td></tr>
<tr><th id="1318">1318</th><td><i>// TODO(nhasabni) We should move this to mkl_util.h.</i></td></tr>
<tr><th id="1319">1319</th><td><em>void</em> MklLayoutRewritePass::GetDummyWorkspaceTensorNode(</td></tr>
<tr><th id="1320">1320</th><td>    std::unique_ptr&lt;Graph&gt;* g, Node** out, Node* orig_node) {</td></tr>
<tr><th id="1321">1321</th><td>  <i>// We use a tensor of shape {1} and value 0 to represent</i></td></tr>
<tr><th id="1322">1322</th><td><i>  // dummy float tensor. We need this as a dummy workspace tensor.</i></td></tr>
<tr><th id="1323">1323</th><td><i>  // Workspace tensor has type float.</i></td></tr>
<tr><th id="1324">1324</th><td>  <em>const</em> DataType dt = DataTypeToEnum&lt;<em>float</em>&gt;::v();</td></tr>
<tr><th id="1325">1325</th><td>  TensorProto proto;</td></tr>
<tr><th id="1326">1326</th><td>  proto.set_dtype(dt);</td></tr>
<tr><th id="1327">1327</th><td>  <em>float</em> zero[<var>1</var>] = {<var>0</var>};</td></tr>
<tr><th id="1328">1328</th><td>  proto.set_tensor_content(string(<b>reinterpret_cast</b>&lt;<em>char</em>*&gt;(&amp;zero), <var>4</var>));</td></tr>
<tr><th id="1329">1329</th><td>  TensorShape dummy_shape({<var>1</var>});</td></tr>
<tr><th id="1330">1330</th><td>  dummy_shape.AsProto(proto.mutable_tensor_shape());</td></tr>
<tr><th id="1331">1331</th><td>  TF_CHECK_OK(NodeBuilder((*g)-&gt;NewName(<q>"DMT"</q>), <q>"Const"</q>)</td></tr>
<tr><th id="1332">1332</th><td>                  .Attr(<q>"value"</q>, proto)</td></tr>
<tr><th id="1333">1333</th><td>                  .Attr(<q>"dtype"</q>, dt)</td></tr>
<tr><th id="1334">1334</th><td>                  .Device(orig_node-&gt;def().device())  <i>// We place this node on</i></td></tr>
<tr><th id="1335">1335</th><td>                                                      <i>// same the device as the</i></td></tr>
<tr><th id="1336">1336</th><td><i>                                                      // device of the original</i></td></tr>
<tr><th id="1337">1337</th><td><i>                                                      // node.</i></td></tr>
<tr><th id="1338">1338</th><td>                  .Finalize(&amp;**g, out));</td></tr>
<tr><th id="1339">1339</th><td></td></tr>
<tr><th id="1340">1340</th><td>  <i>// If number of inputs to the original node is &gt; 0, then we add</i></td></tr>
<tr><th id="1341">1341</th><td><i>  // control dependency between 1st input (index 0) of the original node and</i></td></tr>
<tr><th id="1342">1342</th><td><i>  // the dummy Mkl node. This is needed because control-flow ops such as Enter,</i></td></tr>
<tr><th id="1343">1343</th><td><i>  // Merge, etc, require frame_name of the dummy Mkl node to be same as the</i></td></tr>
<tr><th id="1344">1344</th><td><i>  // rewritten node. Adding control edge between 1st input of the original node</i></td></tr>
<tr><th id="1345">1345</th><td><i>  // and the dummy Mkl node ensures that the dummy node is in the same frame</i></td></tr>
<tr><th id="1346">1346</th><td><i>  // as the original node. Choosing 1st input is not necessary - any input of</i></td></tr>
<tr><th id="1347">1347</th><td><i>  // the original node is fine because all the inputs of a node are always in</i></td></tr>
<tr><th id="1348">1348</th><td><i>  // the same frame.</i></td></tr>
<tr><th id="1349">1349</th><td>  <b>if</b> (orig_node-&gt;num_inputs() &gt; <var>0</var>) {</td></tr>
<tr><th id="1350">1350</th><td>    Node* orig_input0 = <b>nullptr</b>;</td></tr>
<tr><th id="1351">1351</th><td>    TF_CHECK_OK(</td></tr>
<tr><th id="1352">1352</th><td>        orig_node-&gt;input_node(<var>0</var>, <b>const_cast</b>&lt;<em>const</em> Node**&gt;(&amp;orig_input0)));</td></tr>
<tr><th id="1353">1353</th><td>    CHECK_NOTNULL((*g)-&gt;AddControlEdge(orig_input0, *out));</td></tr>
<tr><th id="1354">1354</th><td>  }</td></tr>
<tr><th id="1355">1355</th><td></td></tr>
<tr><th id="1356">1356</th><td>  (*out)-&gt;set_assigned_device_name(orig_node-&gt;assigned_device_name());</td></tr>
<tr><th id="1357">1357</th><td>}</td></tr>
<tr><th id="1358">1358</th><td></td></tr>
<tr><th id="1359">1359</th><td><em>void</em> MklLayoutRewritePass::AddWorkSpaceEdgeIfNeeded(</td></tr>
<tr><th id="1360">1360</th><td>    std::unique_ptr&lt;Graph&gt;* g, Node* orig_node, NodeBuilder* nb,</td></tr>
<tr><th id="1361">1361</th><td>    std::vector&lt;NodeBuilder::NodeOut&gt;* ws_tensors, <em>bool</em>* are_ws_tensors_added) {</td></tr>
<tr><th id="1362">1362</th><td>  <em>bool</em> workspace_edge_added = <b>false</b>;  <i>// Default initializer</i></td></tr>
<tr><th id="1363">1363</th><td>  CHECK_NOTNULL(are_ws_tensors_added);</td></tr>
<tr><th id="1364">1364</th><td>  *are_ws_tensors_added = <b>false</b>;  <i>// Default initializer</i></td></tr>
<tr><th id="1365">1365</th><td></td></tr>
<tr><th id="1366">1366</th><td>  DataType T;</td></tr>
<tr><th id="1367">1367</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="1368">1368</th><td>  <b>for</b> (<em>auto</em> ws : wsinfo_) {</td></tr>
<tr><th id="1369">1369</th><td>    <b>if</b> (orig_node-&gt;type_string() == ws.fwd_op &amp;&amp;</td></tr>
<tr><th id="1370">1370</th><td>        mkl_op_registry::IsMklOp(</td></tr>
<tr><th id="1371">1371</th><td>            mkl_op_registry::GetMklOpName(orig_node-&gt;type_string()), T)) {</td></tr>
<tr><th id="1372">1372</th><td>      <i>// If this op is a fwd op, then we need to check if there is an</i></td></tr>
<tr><th id="1373">1373</th><td><i>      // edge from this node's fwd_slot to bwdop's bwd_slot. If there is</i></td></tr>
<tr><th id="1374">1374</th><td><i>      // an edge, then we just add an attribute on this node for setting</i></td></tr>
<tr><th id="1375">1375</th><td><i>      // workspace_passed to true. We don't add actual workspace edge</i></td></tr>
<tr><th id="1376">1376</th><td><i>      // in this node. Actual workspace edge gets added in the backward</i></td></tr>
<tr><th id="1377">1377</th><td><i>      // op for this node.</i></td></tr>
<tr><th id="1378">1378</th><td>      <b>for</b> (<em>const</em> Edge* e : orig_node-&gt;out_edges()) {</td></tr>
<tr><th id="1379">1379</th><td>        <b>if</b> (e-&gt;src_output() == ws.fwd_slot &amp;&amp;</td></tr>
<tr><th id="1380">1380</th><td>            e-&gt;dst()-&gt;type_string() == ws.bwd_op &amp;&amp;</td></tr>
<tr><th id="1381">1381</th><td>            e-&gt;dst_input() == ws.bwd_slot) {</td></tr>
<tr><th id="1382">1382</th><td>          nb-&gt;Attr(<q>"workspace_enabled"</q>, <b>true</b>);</td></tr>
<tr><th id="1383">1383</th><td>          VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: workspace_enabled for "</q></td></tr>
<tr><th id="1384">1384</th><td>                  &lt;&lt; orig_node-&gt;type_string();</td></tr>
<tr><th id="1385">1385</th><td>          workspace_edge_added = <b>true</b>;</td></tr>
<tr><th id="1386">1386</th><td>          <i>// We found the edge that we were looking for, so break.</i></td></tr>
<tr><th id="1387">1387</th><td>          <b>break</b>;</td></tr>
<tr><th id="1388">1388</th><td>        }</td></tr>
<tr><th id="1389">1389</th><td>      }</td></tr>
<tr><th id="1390">1390</th><td></td></tr>
<tr><th id="1391">1391</th><td>      <b>if</b> (!workspace_edge_added) {</td></tr>
<tr><th id="1392">1392</th><td>        <i>// If we are here, then we did not find backward operator for this</i></td></tr>
<tr><th id="1393">1393</th><td><i>        // node.</i></td></tr>
<tr><th id="1394">1394</th><td>        nb-&gt;Attr(<q>"workspace_enabled"</q>, <b>false</b>);</td></tr>
<tr><th id="1395">1395</th><td>      }</td></tr>
<tr><th id="1396">1396</th><td>    } <b>else</b> <b>if</b> (orig_node-&gt;type_string() == ws.bwd_op &amp;&amp;</td></tr>
<tr><th id="1397">1397</th><td>               mkl_op_registry::IsMklOp(</td></tr>
<tr><th id="1398">1398</th><td>                   mkl_op_registry::GetMklOpName(orig_node-&gt;type_string()),</td></tr>
<tr><th id="1399">1399</th><td>                   T)) {</td></tr>
<tr><th id="1400">1400</th><td>      <i>// If this op is a bwd op, then we need to add workspace edge and</i></td></tr>
<tr><th id="1401">1401</th><td><i>      // it's Mkl tensor edge between its corresponding fwd op and this</i></td></tr>
<tr><th id="1402">1402</th><td><i>      // op. Corresponding fwd op is specified in 'fwd_op' field of</i></td></tr>
<tr><th id="1403">1403</th><td><i>      // workspace info. fwd_slot and bwd_slot in workspace info specify</i></td></tr>
<tr><th id="1404">1404</th><td><i>      // an edge between which slots connect forward and backward op.</i></td></tr>
<tr><th id="1405">1405</th><td><i>      // Once all these criteria match, we add a workspace edge between</i></td></tr>
<tr><th id="1406">1406</th><td><i>      // ws_fwd_slot and ws_bwd_slot. Its corresponding Mkl tensor is</i></td></tr>
<tr><th id="1407">1407</th><td><i>      // determined by interleaved/contiguous ordering. Function</i></td></tr>
<tr><th id="1408">1408</th><td><i>      // DataIndexToMetaDataIndex tells us the location of Mkl tensor</i></td></tr>
<tr><th id="1409">1409</th><td><i>      // from the location of the Tensorflow tensor.</i></td></tr>
<tr><th id="1410">1410</th><td>      <b>for</b> (<em>const</em> Edge* e : orig_node-&gt;in_edges()) {</td></tr>
<tr><th id="1411">1411</th><td>        <b>if</b> (e-&gt;src_output() == ws.fwd_slot &amp;&amp;</td></tr>
<tr><th id="1412">1412</th><td>            <i>// We would have rewritten the forward op, so we need to use</i></td></tr>
<tr><th id="1413">1413</th><td><i>            // GetMklOpName call to get its Mkl name.</i></td></tr>
<tr><th id="1414">1414</th><td>            e-&gt;src()-&gt;type_string() ==</td></tr>
<tr><th id="1415">1415</th><td>                mkl_op_registry::GetMklOpName(ws.fwd_op) &amp;&amp;</td></tr>
<tr><th id="1416">1416</th><td>            e-&gt;dst_input() == ws.bwd_slot) {</td></tr>
<tr><th id="1417">1417</th><td>          nb-&gt;Attr(<q>"workspace_enabled"</q>, <b>true</b>);</td></tr>
<tr><th id="1418">1418</th><td>          CHECK_NOTNULL(ws_tensors);</td></tr>
<tr><th id="1419">1419</th><td>          <i>// Add workspace edge between fwd op and bwd op.</i></td></tr>
<tr><th id="1420">1420</th><td>          ws_tensors-&gt;push_back(NodeBuilder::NodeOut(e-&gt;src(), ws.ws_fwd_slot));</td></tr>
<tr><th id="1421">1421</th><td>          <i>// Add Mkl tensor edge for workspace edge between fwd op and bwd op.</i></td></tr>
<tr><th id="1422">1422</th><td>          ws_tensors-&gt;push_back(NodeBuilder::NodeOut(</td></tr>
<tr><th id="1423">1423</th><td>              e-&gt;src(), DataIndexToMetaDataIndex(ws.ws_fwd_slot,</td></tr>
<tr><th id="1424">1424</th><td>                                                 e-&gt;src()-&gt;num_outputs())));</td></tr>
<tr><th id="1425">1425</th><td>          *are_ws_tensors_added = <b>true</b>;</td></tr>
<tr><th id="1426">1426</th><td>          <i>// In terms of input ordering, we add these calls to add Input</i></td></tr>
<tr><th id="1427">1427</th><td><i>          // here because workspace edge (and its Mkl tensor) is the last</i></td></tr>
<tr><th id="1428">1428</th><td><i>          // edge in the fwdop and bwdop. So all inputs before workspace</i></td></tr>
<tr><th id="1429">1429</th><td><i>          // tensor have been added by SetUpInputs function.</i></td></tr>
<tr><th id="1430">1430</th><td>          VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: workspace_enabled for "</q></td></tr>
<tr><th id="1431">1431</th><td>                  &lt;&lt; orig_node-&gt;type_string();</td></tr>
<tr><th id="1432">1432</th><td>          workspace_edge_added = <b>true</b>;</td></tr>
<tr><th id="1433">1433</th><td>          <i>// We found the edge that we were looking for, so break.</i></td></tr>
<tr><th id="1434">1434</th><td>          <b>break</b>;</td></tr>
<tr><th id="1435">1435</th><td>        }</td></tr>
<tr><th id="1436">1436</th><td>      }</td></tr>
<tr><th id="1437">1437</th><td></td></tr>
<tr><th id="1438">1438</th><td>      <i>// If we are here means we did not find fwd op that feeds to this</i></td></tr>
<tr><th id="1439">1439</th><td><i>      // bwd op. So in this case, we need to generate dummy tensors for</i></td></tr>
<tr><th id="1440">1440</th><td><i>      // workspace input and Mkl tensor for workspace, and set</i></td></tr>
<tr><th id="1441">1441</th><td><i>      // workspace_enabled to false.</i></td></tr>
<tr><th id="1442">1442</th><td>      <b>if</b> (!workspace_edge_added) {</td></tr>
<tr><th id="1443">1443</th><td>        nb-&gt;Attr(<q>"workspace_enabled"</q>, <b>false</b>);</td></tr>
<tr><th id="1444">1444</th><td>        Node* dmt_ws = <b>nullptr</b>;      <i>// Dummy tensor for workspace</i></td></tr>
<tr><th id="1445">1445</th><td>        Node* dmt_mkl_ws = <b>nullptr</b>;  <i>// Dummy Mkl tensor for workspace</i></td></tr>
<tr><th id="1446">1446</th><td>        GetDummyWorkspaceTensorNode(g, &amp;dmt_ws, orig_node);</td></tr>
<tr><th id="1447">1447</th><td>        GetDummyMklTensorNode(g, &amp;dmt_mkl_ws, orig_node);</td></tr>
<tr><th id="1448">1448</th><td>        CHECK_NOTNULL(dmt_ws);</td></tr>
<tr><th id="1449">1449</th><td>        CHECK_NOTNULL(dmt_mkl_ws);</td></tr>
<tr><th id="1450">1450</th><td>        CHECK_NOTNULL(ws_tensors);</td></tr>
<tr><th id="1451">1451</th><td>        <i>// We add dummy tensor as workspace tensor.</i></td></tr>
<tr><th id="1452">1452</th><td>        ws_tensors-&gt;push_back(NodeBuilder::NodeOut(dmt_ws, <var>0</var>));</td></tr>
<tr><th id="1453">1453</th><td>        <i>// We add dummy tensor as Mkl tensor for workspace tensor.</i></td></tr>
<tr><th id="1454">1454</th><td>        ws_tensors-&gt;push_back(NodeBuilder::NodeOut(dmt_mkl_ws, <var>0</var>));</td></tr>
<tr><th id="1455">1455</th><td>        *are_ws_tensors_added = <b>true</b>;</td></tr>
<tr><th id="1456">1456</th><td>        VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: dummy workspace_enabled for "</q></td></tr>
<tr><th id="1457">1457</th><td>                &lt;&lt; orig_node-&gt;type_string();</td></tr>
<tr><th id="1458">1458</th><td>      }</td></tr>
<tr><th id="1459">1459</th><td>    } <b>else</b> {</td></tr>
<tr><th id="1460">1460</th><td>      <i>// If this node does not match any workspace info, then we do not</i></td></tr>
<tr><th id="1461">1461</th><td><i>      // do anything special for workspace propagation for it.</i></td></tr>
<tr><th id="1462">1462</th><td>    }</td></tr>
<tr><th id="1463">1463</th><td>  }</td></tr>
<tr><th id="1464">1464</th><td>}</td></tr>
<tr><th id="1465">1465</th><td></td></tr>
<tr><th id="1466">1466</th><td><i>//////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="1467">1467</th><td><i>// Op-specific functions to copy attributes from old node to new node</i></td></tr>
<tr><th id="1468">1468</th><td><i>//////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="1469">1469</th><td></td></tr>
<tr><th id="1470">1470</th><td><em>void</em> MklLayoutRewritePass::CopyAttrsConv2D(<em>const</em> Node* orig_node,</td></tr>
<tr><th id="1471">1471</th><td>                                           NodeBuilder* nb) {</td></tr>
<tr><th id="1472">1472</th><td>  DataType T;</td></tr>
<tr><th id="1473">1473</th><td>  string data_format;</td></tr>
<tr><th id="1474">1474</th><td>  string padding;</td></tr>
<tr><th id="1475">1475</th><td>  std::vector&lt;int32&gt; strides;</td></tr>
<tr><th id="1476">1476</th><td>  <em>bool</em> use_cudnn_on_gpu;</td></tr>
<tr><th id="1477">1477</th><td></td></tr>
<tr><th id="1478">1478</th><td>  <i>// Get all attributes from old node.</i></td></tr>
<tr><th id="1479">1479</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="1480">1480</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"strides"</q>, &amp;strides));</td></tr>
<tr><th id="1481">1481</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"padding"</q>, &amp;padding));</td></tr>
<tr><th id="1482">1482</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"data_format"</q>, &amp;data_format));</td></tr>
<tr><th id="1483">1483</th><td>  TF_CHECK_OK(</td></tr>
<tr><th id="1484">1484</th><td>      GetNodeAttr(orig_node-&gt;def(), <q>"use_cudnn_on_gpu"</q>, &amp;use_cudnn_on_gpu));</td></tr>
<tr><th id="1485">1485</th><td></td></tr>
<tr><th id="1486">1486</th><td>  <i>// Add attributes to new node.</i></td></tr>
<tr><th id="1487">1487</th><td>  nb-&gt;Attr(<q>"T"</q>, T);</td></tr>
<tr><th id="1488">1488</th><td>  nb-&gt;Attr(<q>"strides"</q>, strides);</td></tr>
<tr><th id="1489">1489</th><td>  nb-&gt;Attr(<q>"padding"</q>, padding);</td></tr>
<tr><th id="1490">1490</th><td>  nb-&gt;Attr(<q>"data_format"</q>, data_format);</td></tr>
<tr><th id="1491">1491</th><td>  nb-&gt;Attr(<q>"use_cudnn_on_gpu"</q>, use_cudnn_on_gpu);</td></tr>
<tr><th id="1492">1492</th><td>}</td></tr>
<tr><th id="1493">1493</th><td></td></tr>
<tr><th id="1494">1494</th><td><em>void</em> MklLayoutRewritePass::CopyAttrsAddN(<em>const</em> Node* orig_node,</td></tr>
<tr><th id="1495">1495</th><td>                                         NodeBuilder* nb) {</td></tr>
<tr><th id="1496">1496</th><td>  DataType T;</td></tr>
<tr><th id="1497">1497</th><td>  <em>int</em> N;</td></tr>
<tr><th id="1498">1498</th><td></td></tr>
<tr><th id="1499">1499</th><td>  <i>// Get all attributes from old node.</i></td></tr>
<tr><th id="1500">1500</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="1501">1501</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"N"</q>, &amp;N));</td></tr>
<tr><th id="1502">1502</th><td></td></tr>
<tr><th id="1503">1503</th><td>  <i>// Add attributes to new node.</i></td></tr>
<tr><th id="1504">1504</th><td>  nb-&gt;Attr(<q>"T"</q>, T);</td></tr>
<tr><th id="1505">1505</th><td>  nb-&gt;Attr(<q>"N"</q>, N);</td></tr>
<tr><th id="1506">1506</th><td>}</td></tr>
<tr><th id="1507">1507</th><td></td></tr>
<tr><th id="1508">1508</th><td><em>void</em> MklLayoutRewritePass::CopyAttrsBiasAddGrad(<em>const</em> Node* orig_node,</td></tr>
<tr><th id="1509">1509</th><td>                                                NodeBuilder* nb) {</td></tr>
<tr><th id="1510">1510</th><td>  DataType T;</td></tr>
<tr><th id="1511">1511</th><td>  string data_format;</td></tr>
<tr><th id="1512">1512</th><td>  std::vector&lt;int32&gt; strides;</td></tr>
<tr><th id="1513">1513</th><td></td></tr>
<tr><th id="1514">1514</th><td>  <i>// Get all attributes from old node.</i></td></tr>
<tr><th id="1515">1515</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="1516">1516</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"strides"</q>, &amp;strides));</td></tr>
<tr><th id="1517">1517</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"data_format"</q>, &amp;data_format));</td></tr>
<tr><th id="1518">1518</th><td></td></tr>
<tr><th id="1519">1519</th><td>  <i>// Add attributes to new node.</i></td></tr>
<tr><th id="1520">1520</th><td>  nb-&gt;Attr(<q>"T"</q>, T);</td></tr>
<tr><th id="1521">1521</th><td>  nb-&gt;Attr(<q>"strides"</q>, strides);</td></tr>
<tr><th id="1522">1522</th><td>  nb-&gt;Attr(<q>"data_format"</q>, data_format);</td></tr>
<tr><th id="1523">1523</th><td>}</td></tr>
<tr><th id="1524">1524</th><td></td></tr>
<tr><th id="1525">1525</th><td><em>void</em> MklLayoutRewritePass::CopyAttrsIdentity(<em>const</em> Node* orig_node,</td></tr>
<tr><th id="1526">1526</th><td>                                             NodeBuilder* nb) {</td></tr>
<tr><th id="1527">1527</th><td>  DataType T;</td></tr>
<tr><th id="1528">1528</th><td></td></tr>
<tr><th id="1529">1529</th><td>  <i>// Get all attributes from old node.</i></td></tr>
<tr><th id="1530">1530</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="1531">1531</th><td>  <i>// Add attributes to new node.</i></td></tr>
<tr><th id="1532">1532</th><td>  nb-&gt;Attr(<q>"T"</q>, T);</td></tr>
<tr><th id="1533">1533</th><td>}</td></tr>
<tr><th id="1534">1534</th><td></td></tr>
<tr><th id="1535">1535</th><td><em>void</em> MklLayoutRewritePass::CopyAttrsLRN(<em>const</em> Node* orig_node,</td></tr>
<tr><th id="1536">1536</th><td>                                        NodeBuilder* nb) {</td></tr>
<tr><th id="1537">1537</th><td>  DataType T;</td></tr>
<tr><th id="1538">1538</th><td>  <em>int</em> depth_radius;</td></tr>
<tr><th id="1539">1539</th><td>  <em>float</em> bias;</td></tr>
<tr><th id="1540">1540</th><td>  <em>float</em> alpha;</td></tr>
<tr><th id="1541">1541</th><td>  <em>float</em> beta;</td></tr>
<tr><th id="1542">1542</th><td></td></tr>
<tr><th id="1543">1543</th><td>  <i>// Get all attributes from old node.</i></td></tr>
<tr><th id="1544">1544</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="1545">1545</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"depth_radius"</q>, &amp;depth_radius));</td></tr>
<tr><th id="1546">1546</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"bias"</q>, &amp;bias));</td></tr>
<tr><th id="1547">1547</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"alpha"</q>, &amp;alpha));</td></tr>
<tr><th id="1548">1548</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"beta"</q>, &amp;beta));</td></tr>
<tr><th id="1549">1549</th><td></td></tr>
<tr><th id="1550">1550</th><td>  <i>// Add attributes to new node.</i></td></tr>
<tr><th id="1551">1551</th><td>  nb-&gt;Attr(<q>"T"</q>, T);</td></tr>
<tr><th id="1552">1552</th><td>  nb-&gt;Attr(<q>"depth_radius"</q>, depth_radius);</td></tr>
<tr><th id="1553">1553</th><td>  nb-&gt;Attr(<q>"bias"</q>, bias);</td></tr>
<tr><th id="1554">1554</th><td>  nb-&gt;Attr(<q>"alpha"</q>, alpha);</td></tr>
<tr><th id="1555">1555</th><td>  nb-&gt;Attr(<q>"beta"</q>, beta);</td></tr>
<tr><th id="1556">1556</th><td>}</td></tr>
<tr><th id="1557">1557</th><td></td></tr>
<tr><th id="1558">1558</th><td><em>void</em> MklLayoutRewritePass::CopyAttrsPooling(<em>const</em> Node* orig_node,</td></tr>
<tr><th id="1559">1559</th><td>                                            NodeBuilder* nb) {</td></tr>
<tr><th id="1560">1560</th><td>  DataType T;</td></tr>
<tr><th id="1561">1561</th><td>  string data_format;</td></tr>
<tr><th id="1562">1562</th><td>  string padding;</td></tr>
<tr><th id="1563">1563</th><td>  std::vector&lt;int32&gt; ksize, strides;</td></tr>
<tr><th id="1564">1564</th><td></td></tr>
<tr><th id="1565">1565</th><td>  <i>// Get all attributes from old node.</i></td></tr>
<tr><th id="1566">1566</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="1567">1567</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"ksize"</q>, &amp;ksize));</td></tr>
<tr><th id="1568">1568</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"strides"</q>, &amp;strides));</td></tr>
<tr><th id="1569">1569</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"padding"</q>, &amp;padding));</td></tr>
<tr><th id="1570">1570</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"data_format"</q>, &amp;data_format));</td></tr>
<tr><th id="1571">1571</th><td></td></tr>
<tr><th id="1572">1572</th><td>  <i>// Add attributes to new node.</i></td></tr>
<tr><th id="1573">1573</th><td>  nb-&gt;Attr(<q>"T"</q>, T);</td></tr>
<tr><th id="1574">1574</th><td>  nb-&gt;Attr(<q>"ksize"</q>, ksize);</td></tr>
<tr><th id="1575">1575</th><td>  nb-&gt;Attr(<q>"strides"</q>, strides);</td></tr>
<tr><th id="1576">1576</th><td>  nb-&gt;Attr(<q>"padding"</q>, padding);</td></tr>
<tr><th id="1577">1577</th><td>  nb-&gt;Attr(<q>"data_format"</q>, data_format);</td></tr>
<tr><th id="1578">1578</th><td>}</td></tr>
<tr><th id="1579">1579</th><td></td></tr>
<tr><th id="1580">1580</th><td><em>void</em> MklLayoutRewritePass::CopyAttrsDataType(<em>const</em> Node* orig_node,</td></tr>
<tr><th id="1581">1581</th><td>                                             NodeBuilder* nb) {</td></tr>
<tr><th id="1582">1582</th><td>  DataType T;</td></tr>
<tr><th id="1583">1583</th><td></td></tr>
<tr><th id="1584">1584</th><td>  <i>// Get all attributes from old node.</i></td></tr>
<tr><th id="1585">1585</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="1586">1586</th><td></td></tr>
<tr><th id="1587">1587</th><td>  <i>// Add attributes to new node.</i></td></tr>
<tr><th id="1588">1588</th><td>  nb-&gt;Attr(<q>"T"</q>, T);</td></tr>
<tr><th id="1589">1589</th><td>}</td></tr>
<tr><th id="1590">1590</th><td></td></tr>
<tr><th id="1591">1591</th><td><em>void</em> MklLayoutRewritePass::CopyAttrsReshape(<em>const</em> Node* orig_node,</td></tr>
<tr><th id="1592">1592</th><td>                                            NodeBuilder* nb) {</td></tr>
<tr><th id="1593">1593</th><td>  DataType T;</td></tr>
<tr><th id="1594">1594</th><td>  DataType Tshape;</td></tr>
<tr><th id="1595">1595</th><td></td></tr>
<tr><th id="1596">1596</th><td>  <i>// Get all attributes from old node.</i></td></tr>
<tr><th id="1597">1597</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="1598">1598</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"Tshape"</q>, &amp;Tshape));</td></tr>
<tr><th id="1599">1599</th><td>  <i>// Add attributes to new node.</i></td></tr>
<tr><th id="1600">1600</th><td>  nb-&gt;Attr(<q>"T"</q>, T);</td></tr>
<tr><th id="1601">1601</th><td>  nb-&gt;Attr(<q>"Tshape"</q>, Tshape);</td></tr>
<tr><th id="1602">1602</th><td>}</td></tr>
<tr><th id="1603">1603</th><td></td></tr>
<tr><th id="1604">1604</th><td><em>void</em> MklLayoutRewritePass::CopyAttrsSplit(<em>const</em> Node* orig_node,</td></tr>
<tr><th id="1605">1605</th><td>                                          NodeBuilder* nb) {</td></tr>
<tr><th id="1606">1606</th><td>  DataType T;</td></tr>
<tr><th id="1607">1607</th><td>  string data_format;</td></tr>
<tr><th id="1608">1608</th><td>  <em>int</em> num_split;</td></tr>
<tr><th id="1609">1609</th><td></td></tr>
<tr><th id="1610">1610</th><td>  <i>// Get all attributes from old node.</i></td></tr>
<tr><th id="1611">1611</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="1612">1612</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"num_split"</q>, &amp;num_split));</td></tr>
<tr><th id="1613">1613</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"data_format"</q>, &amp;data_format));</td></tr>
<tr><th id="1614">1614</th><td></td></tr>
<tr><th id="1615">1615</th><td>  <i>// Add attributes to new node.</i></td></tr>
<tr><th id="1616">1616</th><td>  nb-&gt;Attr(<q>"T"</q>, T);</td></tr>
<tr><th id="1617">1617</th><td>  nb-&gt;Attr(<q>"num_split"</q>, num_split);</td></tr>
<tr><th id="1618">1618</th><td>  nb-&gt;Attr(<q>"data_format"</q>, data_format);</td></tr>
<tr><th id="1619">1619</th><td>}</td></tr>
<tr><th id="1620">1620</th><td></td></tr>
<tr><th id="1621">1621</th><td><em>void</em> MklLayoutRewritePass::CopyAttrsConcat(<em>const</em> Node* orig_node,</td></tr>
<tr><th id="1622">1622</th><td>                                           NodeBuilder* nb) {</td></tr>
<tr><th id="1623">1623</th><td>  DataType T;</td></tr>
<tr><th id="1624">1624</th><td>  <em>int</em> N;</td></tr>
<tr><th id="1625">1625</th><td></td></tr>
<tr><th id="1626">1626</th><td>  <i>// Get all attributes from old node.</i></td></tr>
<tr><th id="1627">1627</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="1628">1628</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"N"</q>, &amp;N));</td></tr>
<tr><th id="1629">1629</th><td></td></tr>
<tr><th id="1630">1630</th><td>  <i>// Add attributes to new node.</i></td></tr>
<tr><th id="1631">1631</th><td>  nb-&gt;Attr(<q>"T"</q>, T);</td></tr>
<tr><th id="1632">1632</th><td>  nb-&gt;Attr(<q>"N"</q>, N);</td></tr>
<tr><th id="1633">1633</th><td>}</td></tr>
<tr><th id="1634">1634</th><td></td></tr>
<tr><th id="1635">1635</th><td><em>void</em> MklLayoutRewritePass::CopyAttrsConcatV2(<em>const</em> Node* orig_node,</td></tr>
<tr><th id="1636">1636</th><td>                                             NodeBuilder* nb) {</td></tr>
<tr><th id="1637">1637</th><td>  DataType T;</td></tr>
<tr><th id="1638">1638</th><td>  <em>int</em> N;</td></tr>
<tr><th id="1639">1639</th><td>  DataType tidx;</td></tr>
<tr><th id="1640">1640</th><td></td></tr>
<tr><th id="1641">1641</th><td>  <i>// Get all attributes from old node.</i></td></tr>
<tr><th id="1642">1642</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="1643">1643</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"N"</q>, &amp;N));</td></tr>
<tr><th id="1644">1644</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"Tidx"</q>, &amp;tidx));</td></tr>
<tr><th id="1645">1645</th><td></td></tr>
<tr><th id="1646">1646</th><td>  <i>// Add attributes to new node.</i></td></tr>
<tr><th id="1647">1647</th><td>  nb-&gt;Attr(<q>"T"</q>, T);</td></tr>
<tr><th id="1648">1648</th><td>  nb-&gt;Attr(<q>"N"</q>, N);</td></tr>
<tr><th id="1649">1649</th><td>  nb-&gt;Attr(<q>"Tidx"</q>, tidx);</td></tr>
<tr><th id="1650">1650</th><td>}</td></tr>
<tr><th id="1651">1651</th><td></td></tr>
<tr><th id="1652">1652</th><td><em>void</em> MklLayoutRewritePass::CopyAttrsFusedBatchNorm(<em>const</em> Node* orig_node,</td></tr>
<tr><th id="1653">1653</th><td>                                                   NodeBuilder* nb) {</td></tr>
<tr><th id="1654">1654</th><td>  DataType T;</td></tr>
<tr><th id="1655">1655</th><td>  <em>float</em> epsilon;</td></tr>
<tr><th id="1656">1656</th><td>  string data_format;</td></tr>
<tr><th id="1657">1657</th><td>  <em>bool</em> is_training;</td></tr>
<tr><th id="1658">1658</th><td></td></tr>
<tr><th id="1659">1659</th><td>  <i>// Get all attributes from old node.</i></td></tr>
<tr><th id="1660">1660</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="1661">1661</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"epsilon"</q>, &amp;epsilon));</td></tr>
<tr><th id="1662">1662</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"data_format"</q>, &amp;data_format));</td></tr>
<tr><th id="1663">1663</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"is_training"</q>, &amp;is_training));</td></tr>
<tr><th id="1664">1664</th><td></td></tr>
<tr><th id="1665">1665</th><td>  <i>// Add attributes to new node.</i></td></tr>
<tr><th id="1666">1666</th><td>  nb-&gt;Attr(<q>"T"</q>, T);</td></tr>
<tr><th id="1667">1667</th><td>  nb-&gt;Attr(<q>"epsilon"</q>, epsilon);</td></tr>
<tr><th id="1668">1668</th><td>  nb-&gt;Attr(<q>"data_format"</q>, data_format);</td></tr>
<tr><th id="1669">1669</th><td>  nb-&gt;Attr(<q>"is_training"</q>, is_training);</td></tr>
<tr><th id="1670">1670</th><td>}</td></tr>
<tr><th id="1671">1671</th><td></td></tr>
<tr><th id="1672">1672</th><td><i>//////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="1673">1673</th><td><i>//           Helper functions related to node merge pass</i></td></tr>
<tr><th id="1674">1674</th><td><i>//////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="1675">1675</th><td></td></tr>
<tr><th id="1676">1676</th><td>Node* MklLayoutRewritePass::CheckForNodeMerge(<em>const</em> Node* a) <em>const</em> {</td></tr>
<tr><th id="1677">1677</th><td>  <i>// TODO(nhasabni) Add check for type of node similar to CheckForNodeRewrite</i></td></tr>
<tr><th id="1678">1678</th><td><i>  // once we support BiasAddGrad as Mkl layer.</i></td></tr>
<tr><th id="1679">1679</th><td><i></i></td></tr>
<tr><th id="1680">1680</th><td><i>  // Search for all matching mergeinfo.</i></td></tr>
<tr><th id="1681">1681</th><td><i>  // We allow more than one match for extensibility.</i></td></tr>
<tr><th id="1682">1682</th><td>  std::vector&lt;<em>const</em> MergeInfo*&gt; matching_mi;</td></tr>
<tr><th id="1683">1683</th><td>  <b>for</b> (<em>auto</em> mi = minfo_.cbegin(); mi != minfo_.cend(); ++mi) {</td></tr>
<tr><th id="1684">1684</th><td>    <b>if</b> (a-&gt;type_string() == mi-&gt;succ) {</td></tr>
<tr><th id="1685">1685</th><td>      matching_mi.push_back(&amp;*mi);</td></tr>
<tr><th id="1686">1686</th><td>    }</td></tr>
<tr><th id="1687">1687</th><td>  }</td></tr>
<tr><th id="1688">1688</th><td></td></tr>
<tr><th id="1689">1689</th><td>  <b>for</b> (<em>const</em> MergeInfo* mi : matching_mi) {</td></tr>
<tr><th id="1690">1690</th><td>    <em>const</em> <em>int</em> N_in = a-&gt;num_inputs();</td></tr>
<tr><th id="1691">1691</th><td>    <b>if</b> (mi-&gt;op &gt;= N_in) {</td></tr>
<tr><th id="1692">1692</th><td>      <b>continue</b>;</td></tr>
<tr><th id="1693">1693</th><td>    }</td></tr>
<tr><th id="1694">1694</th><td></td></tr>
<tr><th id="1695">1695</th><td>    <i>// Get the control edges and input of node</i></td></tr>
<tr><th id="1696">1696</th><td>    gtl::InlinedVector&lt;Node*, <var>4</var>&gt; a_control_edges;</td></tr>
<tr><th id="1697">1697</th><td>    gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt; a_in(N_in);</td></tr>
<tr><th id="1698">1698</th><td>    FillInputs(a, &amp;a_control_edges, &amp;a_in);</td></tr>
<tr><th id="1699">1699</th><td></td></tr>
<tr><th id="1700">1700</th><td>    <i>// Get operand op of the operator</i></td></tr>
<tr><th id="1701">1701</th><td>    Node* b = <b>nullptr</b>;</td></tr>
<tr><th id="1702">1702</th><td>    b = a_in[mi-&gt;op].first;</td></tr>
<tr><th id="1703">1703</th><td>    <b>if</b> (b == <b>nullptr</b> || (b-&gt;type_string() != mi-&gt;pred)) {</td></tr>
<tr><th id="1704">1704</th><td>      <i>// NOTE: Should the first check be assert?</i></td></tr>
<tr><th id="1705">1705</th><td>      <b>continue</b>;</td></tr>
<tr><th id="1706">1706</th><td>    }</td></tr>
<tr><th id="1707">1707</th><td></td></tr>
<tr><th id="1708">1708</th><td>    <em>const</em> <em>int</em> B_in = b-&gt;num_inputs();</td></tr>
<tr><th id="1709">1709</th><td>    gtl::InlinedVector&lt;Node*, <var>4</var>&gt; b_control_edges;</td></tr>
<tr><th id="1710">1710</th><td>    gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt; b_in(B_in);</td></tr>
<tr><th id="1711">1711</th><td>    FillInputs(b, &amp;b_control_edges, &amp;b_in);</td></tr>
<tr><th id="1712">1712</th><td></td></tr>
<tr><th id="1713">1713</th><td>    <i>// Shouldn't merge if a and b have different control edges.</i></td></tr>
<tr><th id="1714">1714</th><td>    <b>if</b> (a_control_edges != b_control_edges) {</td></tr>
<tr><th id="1715">1715</th><td>      <b>continue</b>;</td></tr>
<tr><th id="1716">1716</th><td>    } <b>else</b> {</td></tr>
<tr><th id="1717">1717</th><td>      <i>// We found a match.</i></td></tr>
<tr><th id="1718">1718</th><td>      <b>return</b> b;</td></tr>
<tr><th id="1719">1719</th><td>    }</td></tr>
<tr><th id="1720">1720</th><td>  }</td></tr>
<tr><th id="1721">1721</th><td></td></tr>
<tr><th id="1722">1722</th><td>  <b>return</b> <b>nullptr</b>;</td></tr>
<tr><th id="1723">1723</th><td>}</td></tr>
<tr><th id="1724">1724</th><td></td></tr>
<tr><th id="1725">1725</th><td>Status MklLayoutRewritePass::MergeNode(std::unique_ptr&lt;Graph&gt;* g, Node* succ,</td></tr>
<tr><th id="1726">1726</th><td>                                       Node* pred) {</td></tr>
<tr><th id="1727">1727</th><td>  CHECK_NOTNULL(succ);</td></tr>
<tr><th id="1728">1728</th><td>  CHECK_NOTNULL(pred);</td></tr>
<tr><th id="1729">1729</th><td></td></tr>
<tr><th id="1730">1730</th><td>  <b>if</b> (succ-&gt;type_string() == csinfo_.bias_add &amp;&amp;</td></tr>
<tr><th id="1731">1731</th><td>      pred-&gt;type_string() == csinfo_.mkl_conv2d) {</td></tr>
<tr><th id="1732">1732</th><td>    <i>// 1. Get all attributes from input nodes.</i></td></tr>
<tr><th id="1733">1733</th><td>    DataType T_pred, T_succ;</td></tr>
<tr><th id="1734">1734</th><td>    string padding;</td></tr>
<tr><th id="1735">1735</th><td>    std::vector&lt;int32&gt; strides;</td></tr>
<tr><th id="1736">1736</th><td>    string data_format_pred, data_format_succ;</td></tr>
<tr><th id="1737">1737</th><td>    <em>bool</em> use_cudnn_on_gnu;</td></tr>
<tr><th id="1738">1738</th><td>    TF_CHECK_OK(GetNodeAttr(pred-&gt;def(), <q>"T"</q>, &amp;T_pred));</td></tr>
<tr><th id="1739">1739</th><td>    TF_CHECK_OK(GetNodeAttr(succ-&gt;def(), <q>"T"</q>, &amp;T_succ));</td></tr>
<tr><th id="1740">1740</th><td>    TF_CHECK_OK(GetNodeAttr(pred-&gt;def(), <q>"padding"</q>, &amp;padding));</td></tr>
<tr><th id="1741">1741</th><td>    TF_CHECK_OK(GetNodeAttr(pred-&gt;def(), <q>"strides"</q>, &amp;strides));</td></tr>
<tr><th id="1742">1742</th><td>    TF_CHECK_OK(GetNodeAttr(pred-&gt;def(), <q>"data_format"</q>, &amp;data_format_pred));</td></tr>
<tr><th id="1743">1743</th><td>    TF_CHECK_OK(GetNodeAttr(succ-&gt;def(), <q>"data_format"</q>, &amp;data_format_succ));</td></tr>
<tr><th id="1744">1744</th><td>    TF_CHECK_OK(</td></tr>
<tr><th id="1745">1745</th><td>        GetNodeAttr(pred-&gt;def(), <q>"use_cudnn_on_gpu"</q>, &amp;use_cudnn_on_gnu));</td></tr>
<tr><th id="1746">1746</th><td>    <i>// We check to ensure that data formats of both succ and pred are same.</i></td></tr>
<tr><th id="1747">1747</th><td><i>    // We expect them to be same, so we can enforce this as assert.</i></td></tr>
<tr><th id="1748">1748</th><td><i>    // But assert can be too strict, so we enforce this as a check.</i></td></tr>
<tr><th id="1749">1749</th><td><i>    // If the check fails, then we do not merge two nodes.</i></td></tr>
<tr><th id="1750">1750</th><td><i>    // We also do same check for devices.</i></td></tr>
<tr><th id="1751">1751</th><td>    <b>if</b> (data_format_pred != data_format_succ || T_pred != T_succ ||</td></tr>
<tr><th id="1752">1752</th><td>        pred-&gt;assigned_device_name() != succ-&gt;assigned_device_name() ||</td></tr>
<tr><th id="1753">1753</th><td>        pred-&gt;def().device() != succ-&gt;def().device()) {</td></tr>
<tr><th id="1754">1754</th><td>      <b>return</b> Status(error::Code::INVALID_ARGUMENT,</td></tr>
<tr><th id="1755">1755</th><td>                    <q>"data_format or T attribute or devices of Conv2D and "</q></td></tr>
<tr><th id="1756">1756</th><td>                    <q>"BiasAdd do not match. Will skip node merge optimization"</q>);</td></tr>
<tr><th id="1757">1757</th><td>    }</td></tr>
<tr><th id="1758">1758</th><td></td></tr>
<tr><th id="1759">1759</th><td>    <em>const</em> <em>int</em> succ_num = succ-&gt;num_inputs();</td></tr>
<tr><th id="1760">1760</th><td>    gtl::InlinedVector&lt;Node*, <var>4</var>&gt; succ_control_edges;</td></tr>
<tr><th id="1761">1761</th><td>    gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt; succ_in(succ_num);</td></tr>
<tr><th id="1762">1762</th><td>    FillInputs(succ, &amp;succ_control_edges, &amp;succ_in);</td></tr>
<tr><th id="1763">1763</th><td></td></tr>
<tr><th id="1764">1764</th><td>    <em>const</em> <em>int</em> pred_num = pred-&gt;num_inputs();</td></tr>
<tr><th id="1765">1765</th><td>    gtl::InlinedVector&lt;Node*, <var>4</var>&gt; pred_control_edges;</td></tr>
<tr><th id="1766">1766</th><td>    gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt; pred_in(pred_num);</td></tr>
<tr><th id="1767">1767</th><td>    FillInputs(pred, &amp;pred_control_edges, &amp;pred_in);</td></tr>
<tr><th id="1768">1768</th><td></td></tr>
<tr><th id="1769">1769</th><td>    <i>// We need to ensure that there is only 1 edge between Conv2D and AddBias.</i></td></tr>
<tr><th id="1770">1770</th><td><i>    // Otherwise, merging is semantically incorrect.</i></td></tr>
<tr><th id="1771">1771</th><td>    <b>if</b> (pred-&gt;out_edges().size() != <var>1</var>) {</td></tr>
<tr><th id="1772">1772</th><td>      <b>return</b> Status(error::Code::INVALID_ARGUMENT,</td></tr>
<tr><th id="1773">1773</th><td>                    <q>"Conv2D has multiple outputs."</q></td></tr>
<tr><th id="1774">1774</th><td>                    <q>"Will skip node merge optimization"</q>);</td></tr>
<tr><th id="1775">1775</th><td>    }</td></tr>
<tr><th id="1776">1776</th><td></td></tr>
<tr><th id="1777">1777</th><td>    <b>for</b> (<em>const</em> Edge* e : pred-&gt;out_edges()) {</td></tr>
<tr><th id="1778">1778</th><td>      <b>if</b> (e-&gt;dst() != succ) {</td></tr>
<tr><th id="1779">1779</th><td>        <b>return</b> Status(error::Code::INVALID_ARGUMENT,</td></tr>
<tr><th id="1780">1780</th><td>                      <q>"Conv2D does not feed to BiasAdd."</q></td></tr>
<tr><th id="1781">1781</th><td>                      <q>"Will skip node merge optimization"</q>);</td></tr>
<tr><th id="1782">1782</th><td>      }</td></tr>
<tr><th id="1783">1783</th><td>    }</td></tr>
<tr><th id="1784">1784</th><td></td></tr>
<tr><th id="1785">1785</th><td>    <i>// 2. Get inputs from both the nodes.</i></td></tr>
<tr><th id="1786">1786</th><td><i>    // Find the 2 inputs from the conv and the bias from the add Bias.</i></td></tr>
<tr><th id="1787">1787</th><td><i>    // Get operand 0, 1 of conv2D and their Mkl tensors.</i></td></tr>
<tr><th id="1788">1788</th><td>    CHECK_EQ(pred-&gt;in_edges().size(), <var>4</var>);  <i>// _MklConv2D must have 4 inputs.</i></td></tr>
<tr><th id="1789">1789</th><td>    <i>// Get operand 1 of add_bias</i></td></tr>
<tr><th id="1790">1790</th><td><i>    // BiasAdd must have 2 inputs: Conv, bias</i></td></tr>
<tr><th id="1791">1791</th><td>    CHECK_EQ(succ-&gt;in_edges().size(), <var>2</var>);</td></tr>
<tr><th id="1792">1792</th><td>    Node* oper3_mkl = <b>nullptr</b>;  <i>// Mkl tensor corresponding to oper3</i></td></tr>
<tr><th id="1793">1793</th><td>    <em>int</em> oper3_mkl_slot = <var>0</var>;     <i>// For dummy MKL tensor node, output slot is 0.</i></td></tr>
<tr><th id="1794">1794</th><td>    GetDummyMklTensorNode(g, &amp;oper3_mkl, pred);  <i>// Get dummy Mkl tensor node</i></td></tr>
<tr><th id="1795">1795</th><td>    <i>// as BiasAdd does not have Mkl tensor as input.</i></td></tr>
<tr><th id="1796">1796</th><td>    CHECK_NOTNULL(oper3_mkl);</td></tr>
<tr><th id="1797">1797</th><td></td></tr>
<tr><th id="1798">1798</th><td>    <i>// We will use the node name of BiasAdd as the name of new node</i></td></tr>
<tr><th id="1799">1799</th><td><i>    // Build new node. We use same name as original node, but change the op</i></td></tr>
<tr><th id="1800">1800</th><td><i>    // name.</i></td></tr>
<tr><th id="1801">1801</th><td>    NodeBuilder nb(succ-&gt;name(), csinfo_.mkl_conv2d_with_bias);</td></tr>
<tr><th id="1802">1802</th><td>    <b>if</b> (kTensorOrdering == MklTfTensorOrdering::TENSORS_INTERLEAVED) {</td></tr>
<tr><th id="1803">1803</th><td>      nb.Input(pred_in[<var>0</var>].first, pred_in[<var>0</var>].second);  <i>// In1 of Conv2D</i></td></tr>
<tr><th id="1804">1804</th><td>      <i>// pred_in[1] will be Mkl tensor for In1 if we follow interleaved</i></td></tr>
<tr><th id="1805">1805</th><td><i>      // ordering, and it will be 2nd Tensorflow tensor for Conv2D if</i></td></tr>
<tr><th id="1806">1806</th><td><i>      // we follow contiguous ordering.</i></td></tr>
<tr><th id="1807">1807</th><td>      nb.Input(pred_in[<var>1</var>].first, pred_in[<var>1</var>].second);  <i>// Mkl for In1</i></td></tr>
<tr><th id="1808">1808</th><td>      nb.Input(pred_in[<var>2</var>].first, pred_in[<var>2</var>].second);  <i>// In2 of Conv2D</i></td></tr>
<tr><th id="1809">1809</th><td>      nb.Input(pred_in[<var>3</var>].first, pred_in[<var>3</var>].second);  <i>// Mkl for In2</i></td></tr>
<tr><th id="1810">1810</th><td>      nb.Input(succ_in[<var>1</var>].first, succ_in[<var>1</var>].second);  <i>// In2 of BiasAdd</i></td></tr>
<tr><th id="1811">1811</th><td>      nb.Input(oper3_mkl, oper3_mkl_slot);            <i>// Mkl for In2 of BiasAdd</i></td></tr>
<tr><th id="1812">1812</th><td>    } <b>else</b> {</td></tr>
<tr><th id="1813">1813</th><td>      CHECK_EQ(kTensorOrdering, MklTfTensorOrdering::TENSORS_CONTIGUOUS);</td></tr>
<tr><th id="1814">1814</th><td>      nb.Input(pred_in[<var>0</var>].first, pred_in[<var>0</var>].second);  <i>// In1 of Conv2D</i></td></tr>
<tr><th id="1815">1815</th><td>      <i>// pred_in[1] will be Mkl tensor for In1 if we follow interleaved</i></td></tr>
<tr><th id="1816">1816</th><td><i>      // ordering, and it will be 2nd Tensorflow tensor for Conv2D if</i></td></tr>
<tr><th id="1817">1817</th><td><i>      // we follow contiguous ordering.</i></td></tr>
<tr><th id="1818">1818</th><td>      nb.Input(pred_in[<var>1</var>].first, pred_in[<var>1</var>].second);  <i>// In2 of Conv2D</i></td></tr>
<tr><th id="1819">1819</th><td>      nb.Input(succ_in[<var>1</var>].first, succ_in[<var>1</var>].second);  <i>// In2 of BiasAdd</i></td></tr>
<tr><th id="1820">1820</th><td>      nb.Input(pred_in[<var>2</var>].first, pred_in[<var>2</var>].second);  <i>// Mkl for In1 of Conv2D</i></td></tr>
<tr><th id="1821">1821</th><td>      nb.Input(pred_in[<var>3</var>].first, pred_in[<var>3</var>].second);  <i>// Mkl for In2 of Conv2D</i></td></tr>
<tr><th id="1822">1822</th><td>      nb.Input(oper3_mkl, oper3_mkl_slot);            <i>// Mkl for In2 of BiasAdd</i></td></tr>
<tr><th id="1823">1823</th><td>    }</td></tr>
<tr><th id="1824">1824</th><td></td></tr>
<tr><th id="1825">1825</th><td>    <i>// Copy attributes from Conv2D to Conv2DWithBias.</i></td></tr>
<tr><th id="1826">1826</th><td>    CopyAttrsConv2D(<b>const_cast</b>&lt;<em>const</em> Node*&gt;(pred), &amp;nb);</td></tr>
<tr><th id="1827">1827</th><td></td></tr>
<tr><th id="1828">1828</th><td>    <i>// Copy the device assigned to old node to new node.</i></td></tr>
<tr><th id="1829">1829</th><td>    nb.Device(succ-&gt;def().device());</td></tr>
<tr><th id="1830">1830</th><td></td></tr>
<tr><th id="1831">1831</th><td>    <i>// Create node.</i></td></tr>
<tr><th id="1832">1832</th><td>    Node* new_node;</td></tr>
<tr><th id="1833">1833</th><td>    TF_CHECK_OK(nb.Finalize(&amp;**g, &amp;new_node));</td></tr>
<tr><th id="1834">1834</th><td>    CHECK_NOTNULL(new_node);</td></tr>
<tr><th id="1835">1835</th><td></td></tr>
<tr><th id="1836">1836</th><td>    <i>// Set the Mkl layer label for this op.</i></td></tr>
<tr><th id="1837">1837</th><td>    new_node-&gt;AddAttr(<q>"_kernel"</q>, mkl_op_registry::kMklOpLabel);</td></tr>
<tr><th id="1838">1838</th><td></td></tr>
<tr><th id="1839">1839</th><td>    <i>// Incoming data edges from 'pred' node and 'succ' node to new 'new_node'</i></td></tr>
<tr><th id="1840">1840</th><td><i>    // node are already copied in BuildNode. We handle control edges now.</i></td></tr>
<tr><th id="1841">1841</th><td>    <b>for</b> (<em>const</em> Edge* e : pred-&gt;in_edges()) {</td></tr>
<tr><th id="1842">1842</th><td>      <b>if</b> (e-&gt;IsControlEdge()) {</td></tr>
<tr><th id="1843">1843</th><td>        CHECK_NOTNULL((*g)-&gt;AddControlEdge(e-&gt;src(), new_node));</td></tr>
<tr><th id="1844">1844</th><td>      }</td></tr>
<tr><th id="1845">1845</th><td>    }</td></tr>
<tr><th id="1846">1846</th><td>    <b>for</b> (<em>const</em> Edge* e : succ-&gt;in_edges()) {</td></tr>
<tr><th id="1847">1847</th><td>      <b>if</b> (e-&gt;IsControlEdge()) {</td></tr>
<tr><th id="1848">1848</th><td>        CHECK_NOTNULL((*g)-&gt;AddControlEdge(e-&gt;src(), new_node));</td></tr>
<tr><th id="1849">1849</th><td>      }</td></tr>
<tr><th id="1850">1850</th><td>    }</td></tr>
<tr><th id="1851">1851</th><td></td></tr>
<tr><th id="1852">1852</th><td>    <i>// Incoming edges are fixed, we will fix the outgoing edges now.</i></td></tr>
<tr><th id="1853">1853</th><td><i>    // First, we will fix outgoing control edges from 'pred' node.</i></td></tr>
<tr><th id="1854">1854</th><td><i>    // We don't need to handle outgoing data edges from 'pred' node</i></td></tr>
<tr><th id="1855">1855</th><td><i>    // because pred has only 1 output going to succ node (we enforced</i></td></tr>
<tr><th id="1856">1856</th><td><i>    // this check for merge already).</i></td></tr>
<tr><th id="1857">1857</th><td>    <b>for</b> (<em>const</em> Edge* e : pred-&gt;out_edges()) {</td></tr>
<tr><th id="1858">1858</th><td>      <b>if</b> (e-&gt;IsControlEdge()) {</td></tr>
<tr><th id="1859">1859</th><td>        CHECK_NOTNULL((*g)-&gt;AddControlEdge(new_node, e-&gt;dst()));</td></tr>
<tr><th id="1860">1860</th><td>      }</td></tr>
<tr><th id="1861">1861</th><td>    }</td></tr>
<tr><th id="1862">1862</th><td></td></tr>
<tr><th id="1863">1863</th><td>    <i>// Second, we will fix outgoing control and data edges from 'succ' node.</i></td></tr>
<tr><th id="1864">1864</th><td>    <b>for</b> (<em>const</em> Edge* e : succ-&gt;out_edges()) {</td></tr>
<tr><th id="1865">1865</th><td>      <b>if</b> (e-&gt;IsControlEdge()) {</td></tr>
<tr><th id="1866">1866</th><td>        CHECK_NOTNULL((*g)-&gt;AddControlEdge(new_node, e-&gt;dst()));</td></tr>
<tr><th id="1867">1867</th><td>      } <b>else</b> {</td></tr>
<tr><th id="1868">1868</th><td>        CHECK_NOTNULL(</td></tr>
<tr><th id="1869">1869</th><td>            (*g)-&gt;AddEdge(new_node, e-&gt;src_output(), e-&gt;dst(), e-&gt;dst_input()));</td></tr>
<tr><th id="1870">1870</th><td>      }</td></tr>
<tr><th id="1871">1871</th><td>    }</td></tr>
<tr><th id="1872">1872</th><td></td></tr>
<tr><th id="1873">1873</th><td>    <i>// Copy device assigned to old node to new node.</i></td></tr>
<tr><th id="1874">1874</th><td><i>    // It's ok to use pred or succ as we have enforced a check that</i></td></tr>
<tr><th id="1875">1875</th><td><i>    // both have same device assigned.</i></td></tr>
<tr><th id="1876">1876</th><td>    new_node-&gt;set_assigned_device_name(pred-&gt;assigned_device_name());</td></tr>
<tr><th id="1877">1877</th><td></td></tr>
<tr><th id="1878">1878</th><td>    VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: Merged old node:"</q> &lt;&lt; pred-&gt;DebugString()</td></tr>
<tr><th id="1879">1879</th><td>            &lt;&lt; <q>", and node: "</q> &lt;&lt; succ-&gt;DebugString()</td></tr>
<tr><th id="1880">1880</th><td>            &lt;&lt; <q>", into node:"</q> &lt;&lt; new_node-&gt;DebugString();</td></tr>
<tr><th id="1881">1881</th><td></td></tr>
<tr><th id="1882">1882</th><td>    (*g)-&gt;RemoveNode(succ);</td></tr>
<tr><th id="1883">1883</th><td>    (*g)-&gt;RemoveNode(pred);</td></tr>
<tr><th id="1884">1884</th><td></td></tr>
<tr><th id="1885">1885</th><td>    <b>return</b> Status::OK();</td></tr>
<tr><th id="1886">1886</th><td>  }</td></tr>
<tr><th id="1887">1887</th><td></td></tr>
<tr><th id="1888">1888</th><td>  <b>return</b> Status(error::Code::UNIMPLEMENTED,</td></tr>
<tr><th id="1889">1889</th><td>                <q>"Unimplemented case for node merge optimization."</q>);</td></tr>
<tr><th id="1890">1890</th><td>}</td></tr>
<tr><th id="1891">1891</th><td></td></tr>
<tr><th id="1892">1892</th><td><i>//////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="1893">1893</th><td><i>//           Helper functions for node rewrite</i></td></tr>
<tr><th id="1894">1894</th><td><i>//////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="1895">1895</th><td></td></tr>
<tr><th id="1896">1896</th><td>Status MklLayoutRewritePass::RewriteNode(std::unique_ptr&lt;Graph&gt;* g,</td></tr>
<tr><th id="1897">1897</th><td>                                         Node* orig_node,</td></tr>
<tr><th id="1898">1898</th><td>                                         <em>const</em> RewriteInfo* ri) {</td></tr>
<tr><th id="1899">1899</th><td>  CHECK_NOTNULL(ri);</td></tr>
<tr><th id="1900">1900</th><td>  CHECK_NOTNULL(orig_node);</td></tr>
<tr><th id="1901">1901</th><td></td></tr>
<tr><th id="1902">1902</th><td>  VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: Original node:"</q> &lt;&lt; orig_node-&gt;DebugString();</td></tr>
<tr><th id="1903">1903</th><td></td></tr>
<tr><th id="1904">1904</th><td>  <i>// Check if this is scenario 2 (context-based rewrite).</i></td></tr>
<tr><th id="1905">1905</th><td><i>  // Get the matching ContextInfo if it is.</i></td></tr>
<tr><th id="1906">1906</th><td>  <em>const</em> Node* fwd_node = <b>nullptr</b>;</td></tr>
<tr><th id="1907">1907</th><td>  <em>const</em> ContextInfo* ci = <b>nullptr</b>;</td></tr>
<tr><th id="1908">1908</th><td>  <em>bool</em> is_context_based_rewrite = <b>false</b>;</td></tr>
<tr><th id="1909">1909</th><td>  <b>if</b> ((ci = SearchMatchingContext(orig_node, &amp;fwd_node)) != <b>nullptr</b>) {</td></tr>
<tr><th id="1910">1910</th><td>    is_context_based_rewrite = <b>true</b>;</td></tr>
<tr><th id="1911">1911</th><td></td></tr>
<tr><th id="1912">1912</th><td>    <i>// Sanity checks for context-based rewrite (if any)</i></td></tr>
<tr><th id="1913">1913</th><td>    <b>if</b> (orig_node-&gt;type_string() == csinfo_.bias_add_grad &amp;&amp;</td></tr>
<tr><th id="1914">1914</th><td>        ri-&gt;new_name == csinfo_.mkl_conv2d_with_bias_backprop_bias) {</td></tr>
<tr><th id="1915">1915</th><td>      CHECK_NOTNULL(fwd_node);</td></tr>
<tr><th id="1916">1916</th><td>      DataType orig_T, ctx_T;</td></tr>
<tr><th id="1917">1917</th><td>      string orig_data_format, ctx_data_format;</td></tr>
<tr><th id="1918">1918</th><td>      TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;orig_T));</td></tr>
<tr><th id="1919">1919</th><td>      TF_CHECK_OK(</td></tr>
<tr><th id="1920">1920</th><td>          GetNodeAttr(orig_node-&gt;def(), <q>"data_format"</q>, &amp;orig_data_format));</td></tr>
<tr><th id="1921">1921</th><td>      TF_CHECK_OK(GetNodeAttr(fwd_node-&gt;def(), <q>"T"</q>, &amp;ctx_T));</td></tr>
<tr><th id="1922">1922</th><td>      TF_CHECK_OK(</td></tr>
<tr><th id="1923">1923</th><td>          GetNodeAttr(fwd_node-&gt;def(), <q>"data_format"</q>, &amp;ctx_data_format));</td></tr>
<tr><th id="1924">1924</th><td></td></tr>
<tr><th id="1925">1925</th><td>      <b>if</b> (orig_data_format != ctx_data_format || orig_T != ctx_T ||</td></tr>
<tr><th id="1926">1926</th><td>          orig_node-&gt;assigned_device_name() !=</td></tr>
<tr><th id="1927">1927</th><td>              fwd_node-&gt;assigned_device_name() ||</td></tr>
<tr><th id="1928">1928</th><td>          orig_node-&gt;def().device() != fwd_node-&gt;def().device()) {</td></tr>
<tr><th id="1929">1929</th><td>        <b>return</b> Status(</td></tr>
<tr><th id="1930">1930</th><td>            error::Code::INVALID_ARGUMENT,</td></tr>
<tr><th id="1931">1931</th><td>            <q>"data_format or T attribute or devices of BiasAddGrad and "</q></td></tr>
<tr><th id="1932">1932</th><td>            <q>"Conv2D do not match. Will skip node rewrite optimization"</q>);</td></tr>
<tr><th id="1933">1933</th><td>      }</td></tr>
<tr><th id="1934">1934</th><td>    } <b>else</b> <b>if</b> (orig_node-&gt;type_string() == csinfo_.bias_add_grad &amp;&amp;</td></tr>
<tr><th id="1935">1935</th><td>               ri-&gt;new_name == csinfo_.matmul) {</td></tr>
<tr><th id="1936">1936</th><td>      <i>// When BiasAddGrad has MatMul in context, we do not do any rewrite</i></td></tr>
<tr><th id="1937">1937</th><td><i>      // and leave BiasAddGrad as it is. But we check for this condition</i></td></tr>
<tr><th id="1938">1938</th><td><i>      // when we check for node rewrite rule. So we should not even come</i></td></tr>
<tr><th id="1939">1939</th><td><i>      // here for MatMul. So we will fail now.</i></td></tr>
<tr><th id="1940">1940</th><td>      <b>return</b> Status(</td></tr>
<tr><th id="1941">1941</th><td>          error::Code::INVALID_ARGUMENT,</td></tr>
<tr><th id="1942">1942</th><td>          <q>"No rewrite is required for BiasAddGrad for MatMul context."</q>);</td></tr>
<tr><th id="1943">1943</th><td>    }</td></tr>
<tr><th id="1944">1944</th><td>  }</td></tr>
<tr><th id="1945">1945</th><td></td></tr>
<tr><th id="1946">1946</th><td>  <i>// Get all inputs.</i></td></tr>
<tr><th id="1947">1947</th><td>  <em>int</em> num_inputs = orig_node-&gt;in_edges().size();</td></tr>
<tr><th id="1948">1948</th><td></td></tr>
<tr><th id="1949">1949</th><td>  <i>// Drop count for control edges from inputs</i></td></tr>
<tr><th id="1950">1950</th><td>  <b>for</b> (<em>const</em> Edge* e : orig_node-&gt;in_edges()) {</td></tr>
<tr><th id="1951">1951</th><td>    <b>if</b> (e-&gt;IsControlEdge()) {</td></tr>
<tr><th id="1952">1952</th><td>      num_inputs--;</td></tr>
<tr><th id="1953">1953</th><td>    }</td></tr>
<tr><th id="1954">1954</th><td>  }</td></tr>
<tr><th id="1955">1955</th><td></td></tr>
<tr><th id="1956">1956</th><td>  gtl::InlinedVector&lt;Node*, <var>4</var>&gt; control_edges;</td></tr>
<tr><th id="1957">1957</th><td>  gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt; inputs(num_inputs);</td></tr>
<tr><th id="1958">1958</th><td>  FillInputs(orig_node, &amp;control_edges, &amp;inputs);</td></tr>
<tr><th id="1959">1959</th><td></td></tr>
<tr><th id="1960">1960</th><td>  <i>// Build new node. We use same name as original node, but change the op name.</i></td></tr>
<tr><th id="1961">1961</th><td>  NodeBuilder nb(orig_node-&gt;name().c_str(), ri-&gt;new_name.c_str());</td></tr>
<tr><th id="1962">1962</th><td>  <i>// Copy user-specified device assigned to original node to new node.</i></td></tr>
<tr><th id="1963">1963</th><td>  nb.Device(orig_node-&gt;def().device());</td></tr>
<tr><th id="1964">1964</th><td>  <i>// Set up new inputs to the rewritten node.</i></td></tr>
<tr><th id="1965">1965</th><td>  Status s = SetUpInputs(g, inputs, &amp;nb, orig_node);</td></tr>
<tr><th id="1966">1966</th><td>  <b>if</b> (s != Status::OK()) {</td></tr>
<tr><th id="1967">1967</th><td>    <b>return</b> s;</td></tr>
<tr><th id="1968">1968</th><td>  }</td></tr>
<tr><th id="1969">1969</th><td></td></tr>
<tr><th id="1970">1970</th><td>  <i>// Copy attributes from original node to new node (for scenario 1).</i></td></tr>
<tr><th id="1971">1971</th><td><i>  // For context-based rewrite, we use context to copy the attributes.</i></td></tr>
<tr><th id="1972">1972</th><td>  <b>if</b> (is_context_based_rewrite) {</td></tr>
<tr><th id="1973">1973</th><td>    <b>if</b> (orig_node-&gt;type_string() == csinfo_.bias_add_grad &amp;&amp;</td></tr>
<tr><th id="1974">1974</th><td>        ri-&gt;new_name == csinfo_.mkl_conv2d_with_bias_backprop_bias) {</td></tr>
<tr><th id="1975">1975</th><td>      CHECK_NOTNULL(fwd_node);</td></tr>
<tr><th id="1976">1976</th><td>      ri-&gt;copy_attrs(fwd_node, &amp;nb);</td></tr>
<tr><th id="1977">1977</th><td>    } <b>else</b> {</td></tr>
<tr><th id="1978">1978</th><td>      <b>return</b> Status(error::Code::UNIMPLEMENTED,</td></tr>
<tr><th id="1979">1979</th><td>                    <q>"Unimplemented case for node rewrite optimization."</q>);</td></tr>
<tr><th id="1980">1980</th><td>    }</td></tr>
<tr><th id="1981">1981</th><td>  } <b>else</b> {</td></tr>
<tr><th id="1982">1982</th><td>    ri-&gt;copy_attrs(<b>const_cast</b>&lt;<em>const</em> Node*&gt;(orig_node), &amp;nb);</td></tr>
<tr><th id="1983">1983</th><td>  }</td></tr>
<tr><th id="1984">1984</th><td>  <i>// Set the Mkl layer label for this op.</i></td></tr>
<tr><th id="1985">1985</th><td>  nb.Attr(<q>"_kernel"</q>, mkl_op_registry::kMklOpLabel);</td></tr>
<tr><th id="1986">1986</th><td></td></tr>
<tr><th id="1987">1987</th><td>  <i>// Finalize graph and get new node.</i></td></tr>
<tr><th id="1988">1988</th><td>  Node* new_node = <b>nullptr</b>;</td></tr>
<tr><th id="1989">1989</th><td>  TF_CHECK_OK(nb.Finalize(&amp;**g, &amp;new_node));</td></tr>
<tr><th id="1990">1990</th><td>  CHECK_NOTNULL(new_node);</td></tr>
<tr><th id="1991">1991</th><td></td></tr>
<tr><th id="1992">1992</th><td>  <i>// Incoming data edges from 'orig_node' node to new 'new_node' node are</i></td></tr>
<tr><th id="1993">1993</th><td><i>  // already copied in BuildNode. We need to handle control edges now.</i></td></tr>
<tr><th id="1994">1994</th><td>  <b>for</b> (<em>const</em> Edge* e : orig_node-&gt;in_edges()) {</td></tr>
<tr><th id="1995">1995</th><td>    <b>if</b> (e-&gt;IsControlEdge()) {</td></tr>
<tr><th id="1996">1996</th><td>      CHECK_NOTNULL((*g)-&gt;AddControlEdge(e-&gt;src(), new_node));</td></tr>
<tr><th id="1997">1997</th><td>    }</td></tr>
<tr><th id="1998">1998</th><td>  }</td></tr>
<tr><th id="1999">1999</th><td></td></tr>
<tr><th id="2000">2000</th><td>  <i>// Copy outgoing edges from 'orig_node' node to new</i></td></tr>
<tr><th id="2001">2001</th><td><i>  // 'new_node' node, since the output also follows same ordering among</i></td></tr>
<tr><th id="2002">2002</th><td><i>  // Tensorflow tensors and Mkl tensors. We need to connect Tensorflow</i></td></tr>
<tr><th id="2003">2003</th><td><i>  // tensors appropriately. Specifically, nth output of the original node</i></td></tr>
<tr><th id="2004">2004</th><td><i>  // will become 2*nth output of the Mkl node for the interleaved ordering</i></td></tr>
<tr><th id="2005">2005</th><td><i>  // of the tensors. For the contiguous ordering of the tensors, it will be n.</i></td></tr>
<tr><th id="2006">2006</th><td><i>  // GetTensorDataIndex provides this mapping function.</i></td></tr>
<tr><th id="2007">2007</th><td>  <b>for</b> (<em>const</em> Edge* e : orig_node-&gt;out_edges()) {</td></tr>
<tr><th id="2008">2008</th><td>    <b>if</b> (e-&gt;IsControlEdge()) {</td></tr>
<tr><th id="2009">2009</th><td>      CHECK_NOTNULL((*g)-&gt;AddControlEdge(new_node, e-&gt;dst()));</td></tr>
<tr><th id="2010">2010</th><td>    } <b>else</b> {</td></tr>
<tr><th id="2011">2011</th><td>      CHECK_NOTNULL((*g)-&gt;AddEdge(</td></tr>
<tr><th id="2012">2012</th><td>          new_node,</td></tr>
<tr><th id="2013">2013</th><td>          GetTensorDataIndex(e-&gt;src_output(), e-&gt;src()-&gt;num_outputs()),</td></tr>
<tr><th id="2014">2014</th><td>          e-&gt;dst(), e-&gt;dst_input()));</td></tr>
<tr><th id="2015">2015</th><td>    }</td></tr>
<tr><th id="2016">2016</th><td>  }</td></tr>
<tr><th id="2017">2017</th><td></td></tr>
<tr><th id="2018">2018</th><td>  <i>// Copy the runtime device assigned from original code to new node.</i></td></tr>
<tr><th id="2019">2019</th><td>  new_node-&gt;set_assigned_device_name(orig_node-&gt;assigned_device_name());</td></tr>
<tr><th id="2020">2020</th><td></td></tr>
<tr><th id="2021">2021</th><td>  <i>// Delete original node and mark new node as rewritten.</i></td></tr>
<tr><th id="2022">2022</th><td>  (*g)-&gt;RemoveNode(orig_node);</td></tr>
<tr><th id="2023">2023</th><td></td></tr>
<tr><th id="2024">2024</th><td>  VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: New node:"</q> &lt;&lt; new_node-&gt;DebugString();</td></tr>
<tr><th id="2025">2025</th><td>  <b>return</b> Status::OK();</td></tr>
<tr><th id="2026">2026</th><td>}</td></tr>
<tr><th id="2027">2027</th><td></td></tr>
<tr><th id="2028">2028</th><td><em>const</em> MklLayoutRewritePass::ContextInfo*</td></tr>
<tr><th id="2029">2029</th><td>MklLayoutRewritePass::SearchMatchingContext(<em>const</em> Node* n,</td></tr>
<tr><th id="2030">2030</th><td>                                            <em>const</em> Node** fwd_node) {</td></tr>
<tr><th id="2031">2031</th><td>  CHECK_NOTNULL(n);</td></tr>
<tr><th id="2032">2032</th><td>  CHECK_NOTNULL(fwd_node);</td></tr>
<tr><th id="2033">2033</th><td>  *fwd_node = <b>nullptr</b>;</td></tr>
<tr><th id="2034">2034</th><td></td></tr>
<tr><th id="2035">2035</th><td>  <i>// Search for matching contextinfo based on node name and call</i></td></tr>
<tr><th id="2036">2036</th><td><i>  // callback function using matching contextinfo.</i></td></tr>
<tr><th id="2037">2037</th><td><i>  // There could be more than one matching contextinfos but whichever</i></td></tr>
<tr><th id="2038">2038</th><td><i>  // matches first is returned.</i></td></tr>
<tr><th id="2039">2039</th><td>  <b>for</b> (<em>auto</em> ci = cinfo_.cbegin(); ci != cinfo_.cend(); ++ci) {</td></tr>
<tr><th id="2040">2040</th><td>    <b>if</b> (n-&gt;type_string() == (*ci)-&gt;node &amp;&amp;</td></tr>
<tr><th id="2041">2041</th><td>        (*ci)-&gt;context_match_fn(n, fwd_node, *ci)) {</td></tr>
<tr><th id="2042">2042</th><td>      VLOG(<var>1</var>) &lt;&lt; <q>"Found context as matching: "</q> &lt;&lt; (*ci)-&gt;fwd;</td></tr>
<tr><th id="2043">2043</th><td>      <b>return</b> *ci;</td></tr>
<tr><th id="2044">2044</th><td>    }</td></tr>
<tr><th id="2045">2045</th><td>  }</td></tr>
<tr><th id="2046">2046</th><td>  <b>return</b> <b>nullptr</b>;</td></tr>
<tr><th id="2047">2047</th><td>}</td></tr>
<tr><th id="2048">2048</th><td></td></tr>
<tr><th id="2049">2049</th><td><em>bool</em> MklLayoutRewritePass::ContextMatchRewrite(<em>const</em> Node* n,</td></tr>
<tr><th id="2050">2050</th><td>                                               <em>const</em> ContextInfo* c) {</td></tr>
<tr><th id="2051">2051</th><td>  <em>const</em> Node* fwd_node = <b>nullptr</b>;</td></tr>
<tr><th id="2052">2052</th><td>  <b>return</b> SearchMatchingContext(n, &amp;fwd_node) == c;</td></tr>
<tr><th id="2053">2053</th><td>}</td></tr>
<tr><th id="2054">2054</th><td></td></tr>
<tr><th id="2055">2055</th><td><em>const</em> MklLayoutRewritePass::RewriteInfo*</td></tr>
<tr><th id="2056">2056</th><td>MklLayoutRewritePass::CheckForNodeRewrite(<em>const</em> Node* n) <em>const</em> {</td></tr>
<tr><th id="2057">2057</th><td>  CHECK_NOTNULL(n);</td></tr>
<tr><th id="2058">2058</th><td></td></tr>
<tr><th id="2059">2059</th><td>  <i>// First check if node along with its type is supported by MKL layer.</i></td></tr>
<tr><th id="2060">2060</th><td><i>  // We do not want to rewrite an op into Mkl op if types are not supported.</i></td></tr>
<tr><th id="2061">2061</th><td><i>  // E.g., MklRelu does not support INT32. So we cannot rewrite Relu to</i></td></tr>
<tr><th id="2062">2062</th><td><i>  // MklRelu if type is INT32.</i></td></tr>
<tr><th id="2063">2063</th><td>  DataType T;</td></tr>
<tr><th id="2064">2064</th><td>  <b>if</b> (!GetNodeAttr(n-&gt;def(), <q>"T"</q>, &amp;T).ok()) {</td></tr>
<tr><th id="2065">2065</th><td>    <b>return</b> <b>nullptr</b>;</td></tr>
<tr><th id="2066">2066</th><td>  }</td></tr>
<tr><th id="2067">2067</th><td></td></tr>
<tr><th id="2068">2068</th><td>  <i>// BiasAddGrad is not an Mkl layer, so we make an exception for it.</i></td></tr>
<tr><th id="2069">2069</th><td>  <b>if</b> (n-&gt;type_string() != csinfo_.bias_add_grad) {</td></tr>
<tr><th id="2070">2070</th><td>    <b>if</b> (!mkl_op_registry::IsMklOp(</td></tr>
<tr><th id="2071">2071</th><td>            mkl_op_registry::GetMklOpName(n-&gt;type_string()), T)) {</td></tr>
<tr><th id="2072">2072</th><td>      <b>return</b> <b>nullptr</b>;</td></tr>
<tr><th id="2073">2073</th><td>    }</td></tr>
<tr><th id="2074">2074</th><td>  }</td></tr>
<tr><th id="2075">2075</th><td></td></tr>
<tr><th id="2076">2076</th><td>  <i>// For elementwise node, we reuse the Eigen implementation and pass the MKL</i></td></tr>
<tr><th id="2077">2077</th><td><i>  // metadata tensor through so we can avoid conversions. However, if all</i></td></tr>
<tr><th id="2078">2078</th><td><i>  // incoming edges are in TF format, we don't need all this overhead, so</i></td></tr>
<tr><th id="2079">2079</th><td><i>  // replace the elementwise node only if at least one of its parents is a MKL</i></td></tr>
<tr><th id="2080">2080</th><td><i>  // node.</i></td></tr>
<tr><th id="2081">2081</th><td><i>  //</i></td></tr>
<tr><th id="2082">2082</th><td><i>  // TODO(vrane): Add implementation for element-wise ops that doesn't reuse</i></td></tr>
<tr><th id="2083">2083</th><td><i>  // eigen code to reduce cross-library dependency.</i></td></tr>
<tr><th id="2084">2084</th><td>  <b>if</b> (mkl_op_registry::IsMklElementWiseOp(</td></tr>
<tr><th id="2085">2085</th><td>          mkl_op_registry::GetMklOpName(n-&gt;type_string()), T)) {</td></tr>
<tr><th id="2086">2086</th><td>    <em>bool</em> incoming_mkl_edge = <b>false</b>;</td></tr>
<tr><th id="2087">2087</th><td>    <b>for</b> (<em>auto</em> parent : n-&gt;in_edges()) {</td></tr>
<tr><th id="2088">2088</th><td>      <b>if</b> (mkl_op_registry::IsMklOp(</td></tr>
<tr><th id="2089">2089</th><td>              mkl_op_registry::GetMklOpName(parent-&gt;src()-&gt;type_string()), T)) {</td></tr>
<tr><th id="2090">2090</th><td>        incoming_mkl_edge = <b>true</b>;</td></tr>
<tr><th id="2091">2091</th><td>        <b>break</b>;</td></tr>
<tr><th id="2092">2092</th><td>      } <b>else</b> {</td></tr>
<tr><th id="2093">2093</th><td>        VLOG(<var>1</var>) &lt;&lt; <q>"Non-MKL parent is: "</q> &lt;&lt; parent-&gt;src()-&gt;type_string();</td></tr>
<tr><th id="2094">2094</th><td>      }</td></tr>
<tr><th id="2095">2095</th><td>    }</td></tr>
<tr><th id="2096">2096</th><td>    <b>if</b> (incoming_mkl_edge == <b>false</b>) {</td></tr>
<tr><th id="2097">2097</th><td>      VLOG(<var>1</var>) &lt;&lt; <q>"Skipping replacement of elementwise node which has no MKL "</q></td></tr>
<tr><th id="2098">2098</th><td>                 <q>"parents."</q>;</td></tr>
<tr><th id="2099">2099</th><td>      <b>return</b> <b>nullptr</b>;</td></tr>
<tr><th id="2100">2100</th><td>    }</td></tr>
<tr><th id="2101">2101</th><td>  }</td></tr>
<tr><th id="2102">2102</th><td></td></tr>
<tr><th id="2103">2103</th><td>  <i>// We support 2 types of node rewrites:</i></td></tr>
<tr><th id="2104">2104</th><td><i>  // 1. Rewriting BiasAddGrad depending on its MklConv2DWithBias context.</i></td></tr>
<tr><th id="2105">2105</th><td><i>  // 2. Rewriting an op to Mkl op always</i></td></tr>
<tr><th id="2106">2106</th><td><i>  // We return true if any of these 2 conditions is met.</i></td></tr>
<tr><th id="2107">2107</th><td><i></i></td></tr>
<tr><th id="2108">2108</th><td><i>  // Find matching RewriteInfo and then check that rewrite rule applies.</i></td></tr>
<tr><th id="2109">2109</th><td>  <b>for</b> (<em>auto</em> ri = rinfo_.cbegin(); ri != rinfo_.cend(); ++ri) {</td></tr>
<tr><th id="2110">2110</th><td>    <b>if</b> (n-&gt;type_string().compare(ri-&gt;name) == <var>0</var> &amp;&amp;</td></tr>
<tr><th id="2111">2111</th><td>        ri-&gt;rewrite_rule(n, ri-&gt;context)) {</td></tr>
<tr><th id="2112">2112</th><td>      <i>// If we are rewriting BiasAddGrad into BiasAddGrad for MatMul context,</i></td></tr>
<tr><th id="2113">2113</th><td><i>      // then we just return directly.</i></td></tr>
<tr><th id="2114">2114</th><td>      <b>if</b> (n-&gt;type_string() == csinfo_.bias_add_grad &amp;&amp;</td></tr>
<tr><th id="2115">2115</th><td>          ri-&gt;context-&gt;fwd == csinfo_.matmul &amp;&amp;</td></tr>
<tr><th id="2116">2116</th><td>          ri-&gt;new_name == csinfo_.bias_add_grad) {</td></tr>
<tr><th id="2117">2117</th><td>        <b>return</b> <b>nullptr</b>;</td></tr>
<tr><th id="2118">2118</th><td>      }</td></tr>
<tr><th id="2119">2119</th><td>      <b>return</b> &amp;*ri;</td></tr>
<tr><th id="2120">2120</th><td>    }</td></tr>
<tr><th id="2121">2121</th><td>  }</td></tr>
<tr><th id="2122">2122</th><td></td></tr>
<tr><th id="2123">2123</th><td>  <i>// Else return not found.</i></td></tr>
<tr><th id="2124">2124</th><td>  <b>return</b> <b>nullptr</b>;</td></tr>
<tr><th id="2125">2125</th><td>}</td></tr>
<tr><th id="2126">2126</th><td></td></tr>
<tr><th id="2127">2127</th><td><i>///////////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="2128">2128</th><td><i>//              Run function for the pass</i></td></tr>
<tr><th id="2129">2129</th><td><i>///////////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="2130">2130</th><td></td></tr>
<tr><th id="2131">2131</th><td><em>bool</em> MklLayoutRewritePass::RunPass(std::unique_ptr&lt;Graph&gt;* g) {</td></tr>
<tr><th id="2132">2132</th><td>  <em>bool</em> result = <b>false</b>;</td></tr>
<tr><th id="2133">2133</th><td>  CHECK_NOTNULL(g);</td></tr>
<tr><th id="2134">2134</th><td></td></tr>
<tr><th id="2135">2135</th><td>  DumpGraph(<q>"Before running MklLayoutRewritePass"</q>, &amp;**g);</td></tr>
<tr><th id="2136">2136</th><td></td></tr>
<tr><th id="2137">2137</th><td>  std::vector&lt;Node*&gt; order;</td></tr>
<tr><th id="2138">2138</th><td>  GetReversePostOrder(**g, &amp;order);  <i>// This will give us topological sort.</i></td></tr>
<tr><th id="2139">2139</th><td></td></tr>
<tr><th id="2140">2140</th><td>  <b>for</b> (Node* n : order) {</td></tr>
<tr><th id="2141">2141</th><td>    <i>// If node is not an op or it cannot run on CPU device, then skip.</i></td></tr>
<tr><th id="2142">2142</th><td>    <b>if</b> (!n-&gt;IsOp() || !CanOpRunOnCPUDevice(n)) {</td></tr>
<tr><th id="2143">2143</th><td>      <b>continue</b>;</td></tr>
<tr><th id="2144">2144</th><td>    }</td></tr>
<tr><th id="2145">2145</th><td></td></tr>
<tr><th id="2146">2146</th><td>    <em>const</em> RewriteInfo* ri = <b>nullptr</b>;</td></tr>
<tr><th id="2147">2147</th><td>    Node* predn = <b>nullptr</b>;</td></tr>
<tr><th id="2148">2148</th><td>    <i>// We will first search if node is to be rewritten</i></td></tr>
<tr><th id="2149">2149</th><td>    <b>if</b> ((ri = CheckForNodeRewrite(n)) != <b>nullptr</b>) {</td></tr>
<tr><th id="2150">2150</th><td>      string node_name = n-&gt;name();</td></tr>
<tr><th id="2151">2151</th><td>      string op_name = n-&gt;type_string();</td></tr>
<tr><th id="2152">2152</th><td></td></tr>
<tr><th id="2153">2153</th><td>      VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: Scheduled node "</q> &lt;&lt; node_name</td></tr>
<tr><th id="2154">2154</th><td>              &lt;&lt; <q>" with op "</q> &lt;&lt; op_name &lt;&lt; <q>" for rewrite using"</q></td></tr>
<tr><th id="2155">2155</th><td>              &lt;&lt; <q>" layout optimization."</q>;</td></tr>
<tr><th id="2156">2156</th><td></td></tr>
<tr><th id="2157">2157</th><td>      <b>if</b> (RewriteNode(g, n, ri) == Status::OK()) {</td></tr>
<tr><th id="2158">2158</th><td>        VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: rewrote node "</q> &lt;&lt; node_name</td></tr>
<tr><th id="2159">2159</th><td>                &lt;&lt; <q>" with op "</q> &lt;&lt; op_name &lt;&lt; <q>" for Mkl layout optimization."</q>;</td></tr>
<tr><th id="2160">2160</th><td>        result = <b>true</b>;</td></tr>
<tr><th id="2161">2161</th><td>      }</td></tr>
<tr><th id="2162">2162</th><td>    } <b>else</b> <b>if</b> ((predn = CheckForNodeMerge(n)) != <b>nullptr</b>) {</td></tr>
<tr><th id="2163">2163</th><td>      <i>// Otherwise, we will check if the node is to be merged.</i></td></tr>
<tr><th id="2164">2164</th><td>      string n1_name = n-&gt;name();</td></tr>
<tr><th id="2165">2165</th><td>      string n2_name = predn-&gt;name();</td></tr>
<tr><th id="2166">2166</th><td></td></tr>
<tr><th id="2167">2167</th><td>      VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: Scheduled nodes "</q> &lt;&lt; n1_name &lt;&lt; <q>" and "</q></td></tr>
<tr><th id="2168">2168</th><td>              &lt;&lt; n2_name &lt;&lt; <q>" for merging"</q>;</td></tr>
<tr><th id="2169">2169</th><td></td></tr>
<tr><th id="2170">2170</th><td>      <b>if</b> (MergeNode(g, n, predn) == Status::OK()) {</td></tr>
<tr><th id="2171">2171</th><td>        VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: Merged nodes "</q> &lt;&lt; n1_name &lt;&lt; <q>" and "</q></td></tr>
<tr><th id="2172">2172</th><td>                &lt;&lt; n2_name;</td></tr>
<tr><th id="2173">2173</th><td>        result = <b>true</b>;</td></tr>
<tr><th id="2174">2174</th><td>      }</td></tr>
<tr><th id="2175">2175</th><td>    }</td></tr>
<tr><th id="2176">2176</th><td>  }</td></tr>
<tr><th id="2177">2177</th><td></td></tr>
<tr><th id="2178">2178</th><td>  DumpGraph(<q>"After running MklLayoutRewritePass"</q>, &amp;**g);</td></tr>
<tr><th id="2179">2179</th><td></td></tr>
<tr><th id="2180">2180</th><td>  <b>return</b> result;</td></tr>
<tr><th id="2181">2181</th><td>}</td></tr>
<tr><th id="2182">2182</th><td></td></tr>
<tr><th id="2183">2183</th><td><em>bool</em> RunMklLayoutRewritePass(std::unique_ptr&lt;Graph&gt;* g) {</td></tr>
<tr><th id="2184">2184</th><td>  <b>return</b> MklLayoutRewritePass().RunPass(g);</td></tr>
<tr><th id="2185">2185</th><td>}</td></tr>
<tr><th id="2186">2186</th><td></td></tr>
<tr><th id="2187">2187</th><td>Status MklLayoutRewritePass::Run(<em>const</em> GraphOptimizationPassOptions&amp; options) {</td></tr>
<tr><th id="2188">2188</th><td>  <b>if</b> (options.graph == <b>nullptr</b> &amp;&amp; options.partition_graphs == <b>nullptr</b>) {</td></tr>
<tr><th id="2189">2189</th><td>    <b>return</b> Status::OK();</td></tr>
<tr><th id="2190">2190</th><td>  }</td></tr>
<tr><th id="2191">2191</th><td></td></tr>
<tr><th id="2192">2192</th><td>  <em>auto</em> process_graph = [&amp;](std::unique_ptr&lt;Graph&gt;* g) {</td></tr>
<tr><th id="2193">2193</th><td>    <i>// Get the ownership of a graph</i></td></tr>
<tr><th id="2194">2194</th><td>    std::unique_ptr&lt;Graph&gt;* ng = std::move(g);</td></tr>
<tr><th id="2195">2195</th><td>    RunPass(ng);</td></tr>
<tr><th id="2196">2196</th><td>    <i>// Return the ownership of a graph back</i></td></tr>
<tr><th id="2197">2197</th><td>    g-&gt;reset(ng-&gt;release());</td></tr>
<tr><th id="2198">2198</th><td>  };</td></tr>
<tr><th id="2199">2199</th><td></td></tr>
<tr><th id="2200">2200</th><td>  <b>if</b> (kMklLayoutRewritePassGroup !=</td></tr>
<tr><th id="2201">2201</th><td>      OptimizationPassRegistry::POST_PARTITIONING) {</td></tr>
<tr><th id="2202">2202</th><td>    <i>// For any pre-partitioning phase, a graph is stored in options.graph.</i></td></tr>
<tr><th id="2203">2203</th><td>    process_graph(options.graph);</td></tr>
<tr><th id="2204">2204</th><td>  } <b>else</b> {</td></tr>
<tr><th id="2205">2205</th><td>    <i>// For post partitioning phase, graphs are stored in</i></td></tr>
<tr><th id="2206">2206</th><td><i>    // options.partition_graphs.</i></td></tr>
<tr><th id="2207">2207</th><td>    <b>for</b> (<em>auto</em>&amp; pg : *options.partition_graphs) {</td></tr>
<tr><th id="2208">2208</th><td>      process_graph(&amp;pg.second);</td></tr>
<tr><th id="2209">2209</th><td>    }</td></tr>
<tr><th id="2210">2210</th><td>  }</td></tr>
<tr><th id="2211">2211</th><td></td></tr>
<tr><th id="2212">2212</th><td>  <b>return</b> Status::OK();</td></tr>
<tr><th id="2213">2213</th><td>}</td></tr>
<tr><th id="2214">2214</th><td></td></tr>
<tr><th id="2215">2215</th><td><u>#else   // INTEL_MKL_ML</u></td></tr>
<tr><th id="2216">2216</th><td></td></tr>
<tr><th id="2217">2217</th><td><i>// This pass implements rewriting of graph to support following scenarios:</i></td></tr>
<tr><th id="2218">2218</th><td><i>// (A) Merging nodes in the graph</i></td></tr>
<tr><th id="2219">2219</th><td><i>// (B) Rewriting a node in the graph to a new node</i></td></tr>
<tr><th id="2220">2220</th><td><i>//     Rewrite happens under following scenario:</i></td></tr>
<tr><th id="2221">2221</th><td><i>//     - Propagating Mkl layout as an additional output tensor</i></td></tr>
<tr><th id="2222">2222</th><td><i>//        (we will loosely call a tensor that carries Mkl layout as Mkl tensor</i></td></tr>
<tr><th id="2223">2223</th><td><i>//         henceforth.) from every Mkl supported NN layer.</i></td></tr>
<tr><th id="2224">2224</th><td><i>//</i></td></tr>
<tr><th id="2225">2225</th><td><i>// Example of A : Merging nodes in the graph</i></td></tr>
<tr><th id="2226">2226</th><td><i>// -----------------------------------------</i></td></tr>
<tr><th id="2227">2227</th><td><i>// Currently, we merge Conv2D+AddBias together. Consider Conv2D and BiasAdd as:</i></td></tr>
<tr><th id="2228">2228</th><td><i>//</i></td></tr>
<tr><th id="2229">2229</th><td><i>//           O = Conv2D(A, B)</i></td></tr>
<tr><th id="2230">2230</th><td><i>//           P = BiasAdd(O, C)</i></td></tr>
<tr><th id="2231">2231</th><td><i>//</i></td></tr>
<tr><th id="2232">2232</th><td><i>// We merge them into Conv2DWithBias as:</i></td></tr>
<tr><th id="2233">2233</th><td><i>//           P = _MklConv2DWithBias(A, A_m, B, B_m, C, C_m)</i></td></tr>
<tr><th id="2234">2234</th><td><i>//</i></td></tr>
<tr><th id="2235">2235</th><td><i>// The meaning of A_m, B_m and C_m is explained in B.1.</i></td></tr>
<tr><th id="2236">2236</th><td><i>//</i></td></tr>
<tr><th id="2237">2237</th><td><i>// Merge rules:</i></td></tr>
<tr><th id="2238">2238</th><td><i>//  - The merge for Conv2D and BiasAdd happens when the output of Conv2D _only_</i></td></tr>
<tr><th id="2239">2239</th><td><i>//    goes to BiasAdd.</i></td></tr>
<tr><th id="2240">2240</th><td><i>//  - Also, the intersection of attributes of both the nodes must have same</i></td></tr>
<tr><th id="2241">2241</th><td><i>//    values.</i></td></tr>
<tr><th id="2242">2242</th><td><i>//  - Both the nodes must have been assigned to same device (if any).</i></td></tr>
<tr><th id="2243">2243</th><td><i>//</i></td></tr>
<tr><th id="2244">2244</th><td><i>// Example of B.1 : Rewriting nodes to Mkl nodes</i></td></tr>
<tr><th id="2245">2245</th><td><i>// ---------------------------------------------</i></td></tr>
<tr><th id="2246">2246</th><td><i>// Consider a Relu node. Current definition of Relu node looks like:</i></td></tr>
<tr><th id="2247">2247</th><td><i>//</i></td></tr>
<tr><th id="2248">2248</th><td><i>//           O = Relu(A)</i></td></tr>
<tr><th id="2249">2249</th><td><i>//</i></td></tr>
<tr><th id="2250">2250</th><td><i>// Relu has 1 input (A), and 1 output (O).</i></td></tr>
<tr><th id="2251">2251</th><td><i>//</i></td></tr>
<tr><th id="2252">2252</th><td><i>// This rewrite pass will generate a new graph node for Relu (new node is</i></td></tr>
<tr><th id="2253">2253</th><td><i>// called MklRelu) as:</i></td></tr>
<tr><th id="2254">2254</th><td><i>//</i></td></tr>
<tr><th id="2255">2255</th><td><i>//          O, O_m = MklRelu(A, A_m)</i></td></tr>
<tr><th id="2256">2256</th><td><i>//</i></td></tr>
<tr><th id="2257">2257</th><td><i>// MklRelu has 2 inputs (A and A_m) and 2 outputs (O and O_m). Here input A is</i></td></tr>
<tr><th id="2258">2258</th><td><i>// same as input A of Relu; output O is same as output O of Relu. O_m is the</i></td></tr>
<tr><th id="2259">2259</th><td><i>// additional output tensor that will be set by MklRelu, and it represents</i></td></tr>
<tr><th id="2260">2260</th><td><i>// Mkl tensor corresponding to O -- in other words, O_m is some kind of</i></td></tr>
<tr><th id="2261">2261</th><td><i>// metadata for O. A_m is additional input of Relu, and it represents metadata</i></td></tr>
<tr><th id="2262">2262</th><td><i>// for A - as O_m is metadata for O, A_m is metadata for A. MklRelu receives</i></td></tr>
<tr><th id="2263">2263</th><td><i>// this metadata from previous node in the graph.</i></td></tr>
<tr><th id="2264">2264</th><td><i>//</i></td></tr>
<tr><th id="2265">2265</th><td><i>// When a previous node in the graph is an Mkl node, A_m will represent a valid</i></td></tr>
<tr><th id="2266">2266</th><td><i>// Mkl tensor. But when a previous node is not an Mkl node, A_m will represent</i></td></tr>
<tr><th id="2267">2267</th><td><i>// a dummy Mkl tensor.</i></td></tr>
<tr><th id="2268">2268</th><td><i>//</i></td></tr>
<tr><th id="2269">2269</th><td><i>// Rewriting rules:</i></td></tr>
<tr><th id="2270">2270</th><td><i>//  - Selection of a node for rewriting happens by registering the op type of</i></td></tr>
<tr><th id="2271">2271</th><td><i>//    the node with the rewriting pass. If the op type is not registered, then</i></td></tr>
<tr><th id="2272">2272</th><td><i>//    all nodes of this op type will not be rewritten.</i></td></tr>
<tr><th id="2273">2273</th><td><i>//  - Number of inputs after rewriting:</i></td></tr>
<tr><th id="2274">2274</th><td><i>//      Since for every input Tensorflow tensor, the rewritten node gets Mkl</i></td></tr>
<tr><th id="2275">2275</th><td><i>//      tensor(s), rewritten node gets 2*N inputs, where N is the number of</i></td></tr>
<tr><th id="2276">2276</th><td><i>//      inputs for the original node.</i></td></tr>
<tr><th id="2277">2277</th><td><i>//  - Number of outputs after rewriting:</i></td></tr>
<tr><th id="2278">2278</th><td><i>//      Since for every output Tensorflow tensor, the rewritten node generates</i></td></tr>
<tr><th id="2279">2279</th><td><i>//      Mkl tensor(s), the rewritten node generates 2*N outputs, where N is the</i></td></tr>
<tr><th id="2280">2280</th><td><i>//      number of outputs of the original node.</i></td></tr>
<tr><th id="2281">2281</th><td><i>//  - Ordering of Tensorflow tensors and Mkl tensors:</i></td></tr>
<tr><th id="2282">2282</th><td><i>//      Since every rewritten node generates twice the number of inputs and</i></td></tr>
<tr><th id="2283">2283</th><td><i>//      outputs, one could imagine various orderings among Tensorflow tensors</i></td></tr>
<tr><th id="2284">2284</th><td><i>//      and Mkl tensors. E.g., assume an op 'Conv2D' that takes (A, B) as</i></td></tr>
<tr><th id="2285">2285</th><td><i>//      inputs, then the new op '_MklConv2D' can take inputs A, B, A_m and B_m</i></td></tr>
<tr><th id="2286">2286</th><td><i>//      in A, A_m, B, B_m order or it can also take them in A, B, A_m, B_m</i></td></tr>
<tr><th id="2287">2287</th><td><i>//      order. Among N inputs one can get N! permutations.</i></td></tr>
<tr><th id="2288">2288</th><td><i>//</i></td></tr>
<tr><th id="2289">2289</th><td><i>//      So the question is: which order do we follow? We support 2 types of</i></td></tr>
<tr><th id="2290">2290</th><td><i>//      orderings: (1) interleaved, and (2) contiguous. Interleaved ordering</i></td></tr>
<tr><th id="2291">2291</th><td><i>//      follows an intuitive order where an Mkl tensor follows the</i></td></tr>
<tr><th id="2292">2292</th><td><i>//      corresponding Tensorflow tensor immediately. In the context of the</i></td></tr>
<tr><th id="2293">2293</th><td><i>//      above example, it will be: A, A_m, B, B_m. Note that the ordering rule</i></td></tr>
<tr><th id="2294">2294</th><td><i>//      applies to both the inputs and outputs. Contiguous ordering means</i></td></tr>
<tr><th id="2295">2295</th><td><i>//      all the Tensorflow tensors are contiguous followed by all the Mkl</i></td></tr>
<tr><th id="2296">2296</th><td><i>//      tensors. We use contiguous ordering as default.</i></td></tr>
<tr><th id="2297">2297</th><td><i>//</i></td></tr>
<tr><th id="2298">2298</th><td><i>// Graph rewrite algorithm:</i></td></tr>
<tr><th id="2299">2299</th><td><i>//      Algorithm: Graph Rewrite</i></td></tr>
<tr><th id="2300">2300</th><td><i>//      Input: Graph G, Names of the nodes to rewrite and their new names</i></td></tr>
<tr><th id="2301">2301</th><td><i>//      Output: Modified Graph G' if the nodes are modified, G otherwise.</i></td></tr>
<tr><th id="2302">2302</th><td><i>//      Start:</i></td></tr>
<tr><th id="2303">2303</th><td><i>//        N = Topological_Sort(G) // N is a set of nodes in toposort order.</i></td></tr>
<tr><th id="2304">2304</th><td><i>//        foreach node n in N</i></td></tr>
<tr><th id="2305">2305</th><td><i>//        do</i></td></tr>
<tr><th id="2306">2306</th><td><i>//          if (Is_MKL_Op(n))  // Can this node accept an Mkl layout as input.</i></td></tr>
<tr><th id="2307">2307</th><td><i>//          then</i></td></tr>
<tr><th id="2308">2308</th><td><i>//            E = set of &lt;incoming edge and its src_output slot&gt; of n</i></td></tr>
<tr><th id="2309">2309</th><td><i>//            E' = {}   // a new set of edges for rewritten node</i></td></tr>
<tr><th id="2310">2310</th><td><i>//            foreach &lt;e,s&gt; in E</i></td></tr>
<tr><th id="2311">2311</th><td><i>//            do</i></td></tr>
<tr><th id="2312">2312</th><td><i>//              E' U {&lt;e,s&gt;}  // First copy edge which generates Tensorflow</i></td></tr>
<tr><th id="2313">2313</th><td><i>//                            // tensor as it is</i></td></tr>
<tr><th id="2314">2314</th><td><i>//              m = Source node of edge e</i></td></tr>
<tr><th id="2315">2315</th><td><i>//              if Is_Rewritten(m)  // Did we rewrite this node in this pass?</i></td></tr>
<tr><th id="2316">2316</th><td><i>//              then</i></td></tr>
<tr><th id="2317">2317</th><td><i>//                E' U {&lt;m,s+1&gt;}    // If yes, then m will generate an Mkl</i></td></tr>
<tr><th id="2318">2318</th><td><i>//                                  // tensor as an additional output.</i></td></tr>
<tr><th id="2319">2319</th><td><i>//              else</i></td></tr>
<tr><th id="2320">2320</th><td><i>//                d = Generate_Dummy_Mkl_Tensor()  // If not, generate a dummy</i></td></tr>
<tr><th id="2321">2321</th><td><i>//                                                 // Mkl tensor.</i></td></tr>
<tr><th id="2322">2322</th><td><i>//                E' U {&lt;d,0&gt;}  // The dummy Mkl tensor has only 1 output slot.</i></td></tr>
<tr><th id="2323">2323</th><td><i>//              fi</i></td></tr>
<tr><th id="2324">2324</th><td><i>//            done</i></td></tr>
<tr><th id="2325">2325</th><td><i>//            n' = Build_New_Node(G,new_name,E')</i></td></tr>
<tr><th id="2326">2326</th><td><i>//            Mark_Rewritten(n')  // Mark the new node as being rewritten.</i></td></tr>
<tr><th id="2327">2327</th><td><i>//          fi</i></td></tr>
<tr><th id="2328">2328</th><td><i>//        done</i></td></tr>
<tr><th id="2329">2329</th><td><i>//</i></td></tr>
<tr><th id="2330">2330</th><td><i>//      Explanation:</i></td></tr>
<tr><th id="2331">2331</th><td><i>//        For graph rewrite, we visit nodes of the input graph in the</i></td></tr>
<tr><th id="2332">2332</th><td><i>//        topological sort order. With this ordering, we visit nodes in the</i></td></tr>
<tr><th id="2333">2333</th><td><i>//        top-to-bottom fashion. We need this order because while visiting a</i></td></tr>
<tr><th id="2334">2334</th><td><i>//        node we want that all of its input nodes are visited and rewritten if</i></td></tr>
<tr><th id="2335">2335</th><td><i>//        applicable. This is because if we need to rewrite a given node</i></td></tr>
<tr><th id="2336">2336</th><td><i>//        then all of its input nodes need to be fixed (in other words they</i></td></tr>
<tr><th id="2337">2337</th><td><i>//        cannot be deleted later.)</i></td></tr>
<tr><th id="2338">2338</th><td><i>//</i></td></tr>
<tr><th id="2339">2339</th><td><i>//        While visiting a node, we first check if the op type of the node is</i></td></tr>
<tr><th id="2340">2340</th><td><i>//        an Mkl op. If it is, then we rewrite that node after constructing</i></td></tr>
<tr><th id="2341">2341</th><td><i>//        new inputs to the node. If the op type of the node is not Mkl op,</i></td></tr>
<tr><th id="2342">2342</th><td><i>//        then we do not rewrite that node.</i></td></tr>
<tr><th id="2343">2343</th><td><i>//</i></td></tr>
<tr><th id="2344">2344</th><td><i>// Handling workspace propagation for certain ops:</i></td></tr>
<tr><th id="2345">2345</th><td><i>//</i></td></tr>
<tr><th id="2346">2346</th><td><i>//        Certain backward ops in MKL (MaxPool, LRN and BatchNorm) require</i></td></tr>
<tr><th id="2347">2347</th><td><i>//        passing of a workspace from their respective forward ops. Workspace</i></td></tr>
<tr><th id="2348">2348</th><td><i>//        tensors provide memory for storing results of intermediate operations</i></td></tr>
<tr><th id="2349">2349</th><td><i>//        which are helpful in backward propagation. TensorFlow does not have</i></td></tr>
<tr><th id="2350">2350</th><td><i>//        a notion of a workspace and as a result does not allow producing</i></td></tr>
<tr><th id="2351">2351</th><td><i>//        additional outputs from these forward ops. For these ops, we need</i></td></tr>
<tr><th id="2352">2352</th><td><i>//        to add 2 extra edges between forward ops and their corresponding</i></td></tr>
<tr><th id="2353">2353</th><td><i>//        backward ops - the first extra edge carries a workspace tensor and</i></td></tr>
<tr><th id="2354">2354</th><td><i>//        the second one carries an Mkl tensor for the workspace tensor.</i></td></tr>
<tr><th id="2355">2355</th><td><i>//</i></td></tr>
<tr><th id="2356">2356</th><td><i>//        Example:</i></td></tr>
<tr><th id="2357">2357</th><td><i>//</i></td></tr>
<tr><th id="2358">2358</th><td><i>//        Typical graph for MaxPool and its gradient looks like:</i></td></tr>
<tr><th id="2359">2359</th><td><i>//</i></td></tr>
<tr><th id="2360">2360</th><td><i>//        A = MaxPool(T)</i></td></tr>
<tr><th id="2361">2361</th><td><i>//        B = MaxPoolGrad(X, A, Y)</i></td></tr>
<tr><th id="2362">2362</th><td><i>//</i></td></tr>
<tr><th id="2363">2363</th><td><i>//        We will transform this graph to propagate the workspace as:</i></td></tr>
<tr><th id="2364">2364</th><td><i>//        (with the contiguous ordering)</i></td></tr>
<tr><th id="2365">2365</th><td><i>//</i></td></tr>
<tr><th id="2366">2366</th><td><i>//        A, W, A_m, W_m = MklMaxPool(T, T_m)</i></td></tr>
<tr><th id="2367">2367</th><td><i>//        B, B_m = MklMaxPoolGrad(X, A, Y, W, X_m, A_m, Y_m, W_m)</i></td></tr>
<tr><th id="2368">2368</th><td><i>//</i></td></tr>
<tr><th id="2369">2369</th><td><i>//        Here W is the workspace tensor. Transformed tensor names with the</i></td></tr>
<tr><th id="2370">2370</th><td><i>//        suffix _m are Mkl tensors, and this transformation has been done</i></td></tr>
<tr><th id="2371">2371</th><td><i>//        using the algorithm discussed earlier. The transformation for</i></td></tr>
<tr><th id="2372">2372</th><td><i>//        workspace propagation only adds extra outputs (W, W_m) for a forward</i></td></tr>
<tr><th id="2373">2373</th><td><i>//        op and connects them to the corresponding backward ops.</i></td></tr>
<tr><th id="2374">2374</th><td><i>//</i></td></tr>
<tr><th id="2375">2375</th><td><i>//        Terms:</i></td></tr>
<tr><th id="2376">2376</th><td><i>//</i></td></tr>
<tr><th id="2377">2377</th><td><i>//        Forward op name = name of the op in the forward pass</i></td></tr>
<tr><th id="2378">2378</th><td><i>//          where a workspace tensor originates (MaxPool in this example)</i></td></tr>
<tr><th id="2379">2379</th><td><i>//        Backward op name = name of the op in the backward pass that receives</i></td></tr>
<tr><th id="2380">2380</th><td><i>//          a workspace tensor from the forward op (MaxPoolGrad in the example)</i></td></tr>
<tr><th id="2381">2381</th><td><i>//        Slot = Position of the output or input slot that will be</i></td></tr>
<tr><th id="2382">2382</th><td><i>//               used by the workspace tensor (1 for MklMaxPool as W is the 2nd</i></td></tr>
<tr><th id="2383">2383</th><td><i>//               output of MaxPool (0 is 1st); 3 for MklMaxPoolGrad)</i></td></tr>
<tr><th id="2384">2384</th><td><i>//</i></td></tr>
<tr><th id="2385">2385</th><td><i>//        Question:</i></td></tr>
<tr><th id="2386">2386</th><td><i>//</i></td></tr>
<tr><th id="2387">2387</th><td><i>//        How do we associate a backward op to a forward op? There can be more</i></td></tr>
<tr><th id="2388">2388</th><td><i>//        than one op with the exact same name.</i></td></tr>
<tr><th id="2389">2389</th><td><i>//</i></td></tr>
<tr><th id="2390">2390</th><td><i>//        In this example, we associate MaxPoolGrad with MaxPool. But there</i></td></tr>
<tr><th id="2391">2391</th><td><i>//        could be more than one MaxPool ops. To solve this problem, we look</i></td></tr>
<tr><th id="2392">2392</th><td><i>//        for _direct_ edge between a forward op and a backward op (tensor A is</i></td></tr>
<tr><th id="2393">2393</th><td><i>//        flowing along this edge in the example).</i></td></tr>
<tr><th id="2394">2394</th><td><i>//</i></td></tr>
<tr><th id="2395">2395</th><td><i>//        How do we transform forward and backward ops when there is no direct</i></td></tr>
<tr><th id="2396">2396</th><td><i>//        edge between them? In such a case, we generate dummy tensors for</i></td></tr>
<tr><th id="2397">2397</th><td><i>//        workspace tensors. For the example, transformation of MaxPool will</i></td></tr>
<tr><th id="2398">2398</th><td><i>//        be exactly same as it would be when there is a direct edge between</i></td></tr>
<tr><th id="2399">2399</th><td><i>//        the forward and the backward op --- it is just that MaxPool won't</i></td></tr>
<tr><th id="2400">2400</th><td><i>//        generate any workspace tensor. For MaxPoolGrad, the transformation</i></td></tr>
<tr><th id="2401">2401</th><td><i>//        will also be same, but instead of connecting W and W_m with the</i></td></tr>
<tr><th id="2402">2402</th><td><i>//        outputs of MaxPool, we will produce dummy tensors for them, and we</i></td></tr>
<tr><th id="2403">2403</th><td><i>//        will set workspace_enabled attribute to false.</i></td></tr>
<tr><th id="2404">2404</th><td><i>//</i></td></tr>
<tr><th id="2405">2405</th><td><b>class</b> MklLayoutRewritePass : <b>public</b> GraphOptimizationPass {</td></tr>
<tr><th id="2406">2406</th><td> <b>public</b>:</td></tr>
<tr><th id="2407">2407</th><td>  MklLayoutRewritePass() {</td></tr>
<tr><th id="2408">2408</th><td>    <i>// NOTE: names are alphabetically sorted.</i></td></tr>
<tr><th id="2409">2409</th><td>    csinfo_.addn = <q>"AddN"</q>;</td></tr>
<tr><th id="2410">2410</th><td>    csinfo_.avg_pool = <q>"AvgPool"</q>;</td></tr>
<tr><th id="2411">2411</th><td>    csinfo_.avg_pool_grad = <q>"AvgPoolGrad"</q>;</td></tr>
<tr><th id="2412">2412</th><td>    csinfo_.bias_add = <q>"BiasAdd"</q>;</td></tr>
<tr><th id="2413">2413</th><td>    csinfo_.bias_add_grad = <q>"BiasAddGrad"</q>;</td></tr>
<tr><th id="2414">2414</th><td>    csinfo_.concat = <q>"Concat"</q>;</td></tr>
<tr><th id="2415">2415</th><td>    csinfo_.concatv2 = <q>"ConcatV2"</q>;</td></tr>
<tr><th id="2416">2416</th><td>    csinfo_.conv2d = <q>"Conv2D"</q>;</td></tr>
<tr><th id="2417">2417</th><td>    csinfo_.conv2d_with_bias = <q>"__MklDummyConv2DWithBias"</q>;</td></tr>
<tr><th id="2418">2418</th><td>    csinfo_.conv2d_grad_input = <q>"Conv2DBackpropInput"</q>;</td></tr>
<tr><th id="2419">2419</th><td>    csinfo_.conv2d_grad_filter = <q>"Conv2DBackpropFilter"</q>;</td></tr>
<tr><th id="2420">2420</th><td>    csinfo_.conv2d_grad_filter_with_bias =</td></tr>
<tr><th id="2421">2421</th><td>        <q>"__MklDummyConv2DBackpropFilterWithBias"</q>;</td></tr>
<tr><th id="2422">2422</th><td>    csinfo_.fused_batch_norm = <q>"FusedBatchNorm"</q>;</td></tr>
<tr><th id="2423">2423</th><td>    csinfo_.fused_batch_norm_grad = <q>"FusedBatchNormGrad"</q>;</td></tr>
<tr><th id="2424">2424</th><td>    csinfo_.identity = <q>"Identity"</q>;</td></tr>
<tr><th id="2425">2425</th><td>    csinfo_.lrn = <q>"LRN"</q>;</td></tr>
<tr><th id="2426">2426</th><td>    csinfo_.lrn_grad = <q>"LRNGrad"</q>;</td></tr>
<tr><th id="2427">2427</th><td>    csinfo_.matmul = <q>"MatMul"</q>;</td></tr>
<tr><th id="2428">2428</th><td>    csinfo_.max_pool = <q>"MaxPool"</q>;</td></tr>
<tr><th id="2429">2429</th><td>    csinfo_.max_pool_grad = <q>"MaxPoolGrad"</q>;</td></tr>
<tr><th id="2430">2430</th><td>    csinfo_.mkl_conv2d = <q>"_MklConv2D"</q>;</td></tr>
<tr><th id="2431">2431</th><td>    csinfo_.mkl_conv2d_grad_input = <q>"_MklConv2DBackpropInput"</q>;</td></tr>
<tr><th id="2432">2432</th><td>    csinfo_.mkl_conv2d_grad_filter = <q>"_MklConv2DBackpropFilter"</q>;</td></tr>
<tr><th id="2433">2433</th><td>    csinfo_.mkl_conv2d_with_bias = <q>"_MklConv2DWithBias"</q>;</td></tr>
<tr><th id="2434">2434</th><td>    csinfo_.mkl_conv2d_grad_filter_with_bias =</td></tr>
<tr><th id="2435">2435</th><td>        <q>"_MklConv2DBackpropFilterWithBias"</q>;</td></tr>
<tr><th id="2436">2436</th><td>    csinfo_.relu = <q>"Relu"</q>;</td></tr>
<tr><th id="2437">2437</th><td>    csinfo_.relu_grad = <q>"ReluGrad"</q>;</td></tr>
<tr><th id="2438">2438</th><td>    csinfo_.tanh = <q>"Tanh"</q>;</td></tr>
<tr><th id="2439">2439</th><td>    csinfo_.tanh_grad = <q>"TanhGrad"</q>;</td></tr>
<tr><th id="2440">2440</th><td>    csinfo_.reshape = <q>"Reshape"</q>;</td></tr>
<tr><th id="2441">2441</th><td>    csinfo_.softmax = <q>"Softmax"</q>;</td></tr>
<tr><th id="2442">2442</th><td>    csinfo_.split = <q>"Split"</q>;</td></tr>
<tr><th id="2443">2443</th><td>    <i>// Element-wise ops. Ensure you also add any new ops to IsOpElementWise</i></td></tr>
<tr><th id="2444">2444</th><td><i>    // in the MklUtil.h (IsMklElementWiseOp method) to ensure that the</i></td></tr>
<tr><th id="2445">2445</th><td><i>    // MklInputConversion op is added before it.</i></td></tr>
<tr><th id="2446">2446</th><td>    csinfo_.add = <q>"Add"</q>;</td></tr>
<tr><th id="2447">2447</th><td>    csinfo_.maximum = <q>"Maximum"</q>;</td></tr>
<tr><th id="2448">2448</th><td>    csinfo_.mul = <q>"Mul"</q>;</td></tr>
<tr><th id="2449">2449</th><td>    csinfo_.squared_difference = <q>"SquaredDifference"</q>;</td></tr>
<tr><th id="2450">2450</th><td>    csinfo_.sub = <q>"Sub"</q>;</td></tr>
<tr><th id="2451">2451</th><td>    <i>// End - element-wise ops. See note above.</i></td></tr>
<tr><th id="2452">2452</th><td><i></i></td></tr>
<tr><th id="2453">2453</th><td><i>    // NOTE: names are alphabetically sorted.</i></td></tr>
<tr><th id="2454">2454</th><td>    rinfo_.push_back({csinfo_.addn, mkl_op_registry::GetMklOpName(csinfo_.addn),</td></tr>
<tr><th id="2455">2455</th><td>                      CopyAttrsAddN, AddNRewrite});</td></tr>
<tr><th id="2456">2456</th><td>    rinfo_.push_back({csinfo_.add, mkl_op_registry::GetMklOpName(csinfo_.add),</td></tr>
<tr><th id="2457">2457</th><td>                      CopyAttrsDataType, AlwaysRewrite});</td></tr>
<tr><th id="2458">2458</th><td>    rinfo_.push_back({csinfo_.avg_pool,</td></tr>
<tr><th id="2459">2459</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.avg_pool),</td></tr>
<tr><th id="2460">2460</th><td>                      CopyAttrsPooling, AlwaysRewrite});</td></tr>
<tr><th id="2461">2461</th><td>    rinfo_.push_back({csinfo_.avg_pool_grad,</td></tr>
<tr><th id="2462">2462</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.avg_pool_grad),</td></tr>
<tr><th id="2463">2463</th><td>                      CopyAttrsPooling, AlwaysRewrite});</td></tr>
<tr><th id="2464">2464</th><td>    rinfo_.push_back({csinfo_.concat,</td></tr>
<tr><th id="2465">2465</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.concat),</td></tr>
<tr><th id="2466">2466</th><td>                      CopyAttrsConcat, AlwaysRewrite});</td></tr>
<tr><th id="2467">2467</th><td>    rinfo_.push_back({csinfo_.concatv2,</td></tr>
<tr><th id="2468">2468</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.concatv2),</td></tr>
<tr><th id="2469">2469</th><td>                      CopyAttrsConcatV2, AlwaysRewrite});</td></tr>
<tr><th id="2470">2470</th><td>    rinfo_.push_back({csinfo_.conv2d,</td></tr>
<tr><th id="2471">2471</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.conv2d),</td></tr>
<tr><th id="2472">2472</th><td>                      CopyAttrsConv2D, AlwaysRewrite});</td></tr>
<tr><th id="2473">2473</th><td>    rinfo_.push_back({csinfo_.conv2d_with_bias, csinfo_.mkl_conv2d_with_bias,</td></tr>
<tr><th id="2474">2474</th><td>                      CopyAttrsConv2D, AlwaysRewrite});</td></tr>
<tr><th id="2475">2475</th><td>    rinfo_.push_back({csinfo_.conv2d_grad_filter,</td></tr>
<tr><th id="2476">2476</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.conv2d_grad_filter),</td></tr>
<tr><th id="2477">2477</th><td>                      CopyAttrsConv2D, AlwaysRewrite});</td></tr>
<tr><th id="2478">2478</th><td>    rinfo_.push_back({csinfo_.conv2d_grad_filter_with_bias,</td></tr>
<tr><th id="2479">2479</th><td>                      csinfo_.mkl_conv2d_grad_filter_with_bias, CopyAttrsConv2D,</td></tr>
<tr><th id="2480">2480</th><td>                      AlwaysRewrite});</td></tr>
<tr><th id="2481">2481</th><td>    rinfo_.push_back({csinfo_.conv2d_grad_input,</td></tr>
<tr><th id="2482">2482</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.conv2d_grad_input),</td></tr>
<tr><th id="2483">2483</th><td>                      CopyAttrsConv2D, AlwaysRewrite});</td></tr>
<tr><th id="2484">2484</th><td>    rinfo_.push_back({csinfo_.fused_batch_norm,</td></tr>
<tr><th id="2485">2485</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.fused_batch_norm),</td></tr>
<tr><th id="2486">2486</th><td>                      CopyAttrsFusedBatchNorm, AlwaysRewrite});</td></tr>
<tr><th id="2487">2487</th><td>    rinfo_.push_back(</td></tr>
<tr><th id="2488">2488</th><td>        {csinfo_.fused_batch_norm_grad,</td></tr>
<tr><th id="2489">2489</th><td>         mkl_op_registry::GetMklOpName(csinfo_.fused_batch_norm_grad),</td></tr>
<tr><th id="2490">2490</th><td>         CopyAttrsFusedBatchNorm, AlwaysRewrite});</td></tr>
<tr><th id="2491">2491</th><td>    rinfo_.push_back({csinfo_.identity,</td></tr>
<tr><th id="2492">2492</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.identity),</td></tr>
<tr><th id="2493">2493</th><td>                      CopyAttrsDataType, AlwaysRewrite});</td></tr>
<tr><th id="2494">2494</th><td>    rinfo_.push_back({csinfo_.lrn, mkl_op_registry::GetMklOpName(csinfo_.lrn),</td></tr>
<tr><th id="2495">2495</th><td>                      CopyAttrsLRN, LrnRewrite});</td></tr>
<tr><th id="2496">2496</th><td>    rinfo_.push_back({csinfo_.lrn_grad,</td></tr>
<tr><th id="2497">2497</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.lrn_grad),</td></tr>
<tr><th id="2498">2498</th><td>                      CopyAttrsLRN, LrnRewrite});</td></tr>
<tr><th id="2499">2499</th><td>    rinfo_.push_back({csinfo_.max_pool,</td></tr>
<tr><th id="2500">2500</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.max_pool),</td></tr>
<tr><th id="2501">2501</th><td>                      CopyAttrsPooling, NonDepthBatchWisePoolRewrite});</td></tr>
<tr><th id="2502">2502</th><td>    rinfo_.push_back({csinfo_.max_pool_grad,</td></tr>
<tr><th id="2503">2503</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.max_pool_grad),</td></tr>
<tr><th id="2504">2504</th><td>                      CopyAttrsPooling, AlwaysRewrite});</td></tr>
<tr><th id="2505">2505</th><td></td></tr>
<tr><th id="2506">2506</th><td>    rinfo_.push_back({csinfo_.maximum,</td></tr>
<tr><th id="2507">2507</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.maximum),</td></tr>
<tr><th id="2508">2508</th><td>                      CopyAttrsDataType, AlwaysRewrite});</td></tr>
<tr><th id="2509">2509</th><td>    rinfo_.push_back({csinfo_.mul,</td></tr>
<tr><th id="2510">2510</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.mul),</td></tr>
<tr><th id="2511">2511</th><td>                      CopyAttrsDataType, AlwaysRewrite});</td></tr>
<tr><th id="2512">2512</th><td>    rinfo_.push_back({csinfo_.relu, mkl_op_registry::GetMklOpName(csinfo_.relu),</td></tr>
<tr><th id="2513">2513</th><td>                      CopyAttrsDataType, AlwaysRewrite});</td></tr>
<tr><th id="2514">2514</th><td>    rinfo_.push_back({csinfo_.relu_grad,</td></tr>
<tr><th id="2515">2515</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.relu_grad),</td></tr>
<tr><th id="2516">2516</th><td>                      CopyAttrsDataType, AlwaysRewrite});</td></tr>
<tr><th id="2517">2517</th><td>    <i>/*</i></td></tr>
<tr><th id="2518">2518</th><td><i>    rinfo_.push_back({csinfo_.tanh,</i></td></tr>
<tr><th id="2519">2519</th><td><i>                      mkl_op_registry::GetMklOpName(csinfo_.tanh),</i></td></tr>
<tr><th id="2520">2520</th><td><i>                      CopyAttrsDataType, AlwaysRewrite});</i></td></tr>
<tr><th id="2521">2521</th><td><i>    rinfo_.push_back({csinfo_.tanh_grad,</i></td></tr>
<tr><th id="2522">2522</th><td><i>                      mkl_op_registry::GetMklOpName(csinfo_.tanh_grad),</i></td></tr>
<tr><th id="2523">2523</th><td><i>                      CopyAttrsDataType, AlwaysRewrite});</i></td></tr>
<tr><th id="2524">2524</th><td><i>    */</i></td></tr>
<tr><th id="2525">2525</th><td>    rinfo_.push_back({csinfo_.reshape,</td></tr>
<tr><th id="2526">2526</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.reshape),</td></tr>
<tr><th id="2527">2527</th><td>                      CopyAttrsReshape, AlwaysRewrite});</td></tr>
<tr><th id="2528">2528</th><td>    rinfo_.push_back({csinfo_.softmax,</td></tr>
<tr><th id="2529">2529</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.softmax),</td></tr>
<tr><th id="2530">2530</th><td>                      CopyAttrsDataType, AlwaysRewrite});</td></tr>
<tr><th id="2531">2531</th><td></td></tr>
<tr><th id="2532">2532</th><td>    rinfo_.push_back({csinfo_.squared_difference,</td></tr>
<tr><th id="2533">2533</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.squared_difference),</td></tr>
<tr><th id="2534">2534</th><td>                      CopyAttrsDataType, AlwaysRewrite});</td></tr>
<tr><th id="2535">2535</th><td>    rinfo_.push_back({csinfo_.sub,</td></tr>
<tr><th id="2536">2536</th><td>                      mkl_op_registry::GetMklOpName(csinfo_.sub),</td></tr>
<tr><th id="2537">2537</th><td>                      CopyAttrsDataType, AlwaysRewrite});</td></tr>
<tr><th id="2538">2538</th><td></td></tr>
<tr><th id="2539">2539</th><td>    <i>// Add info about which ops to add workspace edge to and the slots.</i></td></tr>
<tr><th id="2540">2540</th><td>    wsinfo_.push_back({csinfo_.lrn, csinfo_.lrn_grad, <var>0</var>, <var>2</var>, <var>1</var>, <var>3</var>});</td></tr>
<tr><th id="2541">2541</th><td>    wsinfo_.push_back({csinfo_.max_pool, csinfo_.max_pool_grad, <var>0</var>, <var>1</var>, <var>1</var>, <var>3</var>});</td></tr>
<tr><th id="2542">2542</th><td></td></tr>
<tr><th id="2543">2543</th><td>    <i>// Add a rule for merging nodes</i></td></tr>
<tr><th id="2544">2544</th><td>    minfo_.push_back({csinfo_.conv2d, csinfo_.bias_add,</td></tr>
<tr><th id="2545">2545</th><td>                      csinfo_.conv2d_with_bias, GetConv2DOrBiasAdd});</td></tr>
<tr><th id="2546">2546</th><td></td></tr>
<tr><th id="2547">2547</th><td>    minfo_.push_back({csinfo_.conv2d_grad_filter, csinfo_.bias_add_grad,</td></tr>
<tr><th id="2548">2548</th><td>                      csinfo_.conv2d_grad_filter_with_bias,</td></tr>
<tr><th id="2549">2549</th><td>                      GetConv2DBackpropFilterOrBiasAddGrad});</td></tr>
<tr><th id="2550">2550</th><td>  }</td></tr>
<tr><th id="2551">2551</th><td></td></tr>
<tr><th id="2552">2552</th><td>  <i>// Standard interface to run pass</i></td></tr>
<tr><th id="2553">2553</th><td>  Status Run(<em>const</em> GraphOptimizationPassOptions&amp; options);</td></tr>
<tr><th id="2554">2554</th><td></td></tr>
<tr><th id="2555">2555</th><td>  <i>// Helper function which does most of heavy lifting for rewriting</i></td></tr>
<tr><th id="2556">2556</th><td><i>  // Mkl nodes to propagate Mkl tensor as additional output</i></td></tr>
<tr><th id="2557">2557</th><td><i>  //</i></td></tr>
<tr><th id="2558">2558</th><td><i>  // Extracts common functionality between Run public interface and</i></td></tr>
<tr><th id="2559">2559</th><td><i>  // test interface.</i></td></tr>
<tr><th id="2560">2560</th><td><i>  //</i></td></tr>
<tr><th id="2561">2561</th><td><i>  // @return true, if and only if graph is mutated; false otherwise.</i></td></tr>
<tr><th id="2562">2562</th><td>  <em>bool</em> RunPass(std::unique_ptr&lt;Graph&gt;* g);</td></tr>
<tr><th id="2563">2563</th><td></td></tr>
<tr><th id="2564">2564</th><td>  <i class="doc">/// Structure to specify the name of an original node, its new name after</i></td></tr>
<tr><th id="2565">2565</th><td><i class="doc">  /// rewrite, the number of inputs to the original node, the function to</i></td></tr>
<tr><th id="2566">2566</th><td><i class="doc">  /// be used to copy attributes for the op, and the rule (if any) which</i></td></tr>
<tr><th id="2567">2567</th><td><i class="doc">  /// must hold for rewriting the node</i></td></tr>
<tr><th id="2568">2568</th><td>  <b>typedef</b> <b>struct</b> {</td></tr>
<tr><th id="2569">2569</th><td>    string name;      <i>// Original name of op of the node in the graph</i></td></tr>
<tr><th id="2570">2570</th><td>    string new_name;  <i>// New name of the op of the node in the graph</i></td></tr>
<tr><th id="2571">2571</th><td>    <i>// A function handler to copy attributes from an old node to a new node.</i></td></tr>
<tr><th id="2572">2572</th><td>    std::function&lt;<em>void</em>(<em>const</em> Node*, NodeBuilder*)&gt; copy_attrs;</td></tr>
<tr><th id="2573">2573</th><td>    <i>// A rule under which to rewrite this node</i></td></tr>
<tr><th id="2574">2574</th><td>    std::function&lt;<em>bool</em>(<em>const</em> Node*)&gt; rewrite_rule;</td></tr>
<tr><th id="2575">2575</th><td>  } RewriteInfo;</td></tr>
<tr><th id="2576">2576</th><td></td></tr>
<tr><th id="2577">2577</th><td>  <i class="doc">/// Structure to specify a forward op, a backward op, and the slot numbers</i></td></tr>
<tr><th id="2578">2578</th><td><i class="doc">  /// in the forward and backward ops where we will add a workspace edge.</i></td></tr>
<tr><th id="2579">2579</th><td>  <b>typedef</b> <b>struct</b> {</td></tr>
<tr><th id="2580">2580</th><td>    string fwd_op;    <i>// Name of a forward op in the graph</i></td></tr>
<tr><th id="2581">2581</th><td>    string bwd_op;    <i>// Name of a backward op in the graph</i></td></tr>
<tr><th id="2582">2582</th><td>    <em>int</em> fwd_slot;     <i>// Output slot in the forward op node where actual</i></td></tr>
<tr><th id="2583">2583</th><td>                      <i>// output tensor resides</i></td></tr>
<tr><th id="2584">2584</th><td>    <em>int</em> bwd_slot;     <i>// Input slot in the backward op node where actual</i></td></tr>
<tr><th id="2585">2585</th><td>                      <i>// input tensor resides</i></td></tr>
<tr><th id="2586">2586</th><td>    <em>int</em> ws_fwd_slot;  <i>// Output slot in the forward op node where workspace</i></td></tr>
<tr><th id="2587">2587</th><td>                      <i>// edge is added</i></td></tr>
<tr><th id="2588">2588</th><td>    <em>int</em> ws_bwd_slot;  <i>// Input slot in the backward op node where workspace</i></td></tr>
<tr><th id="2589">2589</th><td>                      <i>// edge is added</i></td></tr>
<tr><th id="2590">2590</th><td>  } WorkSpaceInfo;</td></tr>
<tr><th id="2591">2591</th><td></td></tr>
<tr><th id="2592">2592</th><td>  <i class="doc">/// Structure to specify information used in node merge of 2 operators</i></td></tr>
<tr><th id="2593">2593</th><td>  <b>typedef</b> <b>struct</b> {</td></tr>
<tr><th id="2594">2594</th><td>    string op1;       <i>// Node string for one operator.</i></td></tr>
<tr><th id="2595">2595</th><td>    string op2;       <i>// Node string for second operator.</i></td></tr>
<tr><th id="2596">2596</th><td>    string new_node;  <i>// Name of the node after merge</i></td></tr>
<tr><th id="2597">2597</th><td>    <i>// Function that enables user of the node merger to specify how to find</i></td></tr>
<tr><th id="2598">2598</th><td><i>    // second operator given the first operator.</i></td></tr>
<tr><th id="2599">2599</th><td>    std::function&lt;Node*(<em>const</em> Node*)&gt; get_node_to_be_merged;</td></tr>
<tr><th id="2600">2600</th><td>  } MergeInfo;</td></tr>
<tr><th id="2601">2601</th><td></td></tr>
<tr><th id="2602">2602</th><td>  <i class="doc">/// Structure to store all constant strings</i></td></tr>
<tr><th id="2603">2603</th><td><i class="doc">  /// NOTE: names are alphabetically sorted.</i></td></tr>
<tr><th id="2604">2604</th><td>  <b>typedef</b> <b>struct</b> {</td></tr>
<tr><th id="2605">2605</th><td>    string addn;</td></tr>
<tr><th id="2606">2606</th><td>    string add;</td></tr>
<tr><th id="2607">2607</th><td>    string avg_pool;</td></tr>
<tr><th id="2608">2608</th><td>    string avg_pool_grad;</td></tr>
<tr><th id="2609">2609</th><td>    string bias_add;</td></tr>
<tr><th id="2610">2610</th><td>    string bias_add_grad;</td></tr>
<tr><th id="2611">2611</th><td>    string concat;</td></tr>
<tr><th id="2612">2612</th><td>    string concatv2;</td></tr>
<tr><th id="2613">2613</th><td>    string conv2d;</td></tr>
<tr><th id="2614">2614</th><td>    string conv2d_with_bias;</td></tr>
<tr><th id="2615">2615</th><td>    string conv2d_grad_input;</td></tr>
<tr><th id="2616">2616</th><td>    string conv2d_grad_filter;</td></tr>
<tr><th id="2617">2617</th><td>    string conv2d_grad_filter_with_bias;</td></tr>
<tr><th id="2618">2618</th><td>    string fused_batch_norm;</td></tr>
<tr><th id="2619">2619</th><td>    string fused_batch_norm_grad;</td></tr>
<tr><th id="2620">2620</th><td>    string identity;</td></tr>
<tr><th id="2621">2621</th><td>    string lrn;</td></tr>
<tr><th id="2622">2622</th><td>    string lrn_grad;</td></tr>
<tr><th id="2623">2623</th><td>    string matmul;</td></tr>
<tr><th id="2624">2624</th><td>    string max_pool;</td></tr>
<tr><th id="2625">2625</th><td>    string max_pool_grad;</td></tr>
<tr><th id="2626">2626</th><td>    string maximum;</td></tr>
<tr><th id="2627">2627</th><td>    string mkl_conv2d;</td></tr>
<tr><th id="2628">2628</th><td>    string mkl_conv2d_grad_input;</td></tr>
<tr><th id="2629">2629</th><td>    string mkl_conv2d_grad_filter;</td></tr>
<tr><th id="2630">2630</th><td>    string mkl_conv2d_grad_filter_with_bias;</td></tr>
<tr><th id="2631">2631</th><td>    string mkl_conv2d_with_bias;</td></tr>
<tr><th id="2632">2632</th><td>    string mul;</td></tr>
<tr><th id="2633">2633</th><td>    string relu;</td></tr>
<tr><th id="2634">2634</th><td>    string relu_grad;</td></tr>
<tr><th id="2635">2635</th><td>    string tanh;</td></tr>
<tr><th id="2636">2636</th><td>    string tanh_grad;</td></tr>
<tr><th id="2637">2637</th><td>    string reshape;</td></tr>
<tr><th id="2638">2638</th><td>    string softmax;</td></tr>
<tr><th id="2639">2639</th><td>    string split;</td></tr>
<tr><th id="2640">2640</th><td>    string squared_difference;</td></tr>
<tr><th id="2641">2641</th><td>    string sub;</td></tr>
<tr><th id="2642">2642</th><td>  } ConstStringsInfo;</td></tr>
<tr><th id="2643">2643</th><td></td></tr>
<tr><th id="2644">2644</th><td> <b>private</b>:</td></tr>
<tr><th id="2645">2645</th><td>  <i class="doc">/// Maintain info about nodes to rewrite</i></td></tr>
<tr><th id="2646">2646</th><td>  std::vector&lt;RewriteInfo&gt; rinfo_;</td></tr>
<tr><th id="2647">2647</th><td></td></tr>
<tr><th id="2648">2648</th><td>  <i class="doc">/// Maintain info about nodes to add workspace edge</i></td></tr>
<tr><th id="2649">2649</th><td>  std::vector&lt;WorkSpaceInfo&gt; wsinfo_;</td></tr>
<tr><th id="2650">2650</th><td></td></tr>
<tr><th id="2651">2651</th><td>  <i class="doc">/// Maintain info about nodes to be merged</i></td></tr>
<tr><th id="2652">2652</th><td>  std::vector&lt;MergeInfo&gt; minfo_;</td></tr>
<tr><th id="2653">2653</th><td></td></tr>
<tr><th id="2654">2654</th><td>  <i class="doc">/// Maintain structure of constant strings</i></td></tr>
<tr><th id="2655">2655</th><td>  <em>static</em> ConstStringsInfo csinfo_;</td></tr>
<tr><th id="2656">2656</th><td></td></tr>
<tr><th id="2657">2657</th><td> <b>private</b>:</td></tr>
<tr><th id="2658">2658</th><td>  <i>// Is OpDef::ArgDef a list type? It could be N * T or list(type).</i></td></tr>
<tr><th id="2659">2659</th><td><i>  // Refer to opdef.proto for details of list type.</i></td></tr>
<tr><th id="2660">2660</th><td>  <b>inline</b> <em>bool</em> ArgIsList(<em>const</em> OpDef::ArgDef&amp; arg) <em>const</em> {</td></tr>
<tr><th id="2661">2661</th><td>    <b>return</b> !arg.type_list_attr().empty() || !arg.number_attr().empty();</td></tr>
<tr><th id="2662">2662</th><td>  }</td></tr>
<tr><th id="2663">2663</th><td></td></tr>
<tr><th id="2664">2664</th><td>  <i>// Get length of a list in 'n' if 'arg' is of list type. Refer to</i></td></tr>
<tr><th id="2665">2665</th><td><i>  // description of ArgIsList for definition of list type.</i></td></tr>
<tr><th id="2666">2666</th><td>  <b>inline</b> <em>int</em> GetTensorListLength(<em>const</em> OpDef::ArgDef&amp; arg, Node* n) {</td></tr>
<tr><th id="2667">2667</th><td>    CHECK_EQ(ArgIsList(arg), <b>true</b>);</td></tr>
<tr><th id="2668">2668</th><td>    <em>int</em> N = <var>0</var>;</td></tr>
<tr><th id="2669">2669</th><td>    <em>const</em> string attr_name = !arg.type_list_attr().empty()</td></tr>
<tr><th id="2670">2670</th><td>                                 ? arg.type_list_attr()</td></tr>
<tr><th id="2671">2671</th><td>                                 : arg.number_attr();</td></tr>
<tr><th id="2672">2672</th><td>    <b>if</b> (!arg.type_list_attr().empty()) {</td></tr>
<tr><th id="2673">2673</th><td>      std::vector&lt;DataType&gt; value;</td></tr>
<tr><th id="2674">2674</th><td>      TF_CHECK_OK(GetNodeAttr(n-&gt;def(), attr_name, &amp;value));</td></tr>
<tr><th id="2675">2675</th><td>      N = value.size();</td></tr>
<tr><th id="2676">2676</th><td>    } <b>else</b> {</td></tr>
<tr><th id="2677">2677</th><td>      TF_CHECK_OK(GetNodeAttr(n-&gt;def(), attr_name, &amp;N));</td></tr>
<tr><th id="2678">2678</th><td>    }</td></tr>
<tr><th id="2679">2679</th><td>    <b>return</b> N;</td></tr>
<tr><th id="2680">2680</th><td>  }</td></tr>
<tr><th id="2681">2681</th><td></td></tr>
<tr><th id="2682">2682</th><td>  <i>// Can op represented by node 'n' run on DEVICE_CPU?</i></td></tr>
<tr><th id="2683">2683</th><td><i>  // Op can run on CPU with MKL if the runtime assigned device or the</i></td></tr>
<tr><th id="2684">2684</th><td><i>  // user requested device contains device CPU, or both are empty.</i></td></tr>
<tr><th id="2685">2685</th><td>  <em>bool</em> CanOpRunOnCPUDevice(<em>const</em> Node* n) {</td></tr>
<tr><th id="2686">2686</th><td>    <em>bool</em> result = <b>true</b>;</td></tr>
<tr><th id="2687">2687</th><td>    string reason;</td></tr>
<tr><th id="2688">2688</th><td></td></tr>
<tr><th id="2689">2689</th><td>    <i>// Substring that should be checked for in device name for CPU device.</i></td></tr>
<tr><th id="2690">2690</th><td>    <em>const</em> <em>char</em>* <em>const</em> kCPUDeviceSubStr = <q>"CPU"</q>;</td></tr>
<tr><th id="2691">2691</th><td></td></tr>
<tr><th id="2692">2692</th><td>    <i>// If Op has been specifically assigned to a non-CPU device, then No.</i></td></tr>
<tr><th id="2693">2693</th><td>    <b>if</b> (!n-&gt;assigned_device_name().empty() &amp;&amp;</td></tr>
<tr><th id="2694">2694</th><td>        !StringPiece(n-&gt;assigned_device_name()).contains(kCPUDeviceSubStr)) {</td></tr>
<tr><th id="2695">2695</th><td>      result = <b>false</b>;</td></tr>
<tr><th id="2696">2696</th><td>      reason = <q>"Op has been assigned a runtime device that is not CPU."</q>;</td></tr>
<tr><th id="2697">2697</th><td>    }</td></tr>
<tr><th id="2698">2698</th><td></td></tr>
<tr><th id="2699">2699</th><td>    <i>// If user has specifically assigned this op to a non-CPU device, then No.</i></td></tr>
<tr><th id="2700">2700</th><td>    <b>if</b> (!n-&gt;def().device().empty() &amp;&amp;</td></tr>
<tr><th id="2701">2701</th><td>        !StringPiece(n-&gt;def().device()).contains(kCPUDeviceSubStr)) {</td></tr>
<tr><th id="2702">2702</th><td>      result = <b>false</b>;</td></tr>
<tr><th id="2703">2703</th><td>      reason = <q>"User has assigned a device that is not CPU."</q>;</td></tr>
<tr><th id="2704">2704</th><td>    }</td></tr>
<tr><th id="2705">2705</th><td></td></tr>
<tr><th id="2706">2706</th><td>    <b>if</b> (result == <b>false</b>) {</td></tr>
<tr><th id="2707">2707</th><td>      VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: Skipping rewriting of the node "</q></td></tr>
<tr><th id="2708">2708</th><td>              &lt;&lt; n-&gt;type_string() &lt;&lt; <q>", reason: "</q> &lt;&lt; reason;</td></tr>
<tr><th id="2709">2709</th><td>    }</td></tr>
<tr><th id="2710">2710</th><td></td></tr>
<tr><th id="2711">2711</th><td>    <i>// Otherwise Yes.</i></td></tr>
<tr><th id="2712">2712</th><td>    <b>return</b> result;</td></tr>
<tr><th id="2713">2713</th><td>  }</td></tr>
<tr><th id="2714">2714</th><td></td></tr>
<tr><th id="2715">2715</th><td>  <i>// Return a node that can be merged with input node 'n'</i></td></tr>
<tr><th id="2716">2716</th><td><i>  //</i></td></tr>
<tr><th id="2717">2717</th><td><i>  // @return pointer to the node if we can find such a</i></td></tr>
<tr><th id="2718">2718</th><td><i>  // node. Otherwise, it returns nullptr.</i></td></tr>
<tr><th id="2719">2719</th><td>  Node* CheckForNodeMerge(<em>const</em> Node* n) <em>const</em>;</td></tr>
<tr><th id="2720">2720</th><td></td></tr>
<tr><th id="2721">2721</th><td>  <i>// Merge node 'm' with node 'n'.</i></td></tr>
<tr><th id="2722">2722</th><td><i>  // Currently, we merge (1) Conv2D with BiasAdd, and (2) BiasAddGrad with</i></td></tr>
<tr><th id="2723">2723</th><td><i>  // Conv2DBackpropFilter.</i></td></tr>
<tr><th id="2724">2724</th><td><i>  //</i></td></tr>
<tr><th id="2725">2725</th><td><i>  // Input nodes m and n may be deleted if the call to</i></td></tr>
<tr><th id="2726">2726</th><td><i>  // this function is successful. Attempt to use the pointers</i></td></tr>
<tr><th id="2727">2727</th><td><i>  // after the call to function may result in undefined behaviors.</i></td></tr>
<tr><th id="2728">2728</th><td><i>  //</i></td></tr>
<tr><th id="2729">2729</th><td><i>  // @input g - input graph, m - graph node, n - graph node to be merged with m</i></td></tr>
<tr><th id="2730">2730</th><td><i>  // @return Status::OK(), if merging is successful and supported.</i></td></tr>
<tr><th id="2731">2731</th><td><i>  //         Returns appropriate Status error code otherwise.</i></td></tr>
<tr><th id="2732">2732</th><td><i>  //         Graph is updated in case nodes are merged. Otherwise, it is</i></td></tr>
<tr><th id="2733">2733</th><td><i>  //         not updated.</i></td></tr>
<tr><th id="2734">2734</th><td>  Status MergeNode(std::unique_ptr&lt;Graph&gt;* g, Node* m, Node* n);</td></tr>
<tr><th id="2735">2735</th><td></td></tr>
<tr><th id="2736">2736</th><td>  <i>// Helper function to merge different nodes</i></td></tr>
<tr><th id="2737">2737</th><td>  Status MergeConv2DWithBiasAdd(std::unique_ptr&lt;Graph&gt;* g, Node* m, Node* n);</td></tr>
<tr><th id="2738">2738</th><td>  Status MergeConv2DBackpropFilterWithBiasAddGrad(std::unique_ptr&lt;Graph&gt;* g,</td></tr>
<tr><th id="2739">2739</th><td>                                                  Node* m, Node* n);</td></tr>
<tr><th id="2740">2740</th><td></td></tr>
<tr><th id="2741">2741</th><td>  <i>// Find BiasAdd or Conv2D node that can be merged with input node 'm'.</i></td></tr>
<tr><th id="2742">2742</th><td><i>  // If input 'm' is BiasAdd, then check if there exists Conv2D node that can be</i></td></tr>
<tr><th id="2743">2743</th><td><i>  // merged with 'm'. If input 'm' is Conv2D, then check if there exists BiasAdd</i></td></tr>
<tr><th id="2744">2744</th><td><i>  // node that can be merged with 'm'.</i></td></tr>
<tr><th id="2745">2745</th><td>  <em>static</em> Node* GetConv2DOrBiasAdd(<em>const</em> Node* m) {</td></tr>
<tr><th id="2746">2746</th><td>    CHECK_NOTNULL(m);</td></tr>
<tr><th id="2747">2747</th><td>    Node* n = <b>nullptr</b>;</td></tr>
<tr><th id="2748">2748</th><td></td></tr>
<tr><th id="2749">2749</th><td>    <b>if</b> (m-&gt;type_string() == csinfo_.bias_add) {</td></tr>
<tr><th id="2750">2750</th><td>      <i>// If a is BiasAdd, then Conv2D is 0th input of BiasAdd.</i></td></tr>
<tr><th id="2751">2751</th><td>      TF_CHECK_OK(m-&gt;input_node(<var>0</var>, &amp;n));</td></tr>
<tr><th id="2752">2752</th><td>    } <b>else</b> {</td></tr>
<tr><th id="2753">2753</th><td>      CHECK_EQ(m-&gt;type_string(), csinfo_.conv2d);</td></tr>
<tr><th id="2754">2754</th><td>      <i>// Go over all output edges and search for BiasAdd Node.</i></td></tr>
<tr><th id="2755">2755</th><td><i>      // 0th input of BiasAdd is Conv2D.</i></td></tr>
<tr><th id="2756">2756</th><td>      <b>for</b> (<em>const</em> Edge* e : m-&gt;out_edges()) {</td></tr>
<tr><th id="2757">2757</th><td>        <b>if</b> (!e-&gt;IsControlEdge() &amp;&amp;</td></tr>
<tr><th id="2758">2758</th><td>            e-&gt;dst()-&gt;type_string() == csinfo_.bias_add &amp;&amp;</td></tr>
<tr><th id="2759">2759</th><td>            e-&gt;dst_input() == <var>0</var>) {</td></tr>
<tr><th id="2760">2760</th><td>          n = e-&gt;dst();</td></tr>
<tr><th id="2761">2761</th><td>          <b>break</b>;</td></tr>
<tr><th id="2762">2762</th><td>        }</td></tr>
<tr><th id="2763">2763</th><td>      }</td></tr>
<tr><th id="2764">2764</th><td>    }</td></tr>
<tr><th id="2765">2765</th><td></td></tr>
<tr><th id="2766">2766</th><td>    <b>if</b> (n == <b>nullptr</b>) {</td></tr>
<tr><th id="2767">2767</th><td>      VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: Could not find matching "</q></td></tr>
<tr><th id="2768">2768</th><td>              &lt;&lt; <q>"Conv2D and BiasAdd node for merging. Input node: "</q></td></tr>
<tr><th id="2769">2769</th><td>              &lt;&lt; m-&gt;DebugString();</td></tr>
<tr><th id="2770">2770</th><td>    }</td></tr>
<tr><th id="2771">2771</th><td></td></tr>
<tr><th id="2772">2772</th><td>    <b>return</b> n;</td></tr>
<tr><th id="2773">2773</th><td>  }</td></tr>
<tr><th id="2774">2774</th><td></td></tr>
<tr><th id="2775">2775</th><td>  <i>// Find Conv2DBackpropFilter or BiasAddGrad node that can be merged with input</i></td></tr>
<tr><th id="2776">2776</th><td><i>  // node 'm'. If input 'm' is Conv2DBackpropFilter, then check if there exists</i></td></tr>
<tr><th id="2777">2777</th><td><i>  // BiasAddGrad node that can be merged with 'm'. If input 'm' is BiasAddGrad,</i></td></tr>
<tr><th id="2778">2778</th><td><i>  // then check if there exists Conv2DBackpropFilter node that can be merged</i></td></tr>
<tr><th id="2779">2779</th><td><i>  // with 'm'.</i></td></tr>
<tr><th id="2780">2780</th><td><i>  //</i></td></tr>
<tr><th id="2781">2781</th><td><i>  // Graph that will allow us to connect Conv2DBackpropFilter with BiasAddGrad</i></td></tr>
<tr><th id="2782">2782</th><td><i>  // would look like:</i></td></tr>
<tr><th id="2783">2783</th><td><i>  //</i></td></tr>
<tr><th id="2784">2784</th><td><i>  // _ = Conv2DBackpropFilter(F, _, G)</i></td></tr>
<tr><th id="2785">2785</th><td><i>  // _ = BiasAddGrad(G)</i></td></tr>
<tr><th id="2786">2786</th><td><i>  //</i></td></tr>
<tr><th id="2787">2787</th><td><i>  // So 1st input of BiasAddGrad connects with 3rd input of</i></td></tr>
<tr><th id="2788">2788</th><td><i>  // Conv2DBackpropFilter and vice versa.</i></td></tr>
<tr><th id="2789">2789</th><td>  <em>static</em> Node* GetConv2DBackpropFilterOrBiasAddGrad(<em>const</em> Node* m) {</td></tr>
<tr><th id="2790">2790</th><td>    CHECK_NOTNULL(m);</td></tr>
<tr><th id="2791">2791</th><td>    Node* n = <b>nullptr</b>;</td></tr>
<tr><th id="2792">2792</th><td></td></tr>
<tr><th id="2793">2793</th><td>    <b>if</b> (m-&gt;type_string() == csinfo_.bias_add_grad) {</td></tr>
<tr><th id="2794">2794</th><td>      <i>// Get 1st input 'g' of BiasAddGrad.</i></td></tr>
<tr><th id="2795">2795</th><td>      Node* g = <b>nullptr</b>;</td></tr>
<tr><th id="2796">2796</th><td>      TF_CHECK_OK(m-&gt;input_node(<var>0</var>, &amp;g));</td></tr>
<tr><th id="2797">2797</th><td>      <i>// Now traverse all outgoing edges from g that have destination node as</i></td></tr>
<tr><th id="2798">2798</th><td><i>      // Conv2DBackpropFilter.</i></td></tr>
<tr><th id="2799">2799</th><td>      <b>for</b> (<em>const</em> Edge* e : g-&gt;out_edges()) {</td></tr>
<tr><th id="2800">2800</th><td>        <b>if</b> (!e-&gt;IsControlEdge() &amp;&amp;</td></tr>
<tr><th id="2801">2801</th><td>            e-&gt;dst()-&gt;type_string() == csinfo_.conv2d_grad_filter &amp;&amp;</td></tr>
<tr><th id="2802">2802</th><td>            e-&gt;dst_input() == <var>2</var> <i>/* 3rd input of BackpropFilter */</i>) {</td></tr>
<tr><th id="2803">2803</th><td>          n = e-&gt;dst();</td></tr>
<tr><th id="2804">2804</th><td>          <b>break</b>;</td></tr>
<tr><th id="2805">2805</th><td>        }</td></tr>
<tr><th id="2806">2806</th><td>      }</td></tr>
<tr><th id="2807">2807</th><td>    } <b>else</b> {</td></tr>
<tr><th id="2808">2808</th><td>      CHECK_EQ(m-&gt;type_string(), csinfo_.conv2d_grad_filter);</td></tr>
<tr><th id="2809">2809</th><td>      <i>// Get 3rd input 'g' of Conv2DBackpropFilter.</i></td></tr>
<tr><th id="2810">2810</th><td>      Node* g = <b>nullptr</b>;</td></tr>
<tr><th id="2811">2811</th><td>      TF_CHECK_OK(m-&gt;input_node(<var>2</var>, &amp;g));</td></tr>
<tr><th id="2812">2812</th><td>      <i>// Now traverse all outgoing edges from g that have destination node as</i></td></tr>
<tr><th id="2813">2813</th><td><i>      // BiasAddGrad.</i></td></tr>
<tr><th id="2814">2814</th><td>      <b>for</b> (<em>const</em> Edge* e : g-&gt;out_edges()) {</td></tr>
<tr><th id="2815">2815</th><td>        <b>if</b> (!e-&gt;IsControlEdge() &amp;&amp;</td></tr>
<tr><th id="2816">2816</th><td>            e-&gt;dst()-&gt;type_string() == csinfo_.bias_add_grad &amp;&amp;</td></tr>
<tr><th id="2817">2817</th><td>            e-&gt;dst_input() == <var>0</var> <i>/* 1st input of BiasAddGrad */</i>) {</td></tr>
<tr><th id="2818">2818</th><td>          n = e-&gt;dst();</td></tr>
<tr><th id="2819">2819</th><td>          <b>break</b>;</td></tr>
<tr><th id="2820">2820</th><td>        }</td></tr>
<tr><th id="2821">2821</th><td>      }</td></tr>
<tr><th id="2822">2822</th><td>    }</td></tr>
<tr><th id="2823">2823</th><td></td></tr>
<tr><th id="2824">2824</th><td>    <b>if</b> (n == <b>nullptr</b>) {</td></tr>
<tr><th id="2825">2825</th><td>      VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: Could not find matching "</q></td></tr>
<tr><th id="2826">2826</th><td>              &lt;&lt; <q>"Conv2DBackpropFilter and BiasAddGrad node for merging. "</q></td></tr>
<tr><th id="2827">2827</th><td>              &lt;&lt; <q>"Input node: "</q> &lt;&lt; m-&gt;DebugString();</td></tr>
<tr><th id="2828">2828</th><td>    }</td></tr>
<tr><th id="2829">2829</th><td>    <b>return</b> n;</td></tr>
<tr><th id="2830">2830</th><td>  }</td></tr>
<tr><th id="2831">2831</th><td></td></tr>
<tr><th id="2832">2832</th><td>  <i>// Check if the node 'n' has any applicable rewrite rule</i></td></tr>
<tr><th id="2833">2833</th><td><i>  // We check for 2 scenarios for rewrite.</i></td></tr>
<tr><th id="2834">2834</th><td><i>  //</i></td></tr>
<tr><th id="2835">2835</th><td><i>  // @return RewriteInfo* for the applicable rewrite rule</i></td></tr>
<tr><th id="2836">2836</th><td>  <em>const</em> RewriteInfo* CheckForNodeRewrite(<em>const</em> Node* n) <em>const</em>;</td></tr>
<tr><th id="2837">2837</th><td></td></tr>
<tr><th id="2838">2838</th><td>  <i>// Default rewrite rule to be used in scenario 1 for rewrite.</i></td></tr>
<tr><th id="2839">2839</th><td><i>  // @return - true (since we want to always rewrite)</i></td></tr>
<tr><th id="2840">2840</th><td>  <em>static</em> <em>bool</em> AlwaysRewrite(<em>const</em> Node* n) { <b>return</b> <b>true</b>; }</td></tr>
<tr><th id="2841">2841</th><td></td></tr>
<tr><th id="2842">2842</th><td>  <i>// Check if we are performing pooling on depth or batch. If it is, then we</i></td></tr>
<tr><th id="2843">2843</th><td><i>  // do not rewrite MaxPool node to Mkl version.</i></td></tr>
<tr><th id="2844">2844</th><td><i>  // @return - true (if it is not a depth/batch wise pooling case);</i></td></tr>
<tr><th id="2845">2845</th><td><i>  //           false otherwise.</i></td></tr>
<tr><th id="2846">2846</th><td>  <em>static</em> <em>bool</em> NonDepthBatchWisePoolRewrite(<em>const</em> Node* n) {</td></tr>
<tr><th id="2847">2847</th><td>    CHECK_NOTNULL(n);</td></tr>
<tr><th id="2848">2848</th><td></td></tr>
<tr><th id="2849">2849</th><td>    string data_format_str;</td></tr>
<tr><th id="2850">2850</th><td>    TensorFormat data_format;</td></tr>
<tr><th id="2851">2851</th><td>    std::vector&lt;int32&gt; ksize, strides;</td></tr>
<tr><th id="2852">2852</th><td>    CHECK_EQ(GetNodeAttr(n-&gt;def(), <q>"ksize"</q>, &amp;ksize).ok(), <b>true</b>);</td></tr>
<tr><th id="2853">2853</th><td>    CHECK_EQ(GetNodeAttr(n-&gt;def(), <q>"strides"</q>, &amp;strides).ok(), <b>true</b>);</td></tr>
<tr><th id="2854">2854</th><td>    CHECK_EQ(GetNodeAttr(n-&gt;def(), <q>"data_format"</q>, &amp;data_format_str).ok(), <b>true</b>);</td></tr>
<tr><th id="2855">2855</th><td>    CHECK_EQ(FormatFromString(data_format_str, &amp;data_format), <b>true</b>);</td></tr>
<tr><th id="2856">2856</th><td></td></tr>
<tr><th id="2857">2857</th><td>    <i>// Condition that specifies non-batch-wise and non-depth-wise pooling.</i></td></tr>
<tr><th id="2858">2858</th><td>    <b>if</b> (GetTensorDim(ksize, data_format, <kbd>'N'</kbd>) == <var>1</var> &amp;&amp;</td></tr>
<tr><th id="2859">2859</th><td>        GetTensorDim(strides, data_format, <kbd>'N'</kbd>) == <var>1</var> &amp;&amp;</td></tr>
<tr><th id="2860">2860</th><td>        GetTensorDim(ksize, data_format, <kbd>'C'</kbd>) == <var>1</var> &amp;&amp;</td></tr>
<tr><th id="2861">2861</th><td>        GetTensorDim(strides, data_format, <kbd>'C'</kbd>) == <var>1</var>) {</td></tr>
<tr><th id="2862">2862</th><td>      <b>return</b> <b>true</b>;</td></tr>
<tr><th id="2863">2863</th><td>    }</td></tr>
<tr><th id="2864">2864</th><td></td></tr>
<tr><th id="2865">2865</th><td>    <b>return</b> <b>false</b>;</td></tr>
<tr><th id="2866">2866</th><td>  }</td></tr>
<tr><th id="2867">2867</th><td></td></tr>
<tr><th id="2868">2868</th><td>  <i>// If the depth_radius of LRN is not 2, then MKL DNN takes unoptimized </i></td></tr>
<tr><th id="2869">2869</th><td><i>  // path. The unoptimized path is slow. Thus we dont rewrite the node </i></td></tr>
<tr><th id="2870">2870</th><td><i>  // and use default Eigen. But for depth_radius=2, MKL DNN optimized </i></td></tr>
<tr><th id="2871">2871</th><td><i>  // path is taken, i.e., eigen node is rewritten by MKl DNN node.</i></td></tr>
<tr><th id="2872">2872</th><td>  <em>static</em> <em>bool</em> LrnRewrite(<em>const</em> Node* n) {</td></tr>
<tr><th id="2873">2873</th><td>    CHECK_NOTNULL(n);</td></tr>
<tr><th id="2874">2874</th><td></td></tr>
<tr><th id="2875">2875</th><td>    <em>int</em> depth_radius;</td></tr>
<tr><th id="2876">2876</th><td>    CHECK_EQ(GetNodeAttr(n-&gt;def(), <q>"depth_radius"</q>, &amp;depth_radius).ok(), <b>true</b>);</td></tr>
<tr><th id="2877">2877</th><td></td></tr>
<tr><th id="2878">2878</th><td>    <i>// if the depth_radius of LRN is not 2, don't rewrite the node by MKL DNN</i></td></tr>
<tr><th id="2879">2879</th><td><i>    // and use eigen node instead </i></td></tr>
<tr><th id="2880">2880</th><td>    <b>if</b> (depth_radius == <var>2</var>) {</td></tr>
<tr><th id="2881">2881</th><td>      <b>return</b> <b>true</b>;</td></tr>
<tr><th id="2882">2882</th><td>    }</td></tr>
<tr><th id="2883">2883</th><td>    VLOG(<var>1</var>) &lt;&lt; <q>"LrnRewrite: The model sets depth_radius as not 2 which"</q></td></tr>
<tr><th id="2884">2884</th><td>            &lt;&lt; <q>"case is not optimized by Intel MKL, thus using Eigen op"</q></td></tr>
<tr><th id="2885">2885</th><td>            &lt;&lt; <q>"for LRN "</q> ; </td></tr>
<tr><th id="2886">2886</th><td></td></tr>
<tr><th id="2887">2887</th><td>    <b>return</b> <b>false</b>;</td></tr>
<tr><th id="2888">2888</th><td>  }</td></tr>
<tr><th id="2889">2889</th><td></td></tr>
<tr><th id="2890">2890</th><td>  <em>static</em> <em>bool</em> AddNRewrite(<em>const</em> Node* n) {</td></tr>
<tr><th id="2891">2891</th><td>    CHECK_NOTNULL(n);</td></tr>
<tr><th id="2892">2892</th><td></td></tr>
<tr><th id="2893">2893</th><td>    <em>int</em> num;</td></tr>
<tr><th id="2894">2894</th><td>    CHECK_EQ(GetNodeAttr(n-&gt;def(), <q>"N"</q>, &amp;num).ok(), <b>true</b>);</td></tr>
<tr><th id="2895">2895</th><td></td></tr>
<tr><th id="2896">2896</th><td>    <i>// Condition that specifies non-batch-wise and non-depth-wise pooling.</i></td></tr>
<tr><th id="2897">2897</th><td>    <b>if</b> (num == <var>2</var>) {</td></tr>
<tr><th id="2898">2898</th><td>      <b>return</b> <b>true</b>;</td></tr>
<tr><th id="2899">2899</th><td>    }</td></tr>
<tr><th id="2900">2900</th><td></td></tr>
<tr><th id="2901">2901</th><td>    <b>return</b> <b>false</b>;</td></tr>
<tr><th id="2902">2902</th><td>  }</td></tr>
<tr><th id="2903">2903</th><td></td></tr>
<tr><th id="2904">2904</th><td>  <i>// Rewrites input node to a new node specified by its matching rewrite info.</i></td></tr>
<tr><th id="2905">2905</th><td><i>  //</i></td></tr>
<tr><th id="2906">2906</th><td><i>  // Method first searches matching rewrite info for input node and then</i></td></tr>
<tr><th id="2907">2907</th><td><i>  // uses that info to rewrite.</i></td></tr>
<tr><th id="2908">2908</th><td><i>  //</i></td></tr>
<tr><th id="2909">2909</th><td><i>  // Input node may be deleted in case of rewrite. Attempt to use the node</i></td></tr>
<tr><th id="2910">2910</th><td><i>  // after the call can result in undefined behaviors.</i></td></tr>
<tr><th id="2911">2911</th><td><i>  //</i></td></tr>
<tr><th id="2912">2912</th><td><i>  // @input  g - input graph, n - Node to be rewritten,</i></td></tr>
<tr><th id="2913">2913</th><td><i>  //         ri - matching rewriteinfo</i></td></tr>
<tr><th id="2914">2914</th><td><i>  // @return Status::OK(), if the input node is rewritten;</i></td></tr>
<tr><th id="2915">2915</th><td><i>  //         Returns appropriate Status error code otherwise.</i></td></tr>
<tr><th id="2916">2916</th><td><i>  //         Graph is updated in case the input node is rewritten.</i></td></tr>
<tr><th id="2917">2917</th><td><i>  //         Otherwise, it is not updated.</i></td></tr>
<tr><th id="2918">2918</th><td>  Status RewriteNode(std::unique_ptr&lt;Graph&gt;* g, Node* n, <em>const</em> RewriteInfo* ri);</td></tr>
<tr><th id="2919">2919</th><td></td></tr>
<tr><th id="2920">2920</th><td>  <i>// Get nodes that will feed a list of TF tensors to the new</i></td></tr>
<tr><th id="2921">2921</th><td><i>  // node that we are constructing.</i></td></tr>
<tr><th id="2922">2922</th><td><i>  //</i></td></tr>
<tr><th id="2923">2923</th><td><i>  // @input g - input graph,</i></td></tr>
<tr><th id="2924">2924</th><td><i>  // @input inputs - inputs to old node that we are using for constructing</i></td></tr>
<tr><th id="2925">2925</th><td><i>  //                 new inputs,</i></td></tr>
<tr><th id="2926">2926</th><td><i>  // @input input_idx - the index in the 'inputs' vector pointing to the</i></td></tr>
<tr><th id="2927">2927</th><td><i>  //                    current input that we have processed so far</i></td></tr>
<tr><th id="2928">2928</th><td><i>  // @output input_idx - index will be incremented by the number of nodes</i></td></tr>
<tr><th id="2929">2929</th><td><i>  //                     from 'inputs' that are processed</i></td></tr>
<tr><th id="2930">2930</th><td><i>  // @input list_length - The expected length of list of TF tensors</i></td></tr>
<tr><th id="2931">2931</th><td><i>  // @output output_nodes - the list of new nodes creating TF tensors</i></td></tr>
<tr><th id="2932">2932</th><td><i>  //</i></td></tr>
<tr><th id="2933">2933</th><td><i>  // @return None</i></td></tr>
<tr><th id="2934">2934</th><td>  <em>void</em> GetNodesProducingTFTensorList(</td></tr>
<tr><th id="2935">2935</th><td>      <em>const</em> gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt;&amp; inputs,</td></tr>
<tr><th id="2936">2936</th><td>      <em>int</em>* input_idx, <em>int</em> list_length,</td></tr>
<tr><th id="2937">2937</th><td>      std::vector&lt;NodeBuilder::NodeOut&gt;* output_nodes);</td></tr>
<tr><th id="2938">2938</th><td></td></tr>
<tr><th id="2939">2939</th><td>  <i>// Get nodes that will feed a list of Mkl tensors to the new</i></td></tr>
<tr><th id="2940">2940</th><td><i>  // node that we are constructing.</i></td></tr>
<tr><th id="2941">2941</th><td><i>  //</i></td></tr>
<tr><th id="2942">2942</th><td><i>  // @input g - input graph,</i></td></tr>
<tr><th id="2943">2943</th><td><i>  // @input orig_node - Original node that we are rewriting</i></td></tr>
<tr><th id="2944">2944</th><td><i>  // @input inputs - inputs to old node that we are using for constructing</i></td></tr>
<tr><th id="2945">2945</th><td><i>  //                 new inputs,</i></td></tr>
<tr><th id="2946">2946</th><td><i>  // @input input_idx - the index in the 'inputs' vector pointing to the</i></td></tr>
<tr><th id="2947">2947</th><td><i>  //                    current input that we have processed so far</i></td></tr>
<tr><th id="2948">2948</th><td><i>  // @output input_idx - index will be incremented by the number of nodes</i></td></tr>
<tr><th id="2949">2949</th><td><i>  //                     from 'inputs' that are processed</i></td></tr>
<tr><th id="2950">2950</th><td><i>  // @input list_length - The expected length of list of Mkl tensors</i></td></tr>
<tr><th id="2951">2951</th><td><i>  // @output output_nodes - the list of new nodes creating Mkl tensors</i></td></tr>
<tr><th id="2952">2952</th><td><i>  //</i></td></tr>
<tr><th id="2953">2953</th><td><i>  // @return None</i></td></tr>
<tr><th id="2954">2954</th><td>  <em>void</em> GetNodesProducingMklTensorList(</td></tr>
<tr><th id="2955">2955</th><td>      std::unique_ptr&lt;Graph&gt;* g, Node* orig_node,</td></tr>
<tr><th id="2956">2956</th><td>      <em>const</em> gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt;&amp; inputs,</td></tr>
<tr><th id="2957">2957</th><td>      <em>int</em>* input_idx, <em>int</em> list_length,</td></tr>
<tr><th id="2958">2958</th><td>      std::vector&lt;NodeBuilder::NodeOut&gt;* output_nodes);</td></tr>
<tr><th id="2959">2959</th><td></td></tr>
<tr><th id="2960">2960</th><td>  <i>// Get a node that will feed an Mkl tensor to the new</i></td></tr>
<tr><th id="2961">2961</th><td><i>  // node that we are constructing. The output node could be (1) 'n'</i></td></tr>
<tr><th id="2962">2962</th><td><i>  // if it is Mkl layer, or (2) a dummy node producing dummy Mkl tensor</i></td></tr>
<tr><th id="2963">2963</th><td><i>  // if 'n' is not an Mkl layer.</i></td></tr>
<tr><th id="2964">2964</th><td><i>  //</i></td></tr>
<tr><th id="2965">2965</th><td><i>  // @input g - input graph,</i></td></tr>
<tr><th id="2966">2966</th><td><i>  // @input orig_node - Original node that we are rewriting,</i></td></tr>
<tr><th id="2967">2967</th><td><i>  // @input n - Node based on which we are creating Mkl node,</i></td></tr>
<tr><th id="2968">2968</th><td><i>  // @input n_output_slot - the output slot of node 'n'</i></td></tr>
<tr><th id="2969">2969</th><td><i>  //            which is feeding to the node that we are constructing</i></td></tr>
<tr><th id="2970">2970</th><td><i>  // @output mkl_node - the new node that will feed Mkl tensor</i></td></tr>
<tr><th id="2971">2971</th><td><i>  // @output mkl_node_output_slot - the slot number of mkl_node that</i></td></tr>
<tr><th id="2972">2972</th><td><i>  //                                will feed the tensor</i></td></tr>
<tr><th id="2973">2973</th><td><i>  // @return None</i></td></tr>
<tr><th id="2974">2974</th><td>  <em>void</em> GetNodeProducingMklTensor(std::unique_ptr&lt;Graph&gt;* g, Node* orig_node,</td></tr>
<tr><th id="2975">2975</th><td>                                 Node* n, <em>int</em> n_output_slot, Node** mkl_node,</td></tr>
<tr><th id="2976">2976</th><td>                                 <em>int</em>* mkl_node_output_slot);</td></tr>
<tr><th id="2977">2977</th><td></td></tr>
<tr><th id="2978">2978</th><td>  <i>// Setup new inputs using old inputs 'inputs' for the rewritten node in 'nb'</i></td></tr>
<tr><th id="2979">2979</th><td><i>  // in graph 'g'. Original node is input in 'old_node'. Inputs to 'nb' are</i></td></tr>
<tr><th id="2980">2980</th><td><i>  // set up in contiguous fashion. 'workspace_tensors' carry graph nodes</i></td></tr>
<tr><th id="2981">2981</th><td><i>  // producing workspace edges if 'are_workspace_tensors_available' is true.</i></td></tr>
<tr><th id="2982">2982</th><td><i>  // Otherwise, 'workspace_tensors' is empty vector.</i></td></tr>
<tr><th id="2983">2983</th><td><i>  //</i></td></tr>
<tr><th id="2984">2984</th><td><i>  // For details, refer to 'Ordering of inputs after rewriting' section in the</i></td></tr>
<tr><th id="2985">2985</th><td><i>  // documentation above.</i></td></tr>
<tr><th id="2986">2986</th><td><i>  //</i></td></tr>
<tr><th id="2987">2987</th><td><i>  // Returns Status::OK() if setting up inputs is successful, otherwise</i></td></tr>
<tr><th id="2988">2988</th><td><i>  // returns appropriate status code.</i></td></tr>
<tr><th id="2989">2989</th><td>  <em>int</em> SetUpContiguousInputs(</td></tr>
<tr><th id="2990">2990</th><td>      std::unique_ptr&lt;Graph&gt;* g,</td></tr>
<tr><th id="2991">2991</th><td>      <em>const</em> gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt;&amp; old_node_inputs,</td></tr>
<tr><th id="2992">2992</th><td>      NodeBuilder* nb, Node* old_node,</td></tr>
<tr><th id="2993">2993</th><td>      std::vector&lt;NodeBuilder::NodeOut&gt;* workspace_tensors,</td></tr>
<tr><th id="2994">2994</th><td>      <em>bool</em> are_workspace_tensors_available);</td></tr>
<tr><th id="2995">2995</th><td></td></tr>
<tr><th id="2996">2996</th><td>  <i>// Setup new inputs using old inputs 'inputs' for the rewritten node in 'nb'</i></td></tr>
<tr><th id="2997">2997</th><td><i>  // in graph 'g'. Original node is input in 'orig_node'.</i></td></tr>
<tr><th id="2998">2998</th><td><i>  //</i></td></tr>
<tr><th id="2999">2999</th><td><i>  // For details, refer to 'Ordering of Tensorflow tensors and Mkl tensors'</i></td></tr>
<tr><th id="3000">3000</th><td><i>  // section in the documentation above.</i></td></tr>
<tr><th id="3001">3001</th><td><i>  //</i></td></tr>
<tr><th id="3002">3002</th><td><i>  // Returns Status::OK() if setting up inputs is successful, otherwise</i></td></tr>
<tr><th id="3003">3003</th><td><i>  // returns appropriate status code.</i></td></tr>
<tr><th id="3004">3004</th><td>  Status SetUpInputs(std::unique_ptr&lt;Graph&gt;* g,</td></tr>
<tr><th id="3005">3005</th><td>                     <em>const</em> gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt;&amp; inputs,</td></tr>
<tr><th id="3006">3006</th><td>                     NodeBuilder* nb, Node* orig_node);</td></tr>
<tr><th id="3007">3007</th><td></td></tr>
<tr><th id="3008">3008</th><td>  <i>// Add workspace edge on the input or output side of Node 'orig_node' by using</i></td></tr>
<tr><th id="3009">3009</th><td><i>  // NodeBuilder 'nb' for the new node provided. If 'orig_node' does not dictate</i></td></tr>
<tr><th id="3010">3010</th><td><i>  // adding workspace edge then do not add it. Workspace Tensorflow and Mkl</i></td></tr>
<tr><th id="3011">3011</th><td><i>  // tensors, if they need to be added, will be set into these tensors.</i></td></tr>
<tr><th id="3012">3012</th><td><i>  // If we set workspace tensors, then are_ws_tensors_added should be true.</i></td></tr>
<tr><th id="3013">3013</th><td>  <em>void</em> AddWorkSpaceEdgeIfNeeded(std::unique_ptr&lt;Graph&gt;* g, Node* orig_node,</td></tr>
<tr><th id="3014">3014</th><td>                                NodeBuilder* nb,</td></tr>
<tr><th id="3015">3015</th><td>                                std::vector&lt;NodeBuilder::NodeOut&gt;* ws_tensors,</td></tr>
<tr><th id="3016">3016</th><td>                                <em>bool</em>* are_ws_tensors_added);</td></tr>
<tr><th id="3017">3017</th><td></td></tr>
<tr><th id="3018">3018</th><td>  <i>// Functions specific to operators to copy attributes</i></td></tr>
<tr><th id="3019">3019</th><td><i>  // We need operator-specific function to copy attributes because the framework</i></td></tr>
<tr><th id="3020">3020</th><td><i>  // does not provide any generic function for it.</i></td></tr>
<tr><th id="3021">3021</th><td><i>  // NOTE: names are alphabetically sorted.</i></td></tr>
<tr><th id="3022">3022</th><td>  <em>static</em> <em>void</em> CopyAttrsAddN(<em>const</em> Node* orig_node, NodeBuilder* nb);</td></tr>
<tr><th id="3023">3023</th><td>  <em>static</em> <em>void</em> CopyAttrsBiasAddGrad(<em>const</em> Node* orig_node, NodeBuilder* nb);</td></tr>
<tr><th id="3024">3024</th><td>  <em>static</em> <em>void</em> CopyAttrsConcat(<em>const</em> Node* orig_node, NodeBuilder* nb);</td></tr>
<tr><th id="3025">3025</th><td>  <em>static</em> <em>void</em> CopyAttrsConcatV2(<em>const</em> Node* orig_node, NodeBuilder* nb);</td></tr>
<tr><th id="3026">3026</th><td>  <em>static</em> <em>void</em> CopyAttrsConv2D(<em>const</em> Node* orig_node, NodeBuilder* nb);</td></tr>
<tr><th id="3027">3027</th><td>  <em>static</em> <em>void</em> CopyAttrsDataType(<em>const</em> Node* orig_node, NodeBuilder* nb);</td></tr>
<tr><th id="3028">3028</th><td>  <em>static</em> <em>void</em> CopyAttrsFusedBatchNorm(<em>const</em> Node* orig_node, NodeBuilder* nb);</td></tr>
<tr><th id="3029">3029</th><td>  <em>static</em> <em>void</em> CopyAttrsLRN(<em>const</em> Node* orig_node, NodeBuilder* nb);</td></tr>
<tr><th id="3030">3030</th><td>  <em>static</em> <em>void</em> CopyAttrsPooling(<em>const</em> Node* orig_node, NodeBuilder* nb);</td></tr>
<tr><th id="3031">3031</th><td>  <em>static</em> <em>void</em> CopyAttrsReshape(<em>const</em> Node* orig_node, NodeBuilder* nb);</td></tr>
<tr><th id="3032">3032</th><td>  <em>static</em> <em>void</em> CopyAttrsSplit(<em>const</em> Node* orig_node, NodeBuilder* nb);</td></tr>
<tr><th id="3033">3033</th><td></td></tr>
<tr><th id="3034">3034</th><td>  <i>// Generate a graph node in graph 'g' representing a dummy Mkl tensor node,</i></td></tr>
<tr><th id="3035">3035</th><td><i>  // using node for original node 'orig_node' and return it in '*out'.</i></td></tr>
<tr><th id="3036">3036</th><td><i>  // TODO(nhasabni) We should move this to mkl_util.h</i></td></tr>
<tr><th id="3037">3037</th><td>  <em>void</em> GetDummyMklTensorNode(std::unique_ptr&lt;Graph&gt;* g, Node** out,</td></tr>
<tr><th id="3038">3038</th><td>                             Node* orig_node);</td></tr>
<tr><th id="3039">3039</th><td>  <em>void</em> GetDummyWorkspaceTensorNode(std::unique_ptr&lt;Graph&gt;* g, Node** out,</td></tr>
<tr><th id="3040">3040</th><td>                                   Node* orig_node);</td></tr>
<tr><th id="3041">3041</th><td>};</td></tr>
<tr><th id="3042">3042</th><td></td></tr>
<tr><th id="3043">3043</th><td>MklLayoutRewritePass::ConstStringsInfo MklLayoutRewritePass::csinfo_;</td></tr>
<tr><th id="3044">3044</th><td></td></tr>
<tr><th id="3045">3045</th><td><i>// We register Mkl rewrite pass for phase 1 in post partitioning group.</i></td></tr>
<tr><th id="3046">3046</th><td><i>// We register it here so that we get a complete picture of all users of Mkl</i></td></tr>
<tr><th id="3047">3047</th><td><i>// nodes. Do not change the ordering of the Mkl passes.</i></td></tr>
<tr><th id="3048">3048</th><td><em>const</em> OptimizationPassRegistry::Grouping kMklLayoutRewritePassGroup =</td></tr>
<tr><th id="3049">3049</th><td>    OptimizationPassRegistry::POST_PARTITIONING;</td></tr>
<tr><th id="3050">3050</th><td>REGISTER_OPTIMIZATION(kMklLayoutRewritePassGroup, <var>1</var>, MklLayoutRewritePass);</td></tr>
<tr><th id="3051">3051</th><td></td></tr>
<tr><th id="3052">3052</th><td><i>//////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="3053">3053</th><td><i>//           Helper functions for creating new node</i></td></tr>
<tr><th id="3054">3054</th><td><i>//////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="3055">3055</th><td></td></tr>
<tr><th id="3056">3056</th><td><em>static</em> <em>void</em> FillInputs(<em>const</em> Node* n,</td></tr>
<tr><th id="3057">3057</th><td>                       gtl::InlinedVector&lt;Node*, <var>4</var>&gt;* control_edges,</td></tr>
<tr><th id="3058">3058</th><td>                       gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt;* in) {</td></tr>
<tr><th id="3059">3059</th><td>  control_edges-&gt;clear();</td></tr>
<tr><th id="3060">3060</th><td>  <b>for</b> (<em>const</em> Edge* e : n-&gt;in_edges()) {</td></tr>
<tr><th id="3061">3061</th><td>    <b>if</b> (e-&gt;IsControlEdge()) {</td></tr>
<tr><th id="3062">3062</th><td>      control_edges-&gt;push_back(e-&gt;src());</td></tr>
<tr><th id="3063">3063</th><td>    } <b>else</b> {</td></tr>
<tr><th id="3064">3064</th><td>      (*in)[e-&gt;dst_input()] = std::make_pair(e-&gt;src(), e-&gt;src_output());</td></tr>
<tr><th id="3065">3065</th><td>    }</td></tr>
<tr><th id="3066">3066</th><td>  }</td></tr>
<tr><th id="3067">3067</th><td>  std::sort(control_edges-&gt;begin(), control_edges-&gt;end());</td></tr>
<tr><th id="3068">3068</th><td>  <b>if</b> (n-&gt;op_def().is_commutative()) {</td></tr>
<tr><th id="3069">3069</th><td>    <i>// For commutative inputs, we sort the input by the input Node*</i></td></tr>
<tr><th id="3070">3070</th><td><i>    // to get a canonical ordering (so that add(a,b) and add(b, a) will</i></td></tr>
<tr><th id="3071">3071</th><td><i>    // hash to the same value if is_commutative is true for 'add').</i></td></tr>
<tr><th id="3072">3072</th><td>    std::sort(in-&gt;begin(), in-&gt;end());</td></tr>
<tr><th id="3073">3073</th><td>  }</td></tr>
<tr><th id="3074">3074</th><td>}</td></tr>
<tr><th id="3075">3075</th><td></td></tr>
<tr><th id="3076">3076</th><td><em>void</em> MklLayoutRewritePass::GetNodesProducingTFTensorList(</td></tr>
<tr><th id="3077">3077</th><td>    <em>const</em> gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt;&amp; inputs, <em>int</em>* input_idx,</td></tr>
<tr><th id="3078">3078</th><td>    <em>int</em> list_length, std::vector&lt;NodeBuilder::NodeOut&gt;* output_nodes) {</td></tr>
<tr><th id="3079">3079</th><td>  CHECK_LT(*input_idx, inputs.size());</td></tr>
<tr><th id="3080">3080</th><td>  CHECK_GT(list_length, <var>0</var>);</td></tr>
<tr><th id="3081">3081</th><td>  CHECK_NOTNULL(output_nodes);</td></tr>
<tr><th id="3082">3082</th><td>  output_nodes-&gt;reserve(list_length);</td></tr>
<tr><th id="3083">3083</th><td></td></tr>
<tr><th id="3084">3084</th><td>  <b>while</b> (list_length != <var>0</var>) {</td></tr>
<tr><th id="3085">3085</th><td>    CHECK_GT(list_length, <var>0</var>);</td></tr>
<tr><th id="3086">3086</th><td>    CHECK_LT(*input_idx, inputs.size());</td></tr>
<tr><th id="3087">3087</th><td>    Node* n = inputs[*input_idx].first;</td></tr>
<tr><th id="3088">3088</th><td>    <em>int</em> slot = inputs[*input_idx].second;</td></tr>
<tr><th id="3089">3089</th><td>    <i>// If input node 'n' is just producing a single tensor at</i></td></tr>
<tr><th id="3090">3090</th><td><i>    // output slot 'slot' then we just add that single node.</i></td></tr>
<tr><th id="3091">3091</th><td>    output_nodes-&gt;push_back(NodeBuilder::NodeOut(n, slot));</td></tr>
<tr><th id="3092">3092</th><td>    (*input_idx)++;</td></tr>
<tr><th id="3093">3093</th><td>    list_length--;</td></tr>
<tr><th id="3094">3094</th><td>  }</td></tr>
<tr><th id="3095">3095</th><td>}</td></tr>
<tr><th id="3096">3096</th><td></td></tr>
<tr><th id="3097">3097</th><td><i>// TODO(nhasabni) We should move this to mkl_util.h.</i></td></tr>
<tr><th id="3098">3098</th><td><em>void</em> MklLayoutRewritePass::GetDummyMklTensorNode(std::unique_ptr&lt;Graph&gt;* g,</td></tr>
<tr><th id="3099">3099</th><td>                                                 Node** out, Node* orig_node) {</td></tr>
<tr><th id="3100">3100</th><td>  <i>// We use a tensor of shape {8} and value 0,0,0,0,0,0,0,0 to represent</i></td></tr>
<tr><th id="3101">3101</th><td><i>  // dummy Mkl tensor. 8 = 2*size_t.</i></td></tr>
<tr><th id="3102">3102</th><td>  <em>const</em> DataType dt = DataTypeToEnum&lt;uint8&gt;::v();</td></tr>
<tr><th id="3103">3103</th><td>  TensorProto proto;</td></tr>
<tr><th id="3104">3104</th><td>  proto.set_dtype(dt);</td></tr>
<tr><th id="3105">3105</th><td>  uint8 zero[<var>8</var>] = {<var>0</var>, <var>0</var>, <var>0</var>, <var>0</var>, <var>0</var>, <var>0</var>, <var>0</var>, <var>0</var>};</td></tr>
<tr><th id="3106">3106</th><td>  proto.set_tensor_content(string(<b>reinterpret_cast</b>&lt;<em>char</em>*&gt;(&amp;zero), <var>8</var>));</td></tr>
<tr><th id="3107">3107</th><td>  TensorShape dummy_shape({<var>8</var>});</td></tr>
<tr><th id="3108">3108</th><td>  dummy_shape.AsProto(proto.mutable_tensor_shape());</td></tr>
<tr><th id="3109">3109</th><td>  TF_CHECK_OK(NodeBuilder((*g)-&gt;NewName(<q>"DMT"</q>), <q>"Const"</q>)</td></tr>
<tr><th id="3110">3110</th><td>                  .Attr(<q>"value"</q>, proto)</td></tr>
<tr><th id="3111">3111</th><td>                  .Attr(<q>"dtype"</q>, dt)</td></tr>
<tr><th id="3112">3112</th><td>                  .Device(orig_node-&gt;def().device())  <i>// We place this node on</i></td></tr>
<tr><th id="3113">3113</th><td>                                                      <i>// the same device as the</i></td></tr>
<tr><th id="3114">3114</th><td><i>                                                      // device of the original</i></td></tr>
<tr><th id="3115">3115</th><td><i>                                                      // node.</i></td></tr>
<tr><th id="3116">3116</th><td>                  .Finalize(&amp;**g, out));</td></tr>
<tr><th id="3117">3117</th><td></td></tr>
<tr><th id="3118">3118</th><td>  <i>// If number of inputs to the original node is &gt; 0, then we add</i></td></tr>
<tr><th id="3119">3119</th><td><i>  // control dependency between 1st input (index 0) of the original node and</i></td></tr>
<tr><th id="3120">3120</th><td><i>  // the dummy Mkl node. This is needed because control-flow ops such as Enter,</i></td></tr>
<tr><th id="3121">3121</th><td><i>  // Merge, etc, require frame_name of the dummy Mkl node to be same as the</i></td></tr>
<tr><th id="3122">3122</th><td><i>  // rewritten node. Adding control edge between 1st input of the original node</i></td></tr>
<tr><th id="3123">3123</th><td><i>  // and the dummy Mkl node ensures that the dummy node is in the same frame</i></td></tr>
<tr><th id="3124">3124</th><td><i>  // as the original node. Choosing 1st input is not necessary - any input of</i></td></tr>
<tr><th id="3125">3125</th><td><i>  // the original node is fine because all the inputs of a node are always in</i></td></tr>
<tr><th id="3126">3126</th><td><i>  // the same frame.</i></td></tr>
<tr><th id="3127">3127</th><td>  <b>if</b> (orig_node-&gt;num_inputs() &gt; <var>0</var>) {</td></tr>
<tr><th id="3128">3128</th><td>    Node* orig_input0 = <b>nullptr</b>;</td></tr>
<tr><th id="3129">3129</th><td>    TF_CHECK_OK(</td></tr>
<tr><th id="3130">3130</th><td>        orig_node-&gt;input_node(<var>0</var>, <b>const_cast</b>&lt;<em>const</em> Node**&gt;(&amp;orig_input0)));</td></tr>
<tr><th id="3131">3131</th><td>    <i>// Allow duplicate while adding control edge as it would fail (return</i></td></tr>
<tr><th id="3132">3132</th><td><i>    // NULL) if we try to add duplicate edge.</i></td></tr>
<tr><th id="3133">3133</th><td>    CHECK_NOTNULL((*g)-&gt;AddControlEdge(orig_input0, *out, <b>true</b>));</td></tr>
<tr><th id="3134">3134</th><td>  }</td></tr>
<tr><th id="3135">3135</th><td></td></tr>
<tr><th id="3136">3136</th><td>  (*out)-&gt;set_assigned_device_name(orig_node-&gt;assigned_device_name());</td></tr>
<tr><th id="3137">3137</th><td>}</td></tr>
<tr><th id="3138">3138</th><td></td></tr>
<tr><th id="3139">3139</th><td><em>void</em> MklLayoutRewritePass::GetNodesProducingMklTensorList(</td></tr>
<tr><th id="3140">3140</th><td>    std::unique_ptr&lt;Graph&gt;* g, Node* orig_node,</td></tr>
<tr><th id="3141">3141</th><td>    <em>const</em> gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt;&amp; inputs, <em>int</em>* input_idx,</td></tr>
<tr><th id="3142">3142</th><td>    <em>int</em> list_length, std::vector&lt;NodeBuilder::NodeOut&gt;* output_nodes) {</td></tr>
<tr><th id="3143">3143</th><td>  CHECK_LT(*input_idx, inputs.size());</td></tr>
<tr><th id="3144">3144</th><td>  CHECK_GT(list_length, <var>0</var>);</td></tr>
<tr><th id="3145">3145</th><td>  CHECK_NOTNULL(output_nodes);</td></tr>
<tr><th id="3146">3146</th><td>  output_nodes-&gt;reserve(list_length);</td></tr>
<tr><th id="3147">3147</th><td></td></tr>
<tr><th id="3148">3148</th><td>  <b>while</b> (list_length != <var>0</var>) {</td></tr>
<tr><th id="3149">3149</th><td>    CHECK_GT(list_length, <var>0</var>);</td></tr>
<tr><th id="3150">3150</th><td>    CHECK_LT(*input_idx, inputs.size());</td></tr>
<tr><th id="3151">3151</th><td>    Node* n = inputs[*input_idx].first;</td></tr>
<tr><th id="3152">3152</th><td>    <em>int</em> slot = inputs[*input_idx].second;</td></tr>
<tr><th id="3153">3153</th><td>    <i>// If 'n' is producing a single tensor, then create a single Mkl tensor</i></td></tr>
<tr><th id="3154">3154</th><td><i>    // node.</i></td></tr>
<tr><th id="3155">3155</th><td>    Node* mkl_node = <b>nullptr</b>;</td></tr>
<tr><th id="3156">3156</th><td>    <em>int</em> mkl_node_output_slot = <var>0</var>;</td></tr>
<tr><th id="3157">3157</th><td>    GetNodeProducingMklTensor(g, orig_node, n, slot, &amp;mkl_node,</td></tr>
<tr><th id="3158">3158</th><td>                              &amp;mkl_node_output_slot);</td></tr>
<tr><th id="3159">3159</th><td>    output_nodes-&gt;push_back(</td></tr>
<tr><th id="3160">3160</th><td>        NodeBuilder::NodeOut(mkl_node, mkl_node_output_slot));</td></tr>
<tr><th id="3161">3161</th><td>    (*input_idx)++;</td></tr>
<tr><th id="3162">3162</th><td>    list_length--;</td></tr>
<tr><th id="3163">3163</th><td>  }</td></tr>
<tr><th id="3164">3164</th><td>}</td></tr>
<tr><th id="3165">3165</th><td></td></tr>
<tr><th id="3166">3166</th><td><i>// Get an input node that will feed Mkl tensor to the new</i></td></tr>
<tr><th id="3167">3167</th><td><i>// node that we are constructing. An input node could be (1) 'n'</i></td></tr>
<tr><th id="3168">3168</th><td><i>// if it is Mkl layer, or (2) a dummy node producing dummy Mkl tensor</i></td></tr>
<tr><th id="3169">3169</th><td><i>// if 'n' is not an Mkl layer.</i></td></tr>
<tr><th id="3170">3170</th><td><em>void</em> MklLayoutRewritePass::GetNodeProducingMklTensor(</td></tr>
<tr><th id="3171">3171</th><td>    std::unique_ptr&lt;Graph&gt;* g, Node* orig_node, Node* n, <em>int</em> n_output_slot,</td></tr>
<tr><th id="3172">3172</th><td>    Node** mkl_node, <em>int</em>* mkl_node_output_slot) {</td></tr>
<tr><th id="3173">3173</th><td>  CHECK_NOTNULL(n);</td></tr>
<tr><th id="3174">3174</th><td>  CHECK_NOTNULL(mkl_node);</td></tr>
<tr><th id="3175">3175</th><td>  CHECK_NOTNULL(mkl_node_output_slot);</td></tr>
<tr><th id="3176">3176</th><td></td></tr>
<tr><th id="3177">3177</th><td>  <i>// If this is an MKL op, then it will create extra output for MKL layout.</i></td></tr>
<tr><th id="3178">3178</th><td>  DataType T;</td></tr>
<tr><th id="3179">3179</th><td>  <b>if</b> (GetNodeAttr(n-&gt;def(), <q>"T"</q>, &amp;T).ok() &amp;&amp;</td></tr>
<tr><th id="3180">3180</th><td>      mkl_op_registry::IsMklOp(n-&gt;type_string(), T)) {</td></tr>
<tr><th id="3181">3181</th><td>    <i>// If this is an MKL op, then it will generate an edge that will receive</i></td></tr>
<tr><th id="3182">3182</th><td><i>    // Mkl tensor from a node.</i></td></tr>
<tr><th id="3183">3183</th><td><i>    // output slot number for Mkl tensor would be N+slot number of TensorFlow</i></td></tr>
<tr><th id="3184">3184</th><td><i>    // tensor, where N is total number of TensorFlow tensors.</i></td></tr>
<tr><th id="3185">3185</th><td>    *mkl_node = n;</td></tr>
<tr><th id="3186">3186</th><td>    *mkl_node_output_slot =</td></tr>
<tr><th id="3187">3187</th><td>        GetTensorMetaDataIndex(n_output_slot, n-&gt;num_outputs());</td></tr>
<tr><th id="3188">3188</th><td>  } <b>else</b> {</td></tr>
<tr><th id="3189">3189</th><td>    <i>// If we have not visited the node and rewritten it, then we need</i></td></tr>
<tr><th id="3190">3190</th><td><i>    // to create a dummy node that will feed a dummy Mkl tensor to this node.</i></td></tr>
<tr><th id="3191">3191</th><td><i>    // DummyMklTensor node has no input and generates only 1 output</i></td></tr>
<tr><th id="3192">3192</th><td><i>    // (dummy Mkl tensor) as output slot number 0.</i></td></tr>
<tr><th id="3193">3193</th><td>    GetDummyMklTensorNode(g, mkl_node, orig_node);</td></tr>
<tr><th id="3194">3194</th><td>    CHECK_NOTNULL(*mkl_node);</td></tr>
<tr><th id="3195">3195</th><td>    *mkl_node_output_slot = <var>0</var>;</td></tr>
<tr><th id="3196">3196</th><td>  }</td></tr>
<tr><th id="3197">3197</th><td>}</td></tr>
<tr><th id="3198">3198</th><td></td></tr>
<tr><th id="3199">3199</th><td><em>int</em> MklLayoutRewritePass::SetUpContiguousInputs(</td></tr>
<tr><th id="3200">3200</th><td>    std::unique_ptr&lt;Graph&gt;* g,</td></tr>
<tr><th id="3201">3201</th><td>    <em>const</em> gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt;&amp; old_node_inputs,</td></tr>
<tr><th id="3202">3202</th><td>    NodeBuilder* nb, Node* old_node,</td></tr>
<tr><th id="3203">3203</th><td>    std::vector&lt;NodeBuilder::NodeOut&gt;* workspace_tensors,</td></tr>
<tr><th id="3204">3204</th><td>    <em>bool</em> are_workspace_tensors_available) {</td></tr>
<tr><th id="3205">3205</th><td>  CHECK_NOTNULL(workspace_tensors);</td></tr>
<tr><th id="3206">3206</th><td>  CHECK_EQ(kTensorOrdering, MklTfTensorOrdering::TENSORS_CONTIGUOUS);</td></tr>
<tr><th id="3207">3207</th><td></td></tr>
<tr><th id="3208">3208</th><td>  <i>// TODO(nhasabni): Temporary solution to connect filter input of</i></td></tr>
<tr><th id="3209">3209</th><td><i>  // BackpropInput with the converted filter from Conv2D.</i></td></tr>
<tr><th id="3210">3210</th><td>  <em>bool</em> do_connect_conv2d_backprop_input_filter = <b>false</b>;</td></tr>
<tr><th id="3211">3211</th><td>  Node* conv2d_node = <b>nullptr</b>;</td></tr>
<tr><th id="3212">3212</th><td>  <i>// Filter node is 2nd input (slot index 1) of Conv2D.</i></td></tr>
<tr><th id="3213">3213</th><td>  <em>int</em> kConv2DFilterInputSlotIdx = <var>1</var>;</td></tr>
<tr><th id="3214">3214</th><td>  <em>int</em> kConv2DBackpropInputFilterInputSlotIdx = <var>1</var>;</td></tr>
<tr><th id="3215">3215</th><td>  <em>int</em> kConv2DFilterOutputSlotIdx = <var>1</var>;</td></tr>
<tr><th id="3216">3216</th><td>  <b>if</b> (old_node-&gt;type_string() == csinfo_.conv2d_grad_input) {</td></tr>
<tr><th id="3217">3217</th><td>    <i>// We need to find Conv2D node from Conv2DBackpropInput.</i></td></tr>
<tr><th id="3218">3218</th><td><i>    // For that let's first find filter node that is 2nd input (slot 1)</i></td></tr>
<tr><th id="3219">3219</th><td><i>    // of BackpropInput.</i></td></tr>
<tr><th id="3220">3220</th><td>    Node* filter_node = <b>nullptr</b>;</td></tr>
<tr><th id="3221">3221</th><td>    TF_CHECK_OK(old_node-&gt;input_node(kConv2DBackpropInputFilterInputSlotIdx,</td></tr>
<tr><th id="3222">3222</th><td>                                     &amp;filter_node));</td></tr>
<tr><th id="3223">3223</th><td>    CHECK_NOTNULL(filter_node);</td></tr>
<tr><th id="3224">3224</th><td></td></tr>
<tr><th id="3225">3225</th><td>    <i>// Now check which nodes receive from filter_node. Filter feeds as</i></td></tr>
<tr><th id="3226">3226</th><td><i>    // 2nd input (slot 1) of _MklConv2D and _MklConv2DWithBias.</i></td></tr>
<tr><th id="3227">3227</th><td>    <b>for</b> (<em>const</em> Edge* e : filter_node-&gt;out_edges()) {</td></tr>
<tr><th id="3228">3228</th><td>      <b>if</b> ((e-&gt;dst()-&gt;type_string() == csinfo_.mkl_conv2d ||</td></tr>
<tr><th id="3229">3229</th><td>           e-&gt;dst()-&gt;type_string() == csinfo_.mkl_conv2d_with_bias) &amp;&amp;</td></tr>
<tr><th id="3230">3230</th><td>          e-&gt;dst_input() == kConv2DFilterInputSlotIdx</td></tr>
<tr><th id="3231">3231</th><td>          <i>/* filter is 2nd input of Conv2D and _MklConv2D. */</i>) {</td></tr>
<tr><th id="3232">3232</th><td>        <b>if</b> (conv2d_node != <b>nullptr</b>) {</td></tr>
<tr><th id="3233">3233</th><td>          VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: unusual case of same filter"</q></td></tr>
<tr><th id="3234">3234</th><td>                  &lt;&lt; <q>" feeding multiple Conv2D nodes: "</q></td></tr>
<tr><th id="3235">3235</th><td>                  &lt;&lt; filter_node-&gt;DebugString();</td></tr>
<tr><th id="3236">3236</th><td>          <i>// We will not connect filter input of Conv2DBackpropInput</i></td></tr>
<tr><th id="3237">3237</th><td><i>          // to be safe here.</i></td></tr>
<tr><th id="3238">3238</th><td>          do_connect_conv2d_backprop_input_filter = <b>false</b>;</td></tr>
<tr><th id="3239">3239</th><td>          <b>break</b>;</td></tr>
<tr><th id="3240">3240</th><td>        } <b>else</b> {</td></tr>
<tr><th id="3241">3241</th><td>          conv2d_node = e-&gt;dst();</td></tr>
<tr><th id="3242">3242</th><td>          do_connect_conv2d_backprop_input_filter = <b>true</b>;</td></tr>
<tr><th id="3243">3243</th><td>        }</td></tr>
<tr><th id="3244">3244</th><td>      }</td></tr>
<tr><th id="3245">3245</th><td>    }</td></tr>
<tr><th id="3246">3246</th><td>  }</td></tr>
<tr><th id="3247">3247</th><td></td></tr>
<tr><th id="3248">3248</th><td>  <i>// Number of input slots to original op</i></td></tr>
<tr><th id="3249">3249</th><td><i>  // Input slots are represented by .Input() calls in REGISTER_OP.</i></td></tr>
<tr><th id="3250">3250</th><td>  <em>int</em> old_node_input_slots = old_node-&gt;op_def().input_arg_size();</td></tr>
<tr><th id="3251">3251</th><td>  <i>// Actual number of inputs can be greater than or equal to number</i></td></tr>
<tr><th id="3252">3252</th><td><i>  // of Input slots because inputs of type list could be unfolded.</i></td></tr>
<tr><th id="3253">3253</th><td>  CHECK_GE(old_node_inputs.size(), old_node_input_slots);</td></tr>
<tr><th id="3254">3254</th><td>  <em>int</em> nn_slot_idx = <var>0</var>;  <i>// slot index for inputs of new node</i></td></tr>
<tr><th id="3255">3255</th><td></td></tr>
<tr><th id="3256">3256</th><td>  <i>// Let's copy all inputs (TF tensors) of original node to new node.</i></td></tr>
<tr><th id="3257">3257</th><td>  <em>int</em> iidx = <var>0</var>;</td></tr>
<tr><th id="3258">3258</th><td>  <b>for</b> (<em>int</em> on_slot_idx = <var>0</var>; on_slot_idx &lt; old_node_input_slots; on_slot_idx++) {</td></tr>
<tr><th id="3259">3259</th><td>    <i>// An input slot could be a single tensor or a list. We need</i></td></tr>
<tr><th id="3260">3260</th><td><i>    // to handle this case accordingly.</i></td></tr>
<tr><th id="3261">3261</th><td>    CHECK_LT(iidx, old_node_inputs.size());</td></tr>
<tr><th id="3262">3262</th><td>    <em>const</em> OpDef::ArgDef&amp; arg = old_node-&gt;op_def().input_arg(on_slot_idx);</td></tr>
<tr><th id="3263">3263</th><td>    <b>if</b> (ArgIsList(arg)) {</td></tr>
<tr><th id="3264">3264</th><td>      std::vector&lt;NodeBuilder::NodeOut&gt; new_node_inputs;</td></tr>
<tr><th id="3265">3265</th><td>      <em>int</em> N = GetTensorListLength(arg, old_node);</td></tr>
<tr><th id="3266">3266</th><td>      GetNodesProducingTFTensorList(old_node_inputs, &amp;iidx, N,</td></tr>
<tr><th id="3267">3267</th><td>                                    &amp;new_node_inputs);</td></tr>
<tr><th id="3268">3268</th><td>      nb-&gt;Input(new_node_inputs);</td></tr>
<tr><th id="3269">3269</th><td>      nn_slot_idx++;</td></tr>
<tr><th id="3270">3270</th><td>    } <b>else</b> {</td></tr>
<tr><th id="3271">3271</th><td>      <i>// Special case for connecting filter input of Conv2DBackpropInput</i></td></tr>
<tr><th id="3272">3272</th><td>      <b>if</b> (do_connect_conv2d_backprop_input_filter &amp;&amp;</td></tr>
<tr><th id="3273">3273</th><td>          iidx == kConv2DBackpropInputFilterInputSlotIdx) {</td></tr>
<tr><th id="3274">3274</th><td>        nb-&gt;Input(conv2d_node, kConv2DFilterOutputSlotIdx);</td></tr>
<tr><th id="3275">3275</th><td>      } <b>else</b> {</td></tr>
<tr><th id="3276">3276</th><td>        nb-&gt;Input(old_node_inputs[iidx].first, old_node_inputs[iidx].second);</td></tr>
<tr><th id="3277">3277</th><td>      }</td></tr>
<tr><th id="3278">3278</th><td>      iidx++;</td></tr>
<tr><th id="3279">3279</th><td>      nn_slot_idx++;</td></tr>
<tr><th id="3280">3280</th><td>    }</td></tr>
<tr><th id="3281">3281</th><td>  }</td></tr>
<tr><th id="3282">3282</th><td></td></tr>
<tr><th id="3283">3283</th><td>  <i>// If workspace tensors are available for this op and we are using</i></td></tr>
<tr><th id="3284">3284</th><td><i>  // contiguous ordering then we need to add Tensorflow tensor for</i></td></tr>
<tr><th id="3285">3285</th><td><i>  // workspace here because Tensorflow tensor for workspace is the</i></td></tr>
<tr><th id="3286">3286</th><td><i>  // last tensor in the list of Tensorflow tensors.</i></td></tr>
<tr><th id="3287">3287</th><td>  <b>if</b> (are_workspace_tensors_available) {</td></tr>
<tr><th id="3288">3288</th><td>    CHECK_EQ(workspace_tensors-&gt;size(), <var>2</var>);</td></tr>
<tr><th id="3289">3289</th><td>    <i>// Tensorflow tensor</i></td></tr>
<tr><th id="3290">3290</th><td>    nb-&gt;Input((*workspace_tensors)[<var>0</var>].node, (*workspace_tensors)[<var>0</var>].index);</td></tr>
<tr><th id="3291">3291</th><td>    nn_slot_idx++;</td></tr>
<tr><th id="3292">3292</th><td>  }</td></tr>
<tr><th id="3293">3293</th><td></td></tr>
<tr><th id="3294">3294</th><td>  <i>// Let's now setup all Mkl inputs to a new node.</i></td></tr>
<tr><th id="3295">3295</th><td><i>  // Number of Mkl inputs must be same as number of TF inputs.</i></td></tr>
<tr><th id="3296">3296</th><td>  iidx = <var>0</var>;</td></tr>
<tr><th id="3297">3297</th><td>  <b>for</b> (<em>int</em> on_slot_idx = <var>0</var>; on_slot_idx &lt; old_node_input_slots; on_slot_idx++) {</td></tr>
<tr><th id="3298">3298</th><td>    <i>// An input slot could be a single tensor or a list. We need</i></td></tr>
<tr><th id="3299">3299</th><td><i>    // to handle this case accordingly.</i></td></tr>
<tr><th id="3300">3300</th><td>    CHECK_LT(iidx, old_node_inputs.size());</td></tr>
<tr><th id="3301">3301</th><td>    <em>const</em> OpDef::ArgDef&amp; arg = old_node-&gt;op_def().input_arg(on_slot_idx);</td></tr>
<tr><th id="3302">3302</th><td>    <b>if</b> (ArgIsList(arg)) {</td></tr>
<tr><th id="3303">3303</th><td>      std::vector&lt;NodeBuilder::NodeOut&gt; new_node_inputs;</td></tr>
<tr><th id="3304">3304</th><td>      <em>int</em> N = GetTensorListLength(arg, old_node);</td></tr>
<tr><th id="3305">3305</th><td>      GetNodesProducingMklTensorList(g, old_node, old_node_inputs, &amp;iidx, N,</td></tr>
<tr><th id="3306">3306</th><td>                                     &amp;new_node_inputs);</td></tr>
<tr><th id="3307">3307</th><td>      nb-&gt;Input(new_node_inputs);</td></tr>
<tr><th id="3308">3308</th><td>      nn_slot_idx++;</td></tr>
<tr><th id="3309">3309</th><td>    } <b>else</b> {</td></tr>
<tr><th id="3310">3310</th><td>      Node* mkl_node = <b>nullptr</b>;</td></tr>
<tr><th id="3311">3311</th><td>      <em>int</em> mkl_node_output_slot = <var>0</var>;</td></tr>
<tr><th id="3312">3312</th><td>      <i>// Special case for connecting filter input of Conv2DBackpropInput</i></td></tr>
<tr><th id="3313">3313</th><td>      <b>if</b> (do_connect_conv2d_backprop_input_filter &amp;&amp;</td></tr>
<tr><th id="3314">3314</th><td>          iidx == kConv2DBackpropInputFilterInputSlotIdx) {</td></tr>
<tr><th id="3315">3315</th><td>        GetNodeProducingMklTensor(g, old_node, conv2d_node,</td></tr>
<tr><th id="3316">3316</th><td>                                  kConv2DFilterOutputSlotIdx, &amp;mkl_node,</td></tr>
<tr><th id="3317">3317</th><td>                                  &amp;mkl_node_output_slot);</td></tr>
<tr><th id="3318">3318</th><td>      } <b>else</b> {</td></tr>
<tr><th id="3319">3319</th><td>        GetNodeProducingMklTensor(g, old_node, old_node_inputs[iidx].first,</td></tr>
<tr><th id="3320">3320</th><td>                                  old_node_inputs[iidx].second, &amp;mkl_node,</td></tr>
<tr><th id="3321">3321</th><td>                                  &amp;mkl_node_output_slot);</td></tr>
<tr><th id="3322">3322</th><td>      }</td></tr>
<tr><th id="3323">3323</th><td>      nb-&gt;Input(mkl_node, mkl_node_output_slot);</td></tr>
<tr><th id="3324">3324</th><td>      iidx++;</td></tr>
<tr><th id="3325">3325</th><td>      nn_slot_idx++;</td></tr>
<tr><th id="3326">3326</th><td>    }</td></tr>
<tr><th id="3327">3327</th><td>  }</td></tr>
<tr><th id="3328">3328</th><td></td></tr>
<tr><th id="3329">3329</th><td>  <i>// If workspace tensors are available for this op and we are using</i></td></tr>
<tr><th id="3330">3330</th><td><i>  // contiguous ordering then we need to add Mkl tensor for</i></td></tr>
<tr><th id="3331">3331</th><td><i>  // workspace here because Mkl tensor for workspace is the</i></td></tr>
<tr><th id="3332">3332</th><td><i>  // last tensor in the list of Mkl tensors.</i></td></tr>
<tr><th id="3333">3333</th><td>  <b>if</b> (are_workspace_tensors_available) {</td></tr>
<tr><th id="3334">3334</th><td>    CHECK_EQ(workspace_tensors-&gt;size(), <var>2</var>);</td></tr>
<tr><th id="3335">3335</th><td>    <i>// Mkl tensor</i></td></tr>
<tr><th id="3336">3336</th><td>    nb-&gt;Input((*workspace_tensors)[<var>1</var>].node, (*workspace_tensors)[<var>1</var>].index);</td></tr>
<tr><th id="3337">3337</th><td>    nn_slot_idx++;</td></tr>
<tr><th id="3338">3338</th><td>  }</td></tr>
<tr><th id="3339">3339</th><td></td></tr>
<tr><th id="3340">3340</th><td>  <b>return</b> nn_slot_idx;</td></tr>
<tr><th id="3341">3341</th><td>}</td></tr>
<tr><th id="3342">3342</th><td></td></tr>
<tr><th id="3343">3343</th><td>Status MklLayoutRewritePass::SetUpInputs(</td></tr>
<tr><th id="3344">3344</th><td>    std::unique_ptr&lt;Graph&gt;* g,</td></tr>
<tr><th id="3345">3345</th><td>    <em>const</em> gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt;&amp; old_node_inputs,</td></tr>
<tr><th id="3346">3346</th><td>    NodeBuilder* nb, Node* old_node) {</td></tr>
<tr><th id="3347">3347</th><td>  <i>// Let's check if we need to add workspace tensors for this node.</i></td></tr>
<tr><th id="3348">3348</th><td><i>  // We add workspace edge only for MaxPool, LRN and BatchNorm.</i></td></tr>
<tr><th id="3349">3349</th><td>  std::vector&lt;NodeBuilder::NodeOut&gt; workspace_tensors;</td></tr>
<tr><th id="3350">3350</th><td>  <em>bool</em> are_workspace_tensors_available = <b>false</b>;</td></tr>
<tr><th id="3351">3351</th><td>  AddWorkSpaceEdgeIfNeeded(g, old_node, nb, &amp;workspace_tensors,</td></tr>
<tr><th id="3352">3352</th><td>                           &amp;are_workspace_tensors_available);</td></tr>
<tr><th id="3353">3353</th><td></td></tr>
<tr><th id="3354">3354</th><td>  <em>int</em> new_node_input_slots = <var>0</var>;</td></tr>
<tr><th id="3355">3355</th><td>  <b>if</b> (kTensorOrdering == MklTfTensorOrdering::TENSORS_INTERLEAVED) {</td></tr>
<tr><th id="3356">3356</th><td>    <i>// TODO(nhasabni): implement this function just for same of completion.</i></td></tr>
<tr><th id="3357">3357</th><td><i>    // We do not use interleaved ordering right now.</i></td></tr>
<tr><th id="3358">3358</th><td>    <b>return</b> Status(</td></tr>
<tr><th id="3359">3359</th><td>        error::Code::UNIMPLEMENTED,</td></tr>
<tr><th id="3360">3360</th><td>        <q>"Interleaved ordering of tensors is currently not supported."</q>);</td></tr>
<tr><th id="3361">3361</th><td>  } <b>else</b> {</td></tr>
<tr><th id="3362">3362</th><td>    CHECK_EQ(kTensorOrdering, MklTfTensorOrdering::TENSORS_CONTIGUOUS);</td></tr>
<tr><th id="3363">3363</th><td>    new_node_input_slots = SetUpContiguousInputs(</td></tr>
<tr><th id="3364">3364</th><td>        g, old_node_inputs, nb, old_node, &amp;workspace_tensors,</td></tr>
<tr><th id="3365">3365</th><td>        are_workspace_tensors_available);</td></tr>
<tr><th id="3366">3366</th><td>  }</td></tr>
<tr><th id="3367">3367</th><td></td></tr>
<tr><th id="3368">3368</th><td>  <i>// Sanity check</i></td></tr>
<tr><th id="3369">3369</th><td>  <em>int</em> old_node_input_slots = old_node-&gt;op_def().input_arg_size();</td></tr>
<tr><th id="3370">3370</th><td>  <b>if</b> (!are_workspace_tensors_available) {</td></tr>
<tr><th id="3371">3371</th><td>    <i>// If we are not adding workspace tensors for this op, then the total</i></td></tr>
<tr><th id="3372">3372</th><td><i>    // number of input slots to the new node _must_ be 2 times the number</i></td></tr>
<tr><th id="3373">3373</th><td><i>    // of input slots to the original node: N original Tensorflow tensors and</i></td></tr>
<tr><th id="3374">3374</th><td><i>    // N for Mkl tensors corresponding to each Tensorflow tensors.</i></td></tr>
<tr><th id="3375">3375</th><td>    CHECK_EQ(new_node_input_slots, old_node_input_slots * <var>2</var>);</td></tr>
<tr><th id="3376">3376</th><td>  } <b>else</b> {</td></tr>
<tr><th id="3377">3377</th><td>    <i>// If we are adding workspace tensors for this op, then the total</i></td></tr>
<tr><th id="3378">3378</th><td><i>    // The total number of input slots to new node _must_ be 2 times the number</i></td></tr>
<tr><th id="3379">3379</th><td><i>    // of input slots to the original node: N original Tensorflow tensors and</i></td></tr>
<tr><th id="3380">3380</th><td><i>    // N for Mkl tensors corresponding to each Tensorflow tensors plus 2</i></td></tr>
<tr><th id="3381">3381</th><td><i>    // (for workspace Tensorflow tensor and workspace Mkl tensor).</i></td></tr>
<tr><th id="3382">3382</th><td>    CHECK_EQ(new_node_input_slots, old_node_input_slots * <var>2</var> + <var>2</var>);</td></tr>
<tr><th id="3383">3383</th><td>  }</td></tr>
<tr><th id="3384">3384</th><td></td></tr>
<tr><th id="3385">3385</th><td>  <b>return</b> Status::OK();</td></tr>
<tr><th id="3386">3386</th><td>}</td></tr>
<tr><th id="3387">3387</th><td></td></tr>
<tr><th id="3388">3388</th><td><i>//////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="3389">3389</th><td><i>//           Helper functions related to workspace pass</i></td></tr>
<tr><th id="3390">3390</th><td><i>//////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="3391">3391</th><td><i></i></td></tr>
<tr><th id="3392">3392</th><td><i>// TODO(nhasabni) We should move this to mkl_util.h.</i></td></tr>
<tr><th id="3393">3393</th><td><em>void</em> MklLayoutRewritePass::GetDummyWorkspaceTensorNode(</td></tr>
<tr><th id="3394">3394</th><td>    std::unique_ptr&lt;Graph&gt;* g, Node** out, Node* orig_node) {</td></tr>
<tr><th id="3395">3395</th><td>  <i>// We use a tensor of shape {1} and value 0 to represent</i></td></tr>
<tr><th id="3396">3396</th><td><i>  // dummy float tensor. We need this as a dummy workspace tensor.</i></td></tr>
<tr><th id="3397">3397</th><td><i>  // Workspace tensor has type uint8.</i></td></tr>
<tr><th id="3398">3398</th><td>  <em>const</em> DataType dt = DataTypeToEnum&lt;uint8&gt;::v();</td></tr>
<tr><th id="3399">3399</th><td>  TensorProto proto;</td></tr>
<tr><th id="3400">3400</th><td>  proto.set_dtype(dt);</td></tr>
<tr><th id="3401">3401</th><td>  <em>float</em> zero[<var>1</var>] = {<var>0</var>};</td></tr>
<tr><th id="3402">3402</th><td>  proto.set_tensor_content(string(<b>reinterpret_cast</b>&lt;<em>char</em>*&gt;(&amp;zero), <var>4</var>));</td></tr>
<tr><th id="3403">3403</th><td>  TensorShape dummy_shape({<var>1</var>});</td></tr>
<tr><th id="3404">3404</th><td>  dummy_shape.AsProto(proto.mutable_tensor_shape());</td></tr>
<tr><th id="3405">3405</th><td>  TF_CHECK_OK(NodeBuilder((*g)-&gt;NewName(<q>"DMT"</q>), <q>"Const"</q>)</td></tr>
<tr><th id="3406">3406</th><td>                  .Attr(<q>"value"</q>, proto)</td></tr>
<tr><th id="3407">3407</th><td>                  .Attr(<q>"dtype"</q>, dt)</td></tr>
<tr><th id="3408">3408</th><td>                  .Device(orig_node-&gt;def().device())  <i>// We place this node on</i></td></tr>
<tr><th id="3409">3409</th><td>                                                      <i>// same the device as the</i></td></tr>
<tr><th id="3410">3410</th><td><i>                                                      // device of the original</i></td></tr>
<tr><th id="3411">3411</th><td><i>                                                      // node.</i></td></tr>
<tr><th id="3412">3412</th><td>                  .Finalize(&amp;**g, out));</td></tr>
<tr><th id="3413">3413</th><td></td></tr>
<tr><th id="3414">3414</th><td>  <i>// If number of inputs to the original node is &gt; 0, then we add</i></td></tr>
<tr><th id="3415">3415</th><td><i>  // control dependency between 1st input (index 0) of the original node and</i></td></tr>
<tr><th id="3416">3416</th><td><i>  // the dummy Mkl node. This is needed because control-flow ops such as Enter,</i></td></tr>
<tr><th id="3417">3417</th><td><i>  // Merge, etc, require frame_name of the dummy Mkl node to be same as the</i></td></tr>
<tr><th id="3418">3418</th><td><i>  // rewritten node. Adding control edge between 1st input of the original node</i></td></tr>
<tr><th id="3419">3419</th><td><i>  // and the dummy Mkl node ensures that the dummy node is in the same frame</i></td></tr>
<tr><th id="3420">3420</th><td><i>  // as the original node. Choosing 1st input is not necessary - any input of</i></td></tr>
<tr><th id="3421">3421</th><td><i>  // the original node is fine because all the inputs of a node are always in</i></td></tr>
<tr><th id="3422">3422</th><td><i>  // the same frame.</i></td></tr>
<tr><th id="3423">3423</th><td>  <b>if</b> (orig_node-&gt;num_inputs() &gt; <var>0</var>) {</td></tr>
<tr><th id="3424">3424</th><td>    Node* orig_input0 = <b>nullptr</b>;</td></tr>
<tr><th id="3425">3425</th><td>    TF_CHECK_OK(</td></tr>
<tr><th id="3426">3426</th><td>        orig_node-&gt;input_node(<var>0</var>, <b>const_cast</b>&lt;<em>const</em> Node**&gt;(&amp;orig_input0)));</td></tr>
<tr><th id="3427">3427</th><td>    <i>// Allow duplicate while adding control edge as it would fail (return</i></td></tr>
<tr><th id="3428">3428</th><td><i>    // NULL) if we try to add duplicate edge.</i></td></tr>
<tr><th id="3429">3429</th><td>    CHECK_NOTNULL((*g)-&gt;AddControlEdge(orig_input0, *out, <b>true</b>));</td></tr>
<tr><th id="3430">3430</th><td>  }</td></tr>
<tr><th id="3431">3431</th><td></td></tr>
<tr><th id="3432">3432</th><td>  (*out)-&gt;set_assigned_device_name(orig_node-&gt;assigned_device_name());</td></tr>
<tr><th id="3433">3433</th><td>}</td></tr>
<tr><th id="3434">3434</th><td></td></tr>
<tr><th id="3435">3435</th><td><em>void</em> MklLayoutRewritePass::AddWorkSpaceEdgeIfNeeded(</td></tr>
<tr><th id="3436">3436</th><td>    std::unique_ptr&lt;Graph&gt;* g, Node* orig_node, NodeBuilder* nb,</td></tr>
<tr><th id="3437">3437</th><td>    std::vector&lt;NodeBuilder::NodeOut&gt;* ws_tensors, <em>bool</em>* are_ws_tensors_added) {</td></tr>
<tr><th id="3438">3438</th><td>  <em>bool</em> workspace_edge_added = <b>false</b>;  <i>// Default initializer</i></td></tr>
<tr><th id="3439">3439</th><td>  CHECK_NOTNULL(are_ws_tensors_added);</td></tr>
<tr><th id="3440">3440</th><td>  *are_ws_tensors_added = <b>false</b>;  <i>// Default initializer</i></td></tr>
<tr><th id="3441">3441</th><td></td></tr>
<tr><th id="3442">3442</th><td>  DataType T;</td></tr>
<tr><th id="3443">3443</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="3444">3444</th><td>  <b>for</b> (<em>auto</em> ws : wsinfo_) {</td></tr>
<tr><th id="3445">3445</th><td>    <b>if</b> (orig_node-&gt;type_string() == ws.fwd_op &amp;&amp;</td></tr>
<tr><th id="3446">3446</th><td>        mkl_op_registry::IsMklOp(</td></tr>
<tr><th id="3447">3447</th><td>            mkl_op_registry::GetMklOpName(orig_node-&gt;type_string()), T)) {</td></tr>
<tr><th id="3448">3448</th><td>      <i>// If this op is a fwd op, then we need to check if there is an</i></td></tr>
<tr><th id="3449">3449</th><td><i>      // edge from this node's fwd_slot to bwdop's bwd_slot. If there is</i></td></tr>
<tr><th id="3450">3450</th><td><i>      // an edge, then we just add an attribute on this node for setting</i></td></tr>
<tr><th id="3451">3451</th><td><i>      // workspace_passed to true. We don't add actual workspace edge</i></td></tr>
<tr><th id="3452">3452</th><td><i>      // in this node. Actual workspace edge gets added in the backward</i></td></tr>
<tr><th id="3453">3453</th><td><i>      // op for this node.</i></td></tr>
<tr><th id="3454">3454</th><td>      <b>for</b> (<em>const</em> Edge* e : orig_node-&gt;out_edges()) {</td></tr>
<tr><th id="3455">3455</th><td>        <b>if</b> (e-&gt;src_output() == ws.fwd_slot &amp;&amp;</td></tr>
<tr><th id="3456">3456</th><td>            e-&gt;dst()-&gt;type_string() == ws.bwd_op &amp;&amp;</td></tr>
<tr><th id="3457">3457</th><td>            e-&gt;dst_input() == ws.bwd_slot) {</td></tr>
<tr><th id="3458">3458</th><td>          nb-&gt;Attr(<q>"workspace_enabled"</q>, <b>true</b>);</td></tr>
<tr><th id="3459">3459</th><td>          VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: workspace_enabled for "</q></td></tr>
<tr><th id="3460">3460</th><td>                  &lt;&lt; orig_node-&gt;type_string();</td></tr>
<tr><th id="3461">3461</th><td>          workspace_edge_added = <b>true</b>;</td></tr>
<tr><th id="3462">3462</th><td>          <i>// We found the edge that we were looking for, so break.</i></td></tr>
<tr><th id="3463">3463</th><td>          <b>break</b>;</td></tr>
<tr><th id="3464">3464</th><td>        }</td></tr>
<tr><th id="3465">3465</th><td>      }</td></tr>
<tr><th id="3466">3466</th><td></td></tr>
<tr><th id="3467">3467</th><td>      <b>if</b> (!workspace_edge_added) {</td></tr>
<tr><th id="3468">3468</th><td>        <i>// If we are here, then we did not find backward operator for this</i></td></tr>
<tr><th id="3469">3469</th><td><i>        // node.</i></td></tr>
<tr><th id="3470">3470</th><td>        nb-&gt;Attr(<q>"workspace_enabled"</q>, <b>false</b>);</td></tr>
<tr><th id="3471">3471</th><td>      }</td></tr>
<tr><th id="3472">3472</th><td>    } <b>else</b> <b>if</b> (orig_node-&gt;type_string() == ws.bwd_op &amp;&amp;</td></tr>
<tr><th id="3473">3473</th><td>               mkl_op_registry::IsMklOp(</td></tr>
<tr><th id="3474">3474</th><td>                   mkl_op_registry::GetMklOpName(orig_node-&gt;type_string()),</td></tr>
<tr><th id="3475">3475</th><td>                   T)) {</td></tr>
<tr><th id="3476">3476</th><td>      <i>// If this op is a bwd op, then we need to add workspace edge and</i></td></tr>
<tr><th id="3477">3477</th><td><i>      // it's Mkl tensor edge between its corresponding fwd op and this</i></td></tr>
<tr><th id="3478">3478</th><td><i>      // op. Corresponding fwd op is specified in 'fwd_op' field of</i></td></tr>
<tr><th id="3479">3479</th><td><i>      // workspace info. fwd_slot and bwd_slot in workspace info specify</i></td></tr>
<tr><th id="3480">3480</th><td><i>      // an edge between which slots connect forward and backward op.</i></td></tr>
<tr><th id="3481">3481</th><td><i>      // Once all these criteria match, we add a workspace edge between</i></td></tr>
<tr><th id="3482">3482</th><td><i>      // ws_fwd_slot and ws_bwd_slot. Its corresponding Mkl tensor is</i></td></tr>
<tr><th id="3483">3483</th><td><i>      // determined by interleaved/contiguous ordering. Function</i></td></tr>
<tr><th id="3484">3484</th><td><i>      // DataIndexToMetaDataIndex tells us the location of Mkl tensor</i></td></tr>
<tr><th id="3485">3485</th><td><i>      // from the location of the Tensorflow tensor.</i></td></tr>
<tr><th id="3486">3486</th><td>      <b>for</b> (<em>const</em> Edge* e : orig_node-&gt;in_edges()) {</td></tr>
<tr><th id="3487">3487</th><td>        <b>if</b> (e-&gt;src_output() == ws.fwd_slot &amp;&amp;</td></tr>
<tr><th id="3488">3488</th><td>            <i>// We would have rewritten the forward op, so we need to use</i></td></tr>
<tr><th id="3489">3489</th><td><i>            // GetMklOpName call to get its Mkl name.</i></td></tr>
<tr><th id="3490">3490</th><td>            e-&gt;src()-&gt;type_string() ==</td></tr>
<tr><th id="3491">3491</th><td>                mkl_op_registry::GetMklOpName(ws.fwd_op) &amp;&amp;</td></tr>
<tr><th id="3492">3492</th><td>            e-&gt;dst_input() == ws.bwd_slot) {</td></tr>
<tr><th id="3493">3493</th><td>          nb-&gt;Attr(<q>"workspace_enabled"</q>, <b>true</b>);</td></tr>
<tr><th id="3494">3494</th><td>          CHECK_NOTNULL(ws_tensors);</td></tr>
<tr><th id="3495">3495</th><td>          <i>// Add workspace edge between fwd op and bwd op.</i></td></tr>
<tr><th id="3496">3496</th><td>          ws_tensors-&gt;push_back(NodeBuilder::NodeOut(e-&gt;src(), ws.ws_fwd_slot));</td></tr>
<tr><th id="3497">3497</th><td>          <i>// Add Mkl tensor edge for workspace edge between fwd op and bwd op.</i></td></tr>
<tr><th id="3498">3498</th><td>          ws_tensors-&gt;push_back(NodeBuilder::NodeOut(</td></tr>
<tr><th id="3499">3499</th><td>              e-&gt;src(), DataIndexToMetaDataIndex(ws.ws_fwd_slot,</td></tr>
<tr><th id="3500">3500</th><td>                                                 e-&gt;src()-&gt;num_outputs())));</td></tr>
<tr><th id="3501">3501</th><td>          *are_ws_tensors_added = <b>true</b>;</td></tr>
<tr><th id="3502">3502</th><td>          <i>// In terms of input ordering, we add these calls to add Input</i></td></tr>
<tr><th id="3503">3503</th><td><i>          // here because workspace edge (and its Mkl tensor) is the last</i></td></tr>
<tr><th id="3504">3504</th><td><i>          // edge in the fwdop and bwdop. So all inputs before workspace</i></td></tr>
<tr><th id="3505">3505</th><td><i>          // tensor have been added by SetUpInputs function.</i></td></tr>
<tr><th id="3506">3506</th><td>          VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: workspace_enabled for "</q></td></tr>
<tr><th id="3507">3507</th><td>                  &lt;&lt; orig_node-&gt;type_string();</td></tr>
<tr><th id="3508">3508</th><td>          workspace_edge_added = <b>true</b>;</td></tr>
<tr><th id="3509">3509</th><td>          <i>// We found the edge that we were looking for, so break.</i></td></tr>
<tr><th id="3510">3510</th><td>          <b>break</b>;</td></tr>
<tr><th id="3511">3511</th><td>        }</td></tr>
<tr><th id="3512">3512</th><td>      }</td></tr>
<tr><th id="3513">3513</th><td></td></tr>
<tr><th id="3514">3514</th><td>      <i>// If we are here means we did not find fwd op that feeds to this</i></td></tr>
<tr><th id="3515">3515</th><td><i>      // bwd op. So in this case, we need to generate dummy tensors for</i></td></tr>
<tr><th id="3516">3516</th><td><i>      // workspace input and Mkl tensor for workspace, and set</i></td></tr>
<tr><th id="3517">3517</th><td><i>      // workspace_enabled to false.</i></td></tr>
<tr><th id="3518">3518</th><td>      <b>if</b> (!workspace_edge_added) {</td></tr>
<tr><th id="3519">3519</th><td>        nb-&gt;Attr(<q>"workspace_enabled"</q>, <b>false</b>);</td></tr>
<tr><th id="3520">3520</th><td>        Node* dmt_ws = <b>nullptr</b>;      <i>// Dummy tensor for workspace</i></td></tr>
<tr><th id="3521">3521</th><td>        Node* dmt_mkl_ws = <b>nullptr</b>;  <i>// Dummy Mkl tensor for workspace</i></td></tr>
<tr><th id="3522">3522</th><td>        GetDummyWorkspaceTensorNode(g, &amp;dmt_ws, orig_node);</td></tr>
<tr><th id="3523">3523</th><td>        GetDummyMklTensorNode(g, &amp;dmt_mkl_ws, orig_node);</td></tr>
<tr><th id="3524">3524</th><td>        CHECK_NOTNULL(dmt_ws);</td></tr>
<tr><th id="3525">3525</th><td>        CHECK_NOTNULL(dmt_mkl_ws);</td></tr>
<tr><th id="3526">3526</th><td>        CHECK_NOTNULL(ws_tensors);</td></tr>
<tr><th id="3527">3527</th><td>        <i>// We add dummy tensor as workspace tensor.</i></td></tr>
<tr><th id="3528">3528</th><td>        ws_tensors-&gt;push_back(NodeBuilder::NodeOut(dmt_ws, <var>0</var>));</td></tr>
<tr><th id="3529">3529</th><td>        <i>// We add dummy tensor as Mkl tensor for workspace tensor.</i></td></tr>
<tr><th id="3530">3530</th><td>        ws_tensors-&gt;push_back(NodeBuilder::NodeOut(dmt_mkl_ws, <var>0</var>));</td></tr>
<tr><th id="3531">3531</th><td>        *are_ws_tensors_added = <b>true</b>;</td></tr>
<tr><th id="3532">3532</th><td>        VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: dummy workspace_enabled for "</q></td></tr>
<tr><th id="3533">3533</th><td>                &lt;&lt; orig_node-&gt;type_string();</td></tr>
<tr><th id="3534">3534</th><td>      }</td></tr>
<tr><th id="3535">3535</th><td>    } <b>else</b> {</td></tr>
<tr><th id="3536">3536</th><td>      <i>// If this node does not match any workspace info, then we do not</i></td></tr>
<tr><th id="3537">3537</th><td><i>      // do anything special for workspace propagation for it.</i></td></tr>
<tr><th id="3538">3538</th><td>    }</td></tr>
<tr><th id="3539">3539</th><td>  }</td></tr>
<tr><th id="3540">3540</th><td>}</td></tr>
<tr><th id="3541">3541</th><td></td></tr>
<tr><th id="3542">3542</th><td><i>//////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="3543">3543</th><td><i>// Op-specific functions to copy attributes from old node to new node</i></td></tr>
<tr><th id="3544">3544</th><td><i>//////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="3545">3545</th><td></td></tr>
<tr><th id="3546">3546</th><td><em>void</em> MklLayoutRewritePass::CopyAttrsConv2D(<em>const</em> Node* orig_node,</td></tr>
<tr><th id="3547">3547</th><td>                                           NodeBuilder* nb) {</td></tr>
<tr><th id="3548">3548</th><td>  DataType T;</td></tr>
<tr><th id="3549">3549</th><td>  string data_format;</td></tr>
<tr><th id="3550">3550</th><td>  string padding;</td></tr>
<tr><th id="3551">3551</th><td>  std::vector&lt;int32&gt; strides;</td></tr>
<tr><th id="3552">3552</th><td>  std::vector&lt;int32&gt; dilations;</td></tr>
<tr><th id="3553">3553</th><td>  <em>bool</em> use_cudnn_on_gpu;</td></tr>
<tr><th id="3554">3554</th><td></td></tr>
<tr><th id="3555">3555</th><td>  <i>// Get all attributes from old node.</i></td></tr>
<tr><th id="3556">3556</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="3557">3557</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"strides"</q>, &amp;strides));</td></tr>
<tr><th id="3558">3558</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"dilations"</q>, &amp;dilations));</td></tr>
<tr><th id="3559">3559</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"padding"</q>, &amp;padding));</td></tr>
<tr><th id="3560">3560</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"data_format"</q>, &amp;data_format));</td></tr>
<tr><th id="3561">3561</th><td>  TF_CHECK_OK(</td></tr>
<tr><th id="3562">3562</th><td>      GetNodeAttr(orig_node-&gt;def(), <q>"use_cudnn_on_gpu"</q>, &amp;use_cudnn_on_gpu));</td></tr>
<tr><th id="3563">3563</th><td></td></tr>
<tr><th id="3564">3564</th><td>  <i>// Add attributes to new node.</i></td></tr>
<tr><th id="3565">3565</th><td>  nb-&gt;Attr(<q>"T"</q>, T);</td></tr>
<tr><th id="3566">3566</th><td>  nb-&gt;Attr(<q>"strides"</q>, strides);</td></tr>
<tr><th id="3567">3567</th><td>  nb-&gt;Attr(<q>"dilations"</q>, dilations);</td></tr>
<tr><th id="3568">3568</th><td>  nb-&gt;Attr(<q>"padding"</q>, padding);</td></tr>
<tr><th id="3569">3569</th><td>  nb-&gt;Attr(<q>"data_format"</q>, data_format);</td></tr>
<tr><th id="3570">3570</th><td>  nb-&gt;Attr(<q>"use_cudnn_on_gpu"</q>, use_cudnn_on_gpu);</td></tr>
<tr><th id="3571">3571</th><td>}</td></tr>
<tr><th id="3572">3572</th><td></td></tr>
<tr><th id="3573">3573</th><td><em>void</em> MklLayoutRewritePass::CopyAttrsAddN(<em>const</em> Node* orig_node,</td></tr>
<tr><th id="3574">3574</th><td>                                         NodeBuilder* nb) {</td></tr>
<tr><th id="3575">3575</th><td>  DataType T;</td></tr>
<tr><th id="3576">3576</th><td>  <em>int</em> N;</td></tr>
<tr><th id="3577">3577</th><td></td></tr>
<tr><th id="3578">3578</th><td>  <i>// Get all attributes from old node.</i></td></tr>
<tr><th id="3579">3579</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="3580">3580</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"N"</q>, &amp;N));</td></tr>
<tr><th id="3581">3581</th><td></td></tr>
<tr><th id="3582">3582</th><td>  <i>// Add attributes to new node.</i></td></tr>
<tr><th id="3583">3583</th><td>  nb-&gt;Attr(<q>"T"</q>, T);</td></tr>
<tr><th id="3584">3584</th><td>  nb-&gt;Attr(<q>"N"</q>, N);</td></tr>
<tr><th id="3585">3585</th><td>}</td></tr>
<tr><th id="3586">3586</th><td></td></tr>
<tr><th id="3587">3587</th><td><em>void</em> MklLayoutRewritePass::CopyAttrsBiasAddGrad(<em>const</em> Node* orig_node,</td></tr>
<tr><th id="3588">3588</th><td>                                                NodeBuilder* nb) {</td></tr>
<tr><th id="3589">3589</th><td>  DataType T;</td></tr>
<tr><th id="3590">3590</th><td>  string data_format;</td></tr>
<tr><th id="3591">3591</th><td>  std::vector&lt;int32&gt; strides;</td></tr>
<tr><th id="3592">3592</th><td></td></tr>
<tr><th id="3593">3593</th><td>  <i>// Get all attributes from old node.</i></td></tr>
<tr><th id="3594">3594</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="3595">3595</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"strides"</q>, &amp;strides));</td></tr>
<tr><th id="3596">3596</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"data_format"</q>, &amp;data_format));</td></tr>
<tr><th id="3597">3597</th><td></td></tr>
<tr><th id="3598">3598</th><td>  <i>// Add attributes to new node.</i></td></tr>
<tr><th id="3599">3599</th><td>  nb-&gt;Attr(<q>"T"</q>, T);</td></tr>
<tr><th id="3600">3600</th><td>  nb-&gt;Attr(<q>"strides"</q>, strides);</td></tr>
<tr><th id="3601">3601</th><td>  nb-&gt;Attr(<q>"data_format"</q>, data_format);</td></tr>
<tr><th id="3602">3602</th><td>}</td></tr>
<tr><th id="3603">3603</th><td></td></tr>
<tr><th id="3604">3604</th><td><em>void</em> MklLayoutRewritePass::CopyAttrsLRN(<em>const</em> Node* orig_node,</td></tr>
<tr><th id="3605">3605</th><td>                                        NodeBuilder* nb) {</td></tr>
<tr><th id="3606">3606</th><td>  DataType T;</td></tr>
<tr><th id="3607">3607</th><td>  <em>int</em> depth_radius;</td></tr>
<tr><th id="3608">3608</th><td>  <em>float</em> bias;</td></tr>
<tr><th id="3609">3609</th><td>  <em>float</em> alpha;</td></tr>
<tr><th id="3610">3610</th><td>  <em>float</em> beta;</td></tr>
<tr><th id="3611">3611</th><td></td></tr>
<tr><th id="3612">3612</th><td>  <i>// Get all attributes from old node.</i></td></tr>
<tr><th id="3613">3613</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="3614">3614</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"depth_radius"</q>, &amp;depth_radius));</td></tr>
<tr><th id="3615">3615</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"bias"</q>, &amp;bias));</td></tr>
<tr><th id="3616">3616</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"alpha"</q>, &amp;alpha));</td></tr>
<tr><th id="3617">3617</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"beta"</q>, &amp;beta));</td></tr>
<tr><th id="3618">3618</th><td></td></tr>
<tr><th id="3619">3619</th><td>  <i>// Add attributes to new node.</i></td></tr>
<tr><th id="3620">3620</th><td>  nb-&gt;Attr(<q>"T"</q>, T);</td></tr>
<tr><th id="3621">3621</th><td>  nb-&gt;Attr(<q>"depth_radius"</q>, depth_radius);</td></tr>
<tr><th id="3622">3622</th><td>  nb-&gt;Attr(<q>"bias"</q>, bias);</td></tr>
<tr><th id="3623">3623</th><td>  nb-&gt;Attr(<q>"alpha"</q>, alpha);</td></tr>
<tr><th id="3624">3624</th><td>  nb-&gt;Attr(<q>"beta"</q>, beta);</td></tr>
<tr><th id="3625">3625</th><td>}</td></tr>
<tr><th id="3626">3626</th><td></td></tr>
<tr><th id="3627">3627</th><td><em>void</em> MklLayoutRewritePass::CopyAttrsPooling(<em>const</em> Node* orig_node,</td></tr>
<tr><th id="3628">3628</th><td>                                            NodeBuilder* nb) {</td></tr>
<tr><th id="3629">3629</th><td>  DataType T;</td></tr>
<tr><th id="3630">3630</th><td>  string data_format;</td></tr>
<tr><th id="3631">3631</th><td>  string padding;</td></tr>
<tr><th id="3632">3632</th><td>  std::vector&lt;int32&gt; ksize, strides;</td></tr>
<tr><th id="3633">3633</th><td></td></tr>
<tr><th id="3634">3634</th><td>  <i>// Get all attributes from old node.</i></td></tr>
<tr><th id="3635">3635</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="3636">3636</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"ksize"</q>, &amp;ksize));</td></tr>
<tr><th id="3637">3637</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"strides"</q>, &amp;strides));</td></tr>
<tr><th id="3638">3638</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"padding"</q>, &amp;padding));</td></tr>
<tr><th id="3639">3639</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"data_format"</q>, &amp;data_format));</td></tr>
<tr><th id="3640">3640</th><td></td></tr>
<tr><th id="3641">3641</th><td>  <i>// Add attributes to new node.</i></td></tr>
<tr><th id="3642">3642</th><td>  nb-&gt;Attr(<q>"T"</q>, T);</td></tr>
<tr><th id="3643">3643</th><td>  nb-&gt;Attr(<q>"ksize"</q>, ksize);</td></tr>
<tr><th id="3644">3644</th><td>  nb-&gt;Attr(<q>"strides"</q>, strides);</td></tr>
<tr><th id="3645">3645</th><td>  nb-&gt;Attr(<q>"padding"</q>, padding);</td></tr>
<tr><th id="3646">3646</th><td>  nb-&gt;Attr(<q>"data_format"</q>, data_format);</td></tr>
<tr><th id="3647">3647</th><td>}</td></tr>
<tr><th id="3648">3648</th><td></td></tr>
<tr><th id="3649">3649</th><td><em>void</em> MklLayoutRewritePass::CopyAttrsDataType(<em>const</em> Node* orig_node,</td></tr>
<tr><th id="3650">3650</th><td>                                             NodeBuilder* nb) {</td></tr>
<tr><th id="3651">3651</th><td>  DataType T;</td></tr>
<tr><th id="3652">3652</th><td></td></tr>
<tr><th id="3653">3653</th><td>  <i>// Get all attributes from old node.</i></td></tr>
<tr><th id="3654">3654</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="3655">3655</th><td></td></tr>
<tr><th id="3656">3656</th><td>  <i>// Add attributes to new node.</i></td></tr>
<tr><th id="3657">3657</th><td>  nb-&gt;Attr(<q>"T"</q>, T);</td></tr>
<tr><th id="3658">3658</th><td>}</td></tr>
<tr><th id="3659">3659</th><td></td></tr>
<tr><th id="3660">3660</th><td><em>void</em> MklLayoutRewritePass::CopyAttrsReshape(<em>const</em> Node* orig_node,</td></tr>
<tr><th id="3661">3661</th><td>                                            NodeBuilder* nb) {</td></tr>
<tr><th id="3662">3662</th><td>  DataType T;</td></tr>
<tr><th id="3663">3663</th><td>  DataType Tshape;</td></tr>
<tr><th id="3664">3664</th><td></td></tr>
<tr><th id="3665">3665</th><td>  <i>// Get all attributes from old node.</i></td></tr>
<tr><th id="3666">3666</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="3667">3667</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"Tshape"</q>, &amp;Tshape));</td></tr>
<tr><th id="3668">3668</th><td>  <i>// Add attributes to new node.</i></td></tr>
<tr><th id="3669">3669</th><td>  nb-&gt;Attr(<q>"T"</q>, T);</td></tr>
<tr><th id="3670">3670</th><td>  nb-&gt;Attr(<q>"Tshape"</q>, Tshape);</td></tr>
<tr><th id="3671">3671</th><td>}</td></tr>
<tr><th id="3672">3672</th><td></td></tr>
<tr><th id="3673">3673</th><td><em>void</em> MklLayoutRewritePass::CopyAttrsSplit(<em>const</em> Node* orig_node,</td></tr>
<tr><th id="3674">3674</th><td>                                          NodeBuilder* nb) {</td></tr>
<tr><th id="3675">3675</th><td>  DataType T;</td></tr>
<tr><th id="3676">3676</th><td>  string data_format;</td></tr>
<tr><th id="3677">3677</th><td>  <em>int</em> num_split;</td></tr>
<tr><th id="3678">3678</th><td></td></tr>
<tr><th id="3679">3679</th><td>  <i>// Get all attributes from old node.</i></td></tr>
<tr><th id="3680">3680</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="3681">3681</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"num_split"</q>, &amp;num_split));</td></tr>
<tr><th id="3682">3682</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"data_format"</q>, &amp;data_format));</td></tr>
<tr><th id="3683">3683</th><td></td></tr>
<tr><th id="3684">3684</th><td>  <i>// Add attributes to new node.</i></td></tr>
<tr><th id="3685">3685</th><td>  nb-&gt;Attr(<q>"T"</q>, T);</td></tr>
<tr><th id="3686">3686</th><td>  nb-&gt;Attr(<q>"num_split"</q>, num_split);</td></tr>
<tr><th id="3687">3687</th><td>  nb-&gt;Attr(<q>"data_format"</q>, data_format);</td></tr>
<tr><th id="3688">3688</th><td>}</td></tr>
<tr><th id="3689">3689</th><td></td></tr>
<tr><th id="3690">3690</th><td><em>void</em> MklLayoutRewritePass::CopyAttrsConcat(<em>const</em> Node* orig_node,</td></tr>
<tr><th id="3691">3691</th><td>                                           NodeBuilder* nb) {</td></tr>
<tr><th id="3692">3692</th><td>  DataType T;</td></tr>
<tr><th id="3693">3693</th><td>  <em>int</em> N;</td></tr>
<tr><th id="3694">3694</th><td></td></tr>
<tr><th id="3695">3695</th><td>  <i>// Get all attributes from old node.</i></td></tr>
<tr><th id="3696">3696</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="3697">3697</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"N"</q>, &amp;N));</td></tr>
<tr><th id="3698">3698</th><td></td></tr>
<tr><th id="3699">3699</th><td>  <i>// Add attributes to new node.</i></td></tr>
<tr><th id="3700">3700</th><td>  nb-&gt;Attr(<q>"T"</q>, T);</td></tr>
<tr><th id="3701">3701</th><td>  nb-&gt;Attr(<q>"N"</q>, N);</td></tr>
<tr><th id="3702">3702</th><td>}</td></tr>
<tr><th id="3703">3703</th><td></td></tr>
<tr><th id="3704">3704</th><td><em>void</em> MklLayoutRewritePass::CopyAttrsConcatV2(<em>const</em> Node* orig_node,</td></tr>
<tr><th id="3705">3705</th><td>                                             NodeBuilder* nb) {</td></tr>
<tr><th id="3706">3706</th><td>  DataType T;</td></tr>
<tr><th id="3707">3707</th><td>  <em>int</em> N;</td></tr>
<tr><th id="3708">3708</th><td>  DataType tidx;</td></tr>
<tr><th id="3709">3709</th><td></td></tr>
<tr><th id="3710">3710</th><td>  <i>// Get all attributes from old node.</i></td></tr>
<tr><th id="3711">3711</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="3712">3712</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"N"</q>, &amp;N));</td></tr>
<tr><th id="3713">3713</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"Tidx"</q>, &amp;tidx));</td></tr>
<tr><th id="3714">3714</th><td></td></tr>
<tr><th id="3715">3715</th><td>  <i>// Add attributes to new node.</i></td></tr>
<tr><th id="3716">3716</th><td>  nb-&gt;Attr(<q>"T"</q>, T);</td></tr>
<tr><th id="3717">3717</th><td>  nb-&gt;Attr(<q>"N"</q>, N);</td></tr>
<tr><th id="3718">3718</th><td>  nb-&gt;Attr(<q>"Tidx"</q>, tidx);</td></tr>
<tr><th id="3719">3719</th><td>}</td></tr>
<tr><th id="3720">3720</th><td></td></tr>
<tr><th id="3721">3721</th><td><em>void</em> MklLayoutRewritePass::CopyAttrsFusedBatchNorm(<em>const</em> Node* orig_node,</td></tr>
<tr><th id="3722">3722</th><td>                                                   NodeBuilder* nb) {</td></tr>
<tr><th id="3723">3723</th><td>  DataType T;</td></tr>
<tr><th id="3724">3724</th><td>  <em>float</em> epsilon;</td></tr>
<tr><th id="3725">3725</th><td>  string data_format;</td></tr>
<tr><th id="3726">3726</th><td>  <em>bool</em> is_training;</td></tr>
<tr><th id="3727">3727</th><td></td></tr>
<tr><th id="3728">3728</th><td>  <i>// Get all attributes from old node.</i></td></tr>
<tr><th id="3729">3729</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"T"</q>, &amp;T));</td></tr>
<tr><th id="3730">3730</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"epsilon"</q>, &amp;epsilon));</td></tr>
<tr><th id="3731">3731</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"data_format"</q>, &amp;data_format));</td></tr>
<tr><th id="3732">3732</th><td>  TF_CHECK_OK(GetNodeAttr(orig_node-&gt;def(), <q>"is_training"</q>, &amp;is_training));</td></tr>
<tr><th id="3733">3733</th><td></td></tr>
<tr><th id="3734">3734</th><td>  <i>// Add attributes to new node.</i></td></tr>
<tr><th id="3735">3735</th><td>  nb-&gt;Attr(<q>"T"</q>, T);</td></tr>
<tr><th id="3736">3736</th><td>  nb-&gt;Attr(<q>"epsilon"</q>, epsilon);</td></tr>
<tr><th id="3737">3737</th><td>  nb-&gt;Attr(<q>"data_format"</q>, data_format);</td></tr>
<tr><th id="3738">3738</th><td>  nb-&gt;Attr(<q>"is_training"</q>, is_training);</td></tr>
<tr><th id="3739">3739</th><td>}</td></tr>
<tr><th id="3740">3740</th><td></td></tr>
<tr><th id="3741">3741</th><td><i>//////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="3742">3742</th><td><i>//           Helper functions related to node merge pass</i></td></tr>
<tr><th id="3743">3743</th><td><i>//////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="3744">3744</th><td></td></tr>
<tr><th id="3745">3745</th><td>Node* MklLayoutRewritePass::CheckForNodeMerge(<em>const</em> Node* a) <em>const</em> {</td></tr>
<tr><th id="3746">3746</th><td>  <i>// TODO(nhasabni) Add check for type of node similar to CheckForNodeRewrite</i></td></tr>
<tr><th id="3747">3747</th><td><i>  // once we support BiasAddGrad as Mkl layer.</i></td></tr>
<tr><th id="3748">3748</th><td><i></i></td></tr>
<tr><th id="3749">3749</th><td><i>  // Search for all matching mergeinfo.</i></td></tr>
<tr><th id="3750">3750</th><td><i>  // We allow more than one match for extensibility.</i></td></tr>
<tr><th id="3751">3751</th><td>  std::vector&lt;<em>const</em> MergeInfo*&gt; matching_mi;</td></tr>
<tr><th id="3752">3752</th><td>  <b>for</b> (<em>auto</em> mi = minfo_.cbegin(); mi != minfo_.cend(); ++mi) {</td></tr>
<tr><th id="3753">3753</th><td>    <b>if</b> (a-&gt;type_string() == mi-&gt;op1 || a-&gt;type_string() == mi-&gt;op2) {</td></tr>
<tr><th id="3754">3754</th><td>      matching_mi.push_back(&amp;*mi);</td></tr>
<tr><th id="3755">3755</th><td>    }</td></tr>
<tr><th id="3756">3756</th><td>  }</td></tr>
<tr><th id="3757">3757</th><td></td></tr>
<tr><th id="3758">3758</th><td>  <b>for</b> (<em>const</em> MergeInfo* mi : matching_mi) {</td></tr>
<tr><th id="3759">3759</th><td>    <i>// Get the operand with which 'a' can be merged.</i></td></tr>
<tr><th id="3760">3760</th><td>    Node* b = <b>nullptr</b>;</td></tr>
<tr><th id="3761">3761</th><td>    <b>if</b> ((b = mi-&gt;get_node_to_be_merged(a)) == <b>nullptr</b>) {</td></tr>
<tr><th id="3762">3762</th><td>      <b>continue</b>;</td></tr>
<tr><th id="3763">3763</th><td>    }</td></tr>
<tr><th id="3764">3764</th><td></td></tr>
<tr><th id="3765">3765</th><td>    <i>// Get the control edges and input of node</i></td></tr>
<tr><th id="3766">3766</th><td>    <em>const</em> <em>int</em> N_in = a-&gt;num_inputs();</td></tr>
<tr><th id="3767">3767</th><td>    gtl::InlinedVector&lt;Node*, <var>4</var>&gt; a_control_edges;</td></tr>
<tr><th id="3768">3768</th><td>    gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt; a_in(N_in);</td></tr>
<tr><th id="3769">3769</th><td>    FillInputs(a, &amp;a_control_edges, &amp;a_in);</td></tr>
<tr><th id="3770">3770</th><td></td></tr>
<tr><th id="3771">3771</th><td>    <em>const</em> <em>int</em> B_in = b-&gt;num_inputs();</td></tr>
<tr><th id="3772">3772</th><td>    gtl::InlinedVector&lt;Node*, <var>4</var>&gt; b_control_edges;</td></tr>
<tr><th id="3773">3773</th><td>    gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt; b_in(B_in);</td></tr>
<tr><th id="3774">3774</th><td>    FillInputs(b, &amp;b_control_edges, &amp;b_in);</td></tr>
<tr><th id="3775">3775</th><td></td></tr>
<tr><th id="3776">3776</th><td>    <i>// Shouldn't merge if a and b have different control edges.</i></td></tr>
<tr><th id="3777">3777</th><td>    <b>if</b> (a_control_edges != b_control_edges) {</td></tr>
<tr><th id="3778">3778</th><td>      <b>continue</b>;</td></tr>
<tr><th id="3779">3779</th><td>    } <b>else</b> {</td></tr>
<tr><th id="3780">3780</th><td>      <i>// We found a match.</i></td></tr>
<tr><th id="3781">3781</th><td>      <b>return</b> b;</td></tr>
<tr><th id="3782">3782</th><td>    }</td></tr>
<tr><th id="3783">3783</th><td>  }</td></tr>
<tr><th id="3784">3784</th><td></td></tr>
<tr><th id="3785">3785</th><td>  <b>return</b> <b>nullptr</b>;</td></tr>
<tr><th id="3786">3786</th><td>}</td></tr>
<tr><th id="3787">3787</th><td></td></tr>
<tr><th id="3788">3788</th><td>Status MklLayoutRewritePass::MergeConv2DWithBiasAdd(std::unique_ptr&lt;Graph&gt;* g,</td></tr>
<tr><th id="3789">3789</th><td>                                                    Node* m, Node* n) {</td></tr>
<tr><th id="3790">3790</th><td>  CHECK_EQ(((m-&gt;type_string() == csinfo_.bias_add &amp;&amp;</td></tr>
<tr><th id="3791">3791</th><td>             n-&gt;type_string() == csinfo_.conv2d)) ||</td></tr>
<tr><th id="3792">3792</th><td>               ((n-&gt;type_string() == csinfo_.bias_add &amp;&amp;</td></tr>
<tr><th id="3793">3793</th><td>                 m-&gt;type_string() == csinfo_.conv2d)),</td></tr>
<tr><th id="3794">3794</th><td>           <b>true</b>);</td></tr>
<tr><th id="3795">3795</th><td></td></tr>
<tr><th id="3796">3796</th><td>  <i>// If 'm' is BiasAdd, then 'n' is Conv2D. Since Conv2D feeds BiasAdd,</i></td></tr>
<tr><th id="3797">3797</th><td><i>  // BiasAdd is successor node, and Conv2D predecessor node.</i></td></tr>
<tr><th id="3798">3798</th><td>  Node* pred = m-&gt;type_string() == csinfo_.bias_add ? n : m;</td></tr>
<tr><th id="3799">3799</th><td>  Node* succ = m-&gt;type_string() == csinfo_.bias_add ? m : n;</td></tr>
<tr><th id="3800">3800</th><td></td></tr>
<tr><th id="3801">3801</th><td>  <i>// 1. Get all attributes from input nodes.</i></td></tr>
<tr><th id="3802">3802</th><td>  DataType T_pred, T_succ;</td></tr>
<tr><th id="3803">3803</th><td>  string padding;</td></tr>
<tr><th id="3804">3804</th><td>  std::vector&lt;int32&gt; strides;</td></tr>
<tr><th id="3805">3805</th><td>  std::vector&lt;int32&gt; dilations;</td></tr>
<tr><th id="3806">3806</th><td>  string data_format_pred, data_format_succ;</td></tr>
<tr><th id="3807">3807</th><td>  <em>bool</em> use_cudnn_on_gnu;</td></tr>
<tr><th id="3808">3808</th><td>  TF_CHECK_OK(GetNodeAttr(pred-&gt;def(), <q>"T"</q>, &amp;T_pred));</td></tr>
<tr><th id="3809">3809</th><td>  TF_CHECK_OK(GetNodeAttr(succ-&gt;def(), <q>"T"</q>, &amp;T_succ));</td></tr>
<tr><th id="3810">3810</th><td>  TF_CHECK_OK(GetNodeAttr(pred-&gt;def(), <q>"padding"</q>, &amp;padding));</td></tr>
<tr><th id="3811">3811</th><td>  TF_CHECK_OK(GetNodeAttr(pred-&gt;def(), <q>"strides"</q>, &amp;strides));</td></tr>
<tr><th id="3812">3812</th><td>  TF_CHECK_OK(GetNodeAttr(pred-&gt;def(), <q>"dilations"</q>, &amp;dilations));</td></tr>
<tr><th id="3813">3813</th><td>  TF_CHECK_OK(GetNodeAttr(pred-&gt;def(), <q>"data_format"</q>, &amp;data_format_pred));</td></tr>
<tr><th id="3814">3814</th><td>  TF_CHECK_OK(GetNodeAttr(succ-&gt;def(), <q>"data_format"</q>, &amp;data_format_succ));</td></tr>
<tr><th id="3815">3815</th><td>  TF_CHECK_OK(GetNodeAttr(pred-&gt;def(), <q>"use_cudnn_on_gpu"</q>, &amp;use_cudnn_on_gnu));</td></tr>
<tr><th id="3816">3816</th><td>  <i>// We check to ensure that data formats of both succ and pred are same.</i></td></tr>
<tr><th id="3817">3817</th><td><i>  // We expect them to be same, so we can enforce this as assert.</i></td></tr>
<tr><th id="3818">3818</th><td><i>  // But assert can be too strict, so we enforce this as a check.</i></td></tr>
<tr><th id="3819">3819</th><td><i>  // If the check fails, then we do not merge two nodes.</i></td></tr>
<tr><th id="3820">3820</th><td><i>  // We also do same check for devices.</i></td></tr>
<tr><th id="3821">3821</th><td>  <b>if</b> (data_format_pred != data_format_succ || T_pred != T_succ ||</td></tr>
<tr><th id="3822">3822</th><td>      pred-&gt;assigned_device_name() != succ-&gt;assigned_device_name() ||</td></tr>
<tr><th id="3823">3823</th><td>      pred-&gt;def().device() != succ-&gt;def().device()) {</td></tr>
<tr><th id="3824">3824</th><td>    <b>return</b> Status(error::Code::INVALID_ARGUMENT,</td></tr>
<tr><th id="3825">3825</th><td>                  <q>"data_format or T attribute or devices of Conv2D and "</q></td></tr>
<tr><th id="3826">3826</th><td>                  <q>"BiasAdd do not match. Will skip node merge optimization"</q>);</td></tr>
<tr><th id="3827">3827</th><td>  }</td></tr>
<tr><th id="3828">3828</th><td></td></tr>
<tr><th id="3829">3829</th><td>  <em>const</em> <em>int</em> succ_num = succ-&gt;num_inputs();</td></tr>
<tr><th id="3830">3830</th><td>  gtl::InlinedVector&lt;Node*, <var>4</var>&gt; succ_control_edges;</td></tr>
<tr><th id="3831">3831</th><td>  gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt; succ_in(succ_num);</td></tr>
<tr><th id="3832">3832</th><td>  FillInputs(succ, &amp;succ_control_edges, &amp;succ_in);</td></tr>
<tr><th id="3833">3833</th><td></td></tr>
<tr><th id="3834">3834</th><td>  <em>const</em> <em>int</em> pred_num = pred-&gt;num_inputs();</td></tr>
<tr><th id="3835">3835</th><td>  gtl::InlinedVector&lt;Node*, <var>4</var>&gt; pred_control_edges;</td></tr>
<tr><th id="3836">3836</th><td>  gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt; pred_in(pred_num);</td></tr>
<tr><th id="3837">3837</th><td>  FillInputs(pred, &amp;pred_control_edges, &amp;pred_in);</td></tr>
<tr><th id="3838">3838</th><td></td></tr>
<tr><th id="3839">3839</th><td>  <i>// We need to ensure that Conv2D only feeds to BiasAdd (some other operator is</i></td></tr>
<tr><th id="3840">3840</th><td><i>  // not expecting output of Conv2D). If this is not the case, then we cannot</i></td></tr>
<tr><th id="3841">3841</th><td><i>  // merge Conv2D with BiasAdd.</i></td></tr>
<tr><th id="3842">3842</th><td>  <em>const</em> <em>int</em> kFirstOutputSlot = <var>0</var>;</td></tr>
<tr><th id="3843">3843</th><td>  <b>for</b> (<em>const</em> Edge* e : pred-&gt;out_edges()) {</td></tr>
<tr><th id="3844">3844</th><td>    <b>if</b> (e-&gt;src_output() == kFirstOutputSlot &amp;&amp; e-&gt;dst() != succ) {</td></tr>
<tr><th id="3845">3845</th><td>      <b>return</b> Status(error::Code::INVALID_ARGUMENT,</td></tr>
<tr><th id="3846">3846</th><td>                    <q>"Conv2D does not feed to BiasAdd, or "</q></td></tr>
<tr><th id="3847">3847</th><td>                    <q>"it feeds BiasAdd but has multiple outputs. "</q></td></tr>
<tr><th id="3848">3848</th><td>                    <q>"Will skip node merge optimization"</q>);</td></tr>
<tr><th id="3849">3849</th><td>    }</td></tr>
<tr><th id="3850">3850</th><td>  }</td></tr>
<tr><th id="3851">3851</th><td></td></tr>
<tr><th id="3852">3852</th><td>  <i>// 2. Get inputs from both the nodes.</i></td></tr>
<tr><th id="3853">3853</th><td><i>  // Find the 2 inputs from the conv and the bias from the add Bias.</i></td></tr>
<tr><th id="3854">3854</th><td><i>  // Get operand 0, 1 of conv2D.</i></td></tr>
<tr><th id="3855">3855</th><td>  CHECK_EQ(pred-&gt;in_edges().size(), <var>2</var>);  <i>// Conv2D must have 2 inputs.</i></td></tr>
<tr><th id="3856">3856</th><td>  <i>// Get operand 1 of add_bias</i></td></tr>
<tr><th id="3857">3857</th><td><i>  // BiasAdd must have 2 inputs: Conv, bias</i></td></tr>
<tr><th id="3858">3858</th><td>  CHECK_EQ(succ-&gt;in_edges().size(), <var>2</var>);</td></tr>
<tr><th id="3859">3859</th><td></td></tr>
<tr><th id="3860">3860</th><td>  <i>// We will use the node name of BiasAdd as the name of new node</i></td></tr>
<tr><th id="3861">3861</th><td><i>  // Build new node. We use same name as original node, but change the op</i></td></tr>
<tr><th id="3862">3862</th><td><i>  // name.</i></td></tr>
<tr><th id="3863">3863</th><td>  NodeBuilder nb(succ-&gt;name(), csinfo_.conv2d_with_bias);</td></tr>
<tr><th id="3864">3864</th><td>  nb.Input(pred_in[<var>0</var>].first, pred_in[<var>0</var>].second);  <i>// In1 of Conv2D</i></td></tr>
<tr><th id="3865">3865</th><td>  <i>// pred_in[1] will be 2nd Tensorflow tensor for Conv2D.</i></td></tr>
<tr><th id="3866">3866</th><td>  nb.Input(pred_in[<var>1</var>].first, pred_in[<var>1</var>].second);  <i>// In2 of Conv2D</i></td></tr>
<tr><th id="3867">3867</th><td>  <i>// In1 of BiasAdd is same as output of Conv2D.</i></td></tr>
<tr><th id="3868">3868</th><td>  nb.Input(succ_in[<var>1</var>].first, succ_in[<var>1</var>].second);  <i>// In2 of BiasAdd</i></td></tr>
<tr><th id="3869">3869</th><td></td></tr>
<tr><th id="3870">3870</th><td>  <i>// Copy attributes from Conv2D to Conv2DWithBias.</i></td></tr>
<tr><th id="3871">3871</th><td>  CopyAttrsConv2D(<b>const_cast</b>&lt;<em>const</em> Node*&gt;(pred), &amp;nb);</td></tr>
<tr><th id="3872">3872</th><td></td></tr>
<tr><th id="3873">3873</th><td>  <i>// Copy the device assigned to old node to new node.</i></td></tr>
<tr><th id="3874">3874</th><td>  nb.Device(succ-&gt;def().device());</td></tr>
<tr><th id="3875">3875</th><td></td></tr>
<tr><th id="3876">3876</th><td>  <i>// Create node.</i></td></tr>
<tr><th id="3877">3877</th><td>  Node* new_node;</td></tr>
<tr><th id="3878">3878</th><td>  TF_CHECK_OK(nb.Finalize(&amp;**g, &amp;new_node));</td></tr>
<tr><th id="3879">3879</th><td>  CHECK_NOTNULL(new_node);</td></tr>
<tr><th id="3880">3880</th><td></td></tr>
<tr><th id="3881">3881</th><td>  <i>// Incoming data edges from 'pred' node and 'succ' node to new 'new_node'</i></td></tr>
<tr><th id="3882">3882</th><td><i>  // node are already copied in BuildNode. We handle control edges now.</i></td></tr>
<tr><th id="3883">3883</th><td>  <b>for</b> (<em>const</em> Edge* e : pred-&gt;in_edges()) {</td></tr>
<tr><th id="3884">3884</th><td>    <b>if</b> (e-&gt;IsControlEdge()) {</td></tr>
<tr><th id="3885">3885</th><td>      <i>// Allow duplicate while adding control edge as it would fail (return</i></td></tr>
<tr><th id="3886">3886</th><td><i>      // NULL) if we try to add duplicate edge.</i></td></tr>
<tr><th id="3887">3887</th><td>      CHECK_NOTNULL((*g)-&gt;AddControlEdge(e-&gt;src(), new_node, <b>true</b>));</td></tr>
<tr><th id="3888">3888</th><td>    }</td></tr>
<tr><th id="3889">3889</th><td>  }</td></tr>
<tr><th id="3890">3890</th><td>  <b>for</b> (<em>const</em> Edge* e : succ-&gt;in_edges()) {</td></tr>
<tr><th id="3891">3891</th><td>    <b>if</b> (e-&gt;IsControlEdge()) {</td></tr>
<tr><th id="3892">3892</th><td>      <i>// Allow duplicate while adding control edge as it would fail (return</i></td></tr>
<tr><th id="3893">3893</th><td><i>      // NULL) if we try to add duplicate edge.</i></td></tr>
<tr><th id="3894">3894</th><td>      CHECK_NOTNULL((*g)-&gt;AddControlEdge(e-&gt;src(), new_node, <b>true</b>));</td></tr>
<tr><th id="3895">3895</th><td>    }</td></tr>
<tr><th id="3896">3896</th><td>  }</td></tr>
<tr><th id="3897">3897</th><td></td></tr>
<tr><th id="3898">3898</th><td>  <i>// Incoming edges are fixed, we will fix the outgoing edges now.</i></td></tr>
<tr><th id="3899">3899</th><td><i>  // First, we will fix outgoing control edges from 'pred' node.</i></td></tr>
<tr><th id="3900">3900</th><td>  <b>for</b> (<em>const</em> Edge* e : pred-&gt;out_edges()) {</td></tr>
<tr><th id="3901">3901</th><td>    <b>if</b> (e-&gt;IsControlEdge()) {</td></tr>
<tr><th id="3902">3902</th><td>      <i>// Allow duplicate while adding control edge as it would fail (return</i></td></tr>
<tr><th id="3903">3903</th><td><i>      // NULL) if we try to add duplicate edge.</i></td></tr>
<tr><th id="3904">3904</th><td>      CHECK_NOTNULL((*g)-&gt;AddControlEdge(new_node, e-&gt;dst(), <b>true</b>));</td></tr>
<tr><th id="3905">3905</th><td>    }</td></tr>
<tr><th id="3906">3906</th><td>  }</td></tr>
<tr><th id="3907">3907</th><td></td></tr>
<tr><th id="3908">3908</th><td>  <i>// Second, we will fix outgoing control and data edges from 'succ' node.</i></td></tr>
<tr><th id="3909">3909</th><td>  <b>for</b> (<em>const</em> Edge* e : succ-&gt;out_edges()) {</td></tr>
<tr><th id="3910">3910</th><td>    <b>if</b> (e-&gt;IsControlEdge()) {</td></tr>
<tr><th id="3911">3911</th><td>      <i>// Allow duplicate while adding control edge as it would fail (return</i></td></tr>
<tr><th id="3912">3912</th><td><i>      // NULL) if we try to add duplicate edge.</i></td></tr>
<tr><th id="3913">3913</th><td>      CHECK_NOTNULL((*g)-&gt;AddControlEdge(new_node, e-&gt;dst(), <b>true</b>));</td></tr>
<tr><th id="3914">3914</th><td>    } <b>else</b> {</td></tr>
<tr><th id="3915">3915</th><td>      <i>// BiasAdd has only 1 output (at slot 0) and merged node also has only 1</i></td></tr>
<tr><th id="3916">3916</th><td><i>      // output (at slot 0).</i></td></tr>
<tr><th id="3917">3917</th><td>      <em>const</em> <em>int</em> kConv2DWithBiasOutputSlot = <var>0</var>;</td></tr>
<tr><th id="3918">3918</th><td>      CHECK_NOTNULL((*g)-&gt;AddEdge(new_node, kConv2DWithBiasOutputSlot, e-&gt;dst(),</td></tr>
<tr><th id="3919">3919</th><td>                                  e-&gt;dst_input()));</td></tr>
<tr><th id="3920">3920</th><td>    }</td></tr>
<tr><th id="3921">3921</th><td>  }</td></tr>
<tr><th id="3922">3922</th><td></td></tr>
<tr><th id="3923">3923</th><td>  <i>// Copy device assigned to old node to new node.</i></td></tr>
<tr><th id="3924">3924</th><td><i>  // It's ok to use pred or succ as we have enforced a check that</i></td></tr>
<tr><th id="3925">3925</th><td><i>  // both have same device assigned.</i></td></tr>
<tr><th id="3926">3926</th><td>  new_node-&gt;set_assigned_device_name(pred-&gt;assigned_device_name());</td></tr>
<tr><th id="3927">3927</th><td></td></tr>
<tr><th id="3928">3928</th><td>  VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: Merged old node:"</q> &lt;&lt; pred-&gt;DebugString()</td></tr>
<tr><th id="3929">3929</th><td>          &lt;&lt; <q>", and node: "</q> &lt;&lt; succ-&gt;DebugString()</td></tr>
<tr><th id="3930">3930</th><td>          &lt;&lt; <q>", into node:"</q> &lt;&lt; new_node-&gt;DebugString();</td></tr>
<tr><th id="3931">3931</th><td></td></tr>
<tr><th id="3932">3932</th><td>  (*g)-&gt;RemoveNode(succ);</td></tr>
<tr><th id="3933">3933</th><td>  (*g)-&gt;RemoveNode(pred);</td></tr>
<tr><th id="3934">3934</th><td></td></tr>
<tr><th id="3935">3935</th><td>  <b>return</b> Status::OK();</td></tr>
<tr><th id="3936">3936</th><td>}</td></tr>
<tr><th id="3937">3937</th><td></td></tr>
<tr><th id="3938">3938</th><td>Status MklLayoutRewritePass::MergeConv2DBackpropFilterWithBiasAddGrad(</td></tr>
<tr><th id="3939">3939</th><td>    std::unique_ptr&lt;Graph&gt;* g, Node* m, Node* n) {</td></tr>
<tr><th id="3940">3940</th><td>  CHECK_EQ(((m-&gt;type_string() == csinfo_.bias_add_grad &amp;&amp;</td></tr>
<tr><th id="3941">3941</th><td>             n-&gt;type_string() == csinfo_.conv2d_grad_filter)) ||</td></tr>
<tr><th id="3942">3942</th><td>               ((n-&gt;type_string() == csinfo_.bias_add_grad &amp;&amp;</td></tr>
<tr><th id="3943">3943</th><td>                 m-&gt;type_string() == csinfo_.conv2d_grad_filter)),</td></tr>
<tr><th id="3944">3944</th><td>           <b>true</b>);</td></tr>
<tr><th id="3945">3945</th><td></td></tr>
<tr><th id="3946">3946</th><td>  <i>// If 'm' is BiasAddGrad, then 'n' is BackpropFilter.</i></td></tr>
<tr><th id="3947">3947</th><td>  Node* badd = m-&gt;type_string() == csinfo_.bias_add_grad ? m : n;</td></tr>
<tr><th id="3948">3948</th><td>  Node* fltr = m-&gt;type_string() == csinfo_.bias_add_grad ? n : m;</td></tr>
<tr><th id="3949">3949</th><td></td></tr>
<tr><th id="3950">3950</th><td>  <i>// Sanity check for attributes from input nodes.</i></td></tr>
<tr><th id="3951">3951</th><td>  DataType T_b, T_f;</td></tr>
<tr><th id="3952">3952</th><td>  string data_format_b, data_format_f;</td></tr>
<tr><th id="3953">3953</th><td>  TF_CHECK_OK(GetNodeAttr(badd-&gt;def(), <q>"T"</q>, &amp;T_b));</td></tr>
<tr><th id="3954">3954</th><td>  TF_CHECK_OK(GetNodeAttr(fltr-&gt;def(), <q>"T"</q>, &amp;T_f));</td></tr>
<tr><th id="3955">3955</th><td>  TF_CHECK_OK(GetNodeAttr(badd-&gt;def(), <q>"data_format"</q>, &amp;data_format_b));</td></tr>
<tr><th id="3956">3956</th><td>  TF_CHECK_OK(GetNodeAttr(fltr-&gt;def(), <q>"data_format"</q>, &amp;data_format_f));</td></tr>
<tr><th id="3957">3957</th><td>  <b>if</b> (data_format_b != data_format_f || T_b != T_f ||</td></tr>
<tr><th id="3958">3958</th><td>      badd-&gt;assigned_device_name() != fltr-&gt;assigned_device_name() ||</td></tr>
<tr><th id="3959">3959</th><td>      badd-&gt;def().device() != fltr-&gt;def().device()) {</td></tr>
<tr><th id="3960">3960</th><td>    <b>return</b> Status(error::Code::INVALID_ARGUMENT,</td></tr>
<tr><th id="3961">3961</th><td>                  <q>"data_format or T attribute or devices of "</q></td></tr>
<tr><th id="3962">3962</th><td>                  <q>"Conv2DBackpropFilter and BiasAddGrad do not match. "</q></td></tr>
<tr><th id="3963">3963</th><td>                  <q>"Will skip node merge optimization"</q>);</td></tr>
<tr><th id="3964">3964</th><td>  }</td></tr>
<tr><th id="3965">3965</th><td></td></tr>
<tr><th id="3966">3966</th><td>  <i>// We will use the node name of Conv2DBackpropFilter as the name of new node.</i></td></tr>
<tr><th id="3967">3967</th><td><i>  // This is because BackpropFilterWithBias is going to emit bias output also.</i></td></tr>
<tr><th id="3968">3968</th><td>  NodeBuilder nb(fltr-&gt;name(), csinfo_.conv2d_grad_filter_with_bias);</td></tr>
<tr><th id="3969">3969</th><td>  <i>// Since Conv2DBackpropFilterWithBias has same number of inputs as</i></td></tr>
<tr><th id="3970">3970</th><td><i>  // Conv2DBackpropFilter, we can just copy input edges directly. We dont need</i></td></tr>
<tr><th id="3971">3971</th><td><i>  // to copy any data input of BiasAddGrad because that input also goes to</i></td></tr>
<tr><th id="3972">3972</th><td><i>  // Conv2DBackpropFilter.</i></td></tr>
<tr><th id="3973">3973</th><td>  <em>const</em> <em>int</em> fltr_ins = fltr-&gt;num_inputs();</td></tr>
<tr><th id="3974">3974</th><td>  gtl::InlinedVector&lt;Node*, <var>4</var>&gt; fltr_control_edges;</td></tr>
<tr><th id="3975">3975</th><td>  gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt; fltr_in_edges(fltr_ins);</td></tr>
<tr><th id="3976">3976</th><td>  FillInputs(fltr, &amp;fltr_control_edges, &amp;fltr_in_edges);</td></tr>
<tr><th id="3977">3977</th><td>  <b>for</b> (<em>int</em> idx = <var>0</var>; idx &lt; fltr_ins; idx++) {</td></tr>
<tr><th id="3978">3978</th><td>    nb.Input(fltr_in_edges[idx].first, fltr_in_edges[idx].second);</td></tr>
<tr><th id="3979">3979</th><td>  }</td></tr>
<tr><th id="3980">3980</th><td></td></tr>
<tr><th id="3981">3981</th><td>  <i>// Copy attributes from Conv2DBackpropFilter.</i></td></tr>
<tr><th id="3982">3982</th><td>  CopyAttrsConv2D(<b>const_cast</b>&lt;<em>const</em> Node*&gt;(fltr), &amp;nb);</td></tr>
<tr><th id="3983">3983</th><td></td></tr>
<tr><th id="3984">3984</th><td>  <i>// Copy the device assigned to old node to new node.</i></td></tr>
<tr><th id="3985">3985</th><td>  nb.Device(fltr-&gt;def().device());</td></tr>
<tr><th id="3986">3986</th><td></td></tr>
<tr><th id="3987">3987</th><td>  <i>// Create node.</i></td></tr>
<tr><th id="3988">3988</th><td>  Node* new_node;</td></tr>
<tr><th id="3989">3989</th><td>  TF_CHECK_OK(nb.Finalize(&amp;**g, &amp;new_node));</td></tr>
<tr><th id="3990">3990</th><td>  CHECK_NOTNULL(new_node);</td></tr>
<tr><th id="3991">3991</th><td></td></tr>
<tr><th id="3992">3992</th><td>  <i>// Incoming data edges from BiasAddGrad node and Conv2DBackpropFilter node to</i></td></tr>
<tr><th id="3993">3993</th><td><i>  // new 'new_node' node are already copied in BuildNode. We handle control</i></td></tr>
<tr><th id="3994">3994</th><td><i>  // edges now.</i></td></tr>
<tr><th id="3995">3995</th><td>  <b>for</b> (<em>const</em> Edge* e : badd-&gt;in_edges()) {</td></tr>
<tr><th id="3996">3996</th><td>    <b>if</b> (e-&gt;IsControlEdge()) {</td></tr>
<tr><th id="3997">3997</th><td>      <i>// Allow duplicate while adding control edge as it would fail (return</i></td></tr>
<tr><th id="3998">3998</th><td><i>      // NULL) if we try to add duplicate edge.</i></td></tr>
<tr><th id="3999">3999</th><td>      CHECK_NOTNULL((*g)-&gt;AddControlEdge(e-&gt;src(), new_node, <b>true</b>));</td></tr>
<tr><th id="4000">4000</th><td>    }</td></tr>
<tr><th id="4001">4001</th><td>  }</td></tr>
<tr><th id="4002">4002</th><td>  <b>for</b> (<em>const</em> Edge* e : fltr-&gt;in_edges()) {</td></tr>
<tr><th id="4003">4003</th><td>    <b>if</b> (e-&gt;IsControlEdge()) {</td></tr>
<tr><th id="4004">4004</th><td>      <i>// Allow duplicate while adding control edge as it would fail (return</i></td></tr>
<tr><th id="4005">4005</th><td><i>      // NULL) if we try to add duplicate edge.</i></td></tr>
<tr><th id="4006">4006</th><td>      CHECK_NOTNULL((*g)-&gt;AddControlEdge(e-&gt;src(), new_node, <b>true</b>));</td></tr>
<tr><th id="4007">4007</th><td>    }</td></tr>
<tr><th id="4008">4008</th><td>  }</td></tr>
<tr><th id="4009">4009</th><td></td></tr>
<tr><th id="4010">4010</th><td>  <i>// Incoming edges are fixed, we will fix the outgoing edges now.</i></td></tr>
<tr><th id="4011">4011</th><td><i>  // First, we will fix outgoing control edges from 'badd' node.</i></td></tr>
<tr><th id="4012">4012</th><td><i>  // Conv2DBackpropFilter has 1 output -- filter_grad.</i></td></tr>
<tr><th id="4013">4013</th><td><i>  // Conv2DBackpropFilterWithBias has 2 outputs -- filter_grad and</i></td></tr>
<tr><th id="4014">4014</th><td><i>  // bias_grad. But filter_grad is at same slot number (0) in both the</i></td></tr>
<tr><th id="4015">4015</th><td><i>  // nodes. bias_grad is at slot number 1 in Conv2DBackpropFilterWithBias, while</i></td></tr>
<tr><th id="4016">4016</th><td><i>  // it is at slot number 0 in BiasAddGrad.</i></td></tr>
<tr><th id="4017">4017</th><td>  <em>const</em> <em>int</em> kMergedNodeFilterGradOutputIdx = <var>0</var>;</td></tr>
<tr><th id="4018">4018</th><td>  <em>const</em> <em>int</em> kMergedNodeBiasGradOutputIdx = <var>1</var>;</td></tr>
<tr><th id="4019">4019</th><td></td></tr>
<tr><th id="4020">4020</th><td>  <b>for</b> (<em>const</em> Edge* e : badd-&gt;out_edges()) {</td></tr>
<tr><th id="4021">4021</th><td>    <b>if</b> (e-&gt;IsControlEdge()) {</td></tr>
<tr><th id="4022">4022</th><td>      <i>// Allow duplicate while adding control edge as it would fail (return</i></td></tr>
<tr><th id="4023">4023</th><td><i>      // NULL) if we try to add duplicate edge.</i></td></tr>
<tr><th id="4024">4024</th><td>      CHECK_NOTNULL((*g)-&gt;AddControlEdge(new_node, e-&gt;dst(), <b>true</b>));</td></tr>
<tr><th id="4025">4025</th><td>    } <b>else</b> {</td></tr>
<tr><th id="4026">4026</th><td>      CHECK_NOTNULL((*g)-&gt;AddEdge(new_node, kMergedNodeBiasGradOutputIdx,</td></tr>
<tr><th id="4027">4027</th><td>                                  e-&gt;dst(), e-&gt;dst_input()));</td></tr>
<tr><th id="4028">4028</th><td>    }</td></tr>
<tr><th id="4029">4029</th><td>  }</td></tr>
<tr><th id="4030">4030</th><td></td></tr>
<tr><th id="4031">4031</th><td>  <i>// Second, we will fix outgoing control and data edges from 'fltr' node.</i></td></tr>
<tr><th id="4032">4032</th><td>  <b>for</b> (<em>const</em> Edge* e : fltr-&gt;out_edges()) {</td></tr>
<tr><th id="4033">4033</th><td>    <b>if</b> (e-&gt;IsControlEdge()) {</td></tr>
<tr><th id="4034">4034</th><td>      <i>// We allow duplicate edge for this case since we already add control</i></td></tr>
<tr><th id="4035">4035</th><td><i>      // edge from new_node in line 3990. Line below could be adding same</i></td></tr>
<tr><th id="4036">4036</th><td><i>      // edge to same destination again. In such case, if we do not allow</i></td></tr>
<tr><th id="4037">4037</th><td><i>      // duplicate edge, then this call will fail.</i></td></tr>
<tr><th id="4038">4038</th><td>      CHECK_NOTNULL((*g)-&gt;AddControlEdge(new_node, e-&gt;dst(), <b>true</b>));</td></tr>
<tr><th id="4039">4039</th><td>    } <b>else</b> {</td></tr>
<tr><th id="4040">4040</th><td>      CHECK_NOTNULL((*g)-&gt;AddEdge(new_node, kMergedNodeFilterGradOutputIdx,</td></tr>
<tr><th id="4041">4041</th><td>                                  e-&gt;dst(), e-&gt;dst_input()));</td></tr>
<tr><th id="4042">4042</th><td>    }</td></tr>
<tr><th id="4043">4043</th><td>  }</td></tr>
<tr><th id="4044">4044</th><td></td></tr>
<tr><th id="4045">4045</th><td>  <i>// Copy device assigned to old node to new node.</i></td></tr>
<tr><th id="4046">4046</th><td><i>  // It's ok to use badd or fltr as we have enforced a check that</i></td></tr>
<tr><th id="4047">4047</th><td><i>  // both have same device assigned.</i></td></tr>
<tr><th id="4048">4048</th><td>  new_node-&gt;set_assigned_device_name(badd-&gt;assigned_device_name());</td></tr>
<tr><th id="4049">4049</th><td></td></tr>
<tr><th id="4050">4050</th><td>  VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: Merged old node:"</q> &lt;&lt; badd-&gt;DebugString()</td></tr>
<tr><th id="4051">4051</th><td>          &lt;&lt; <q>", and node: "</q> &lt;&lt; fltr-&gt;DebugString()</td></tr>
<tr><th id="4052">4052</th><td>          &lt;&lt; <q>", into node:"</q> &lt;&lt; new_node-&gt;DebugString();</td></tr>
<tr><th id="4053">4053</th><td></td></tr>
<tr><th id="4054">4054</th><td>  (*g)-&gt;RemoveNode(badd);</td></tr>
<tr><th id="4055">4055</th><td>  (*g)-&gt;RemoveNode(fltr);</td></tr>
<tr><th id="4056">4056</th><td></td></tr>
<tr><th id="4057">4057</th><td>  <b>return</b> Status::OK();</td></tr>
<tr><th id="4058">4058</th><td>}</td></tr>
<tr><th id="4059">4059</th><td></td></tr>
<tr><th id="4060">4060</th><td>Status MklLayoutRewritePass::MergeNode(std::unique_ptr&lt;Graph&gt;* g, Node* m,</td></tr>
<tr><th id="4061">4061</th><td>                                       Node* n) {</td></tr>
<tr><th id="4062">4062</th><td>  CHECK_NOTNULL(m);</td></tr>
<tr><th id="4063">4063</th><td>  CHECK_NOTNULL(n);</td></tr>
<tr><th id="4064">4064</th><td></td></tr>
<tr><th id="4065">4065</th><td>  <b>if</b> (((m-&gt;type_string() == csinfo_.bias_add &amp;&amp;</td></tr>
<tr><th id="4066">4066</th><td>        n-&gt;type_string() == csinfo_.conv2d)) ||</td></tr>
<tr><th id="4067">4067</th><td>      ((n-&gt;type_string() == csinfo_.bias_add &amp;&amp;</td></tr>
<tr><th id="4068">4068</th><td>        m-&gt;type_string() == csinfo_.conv2d))) {</td></tr>
<tr><th id="4069">4069</th><td>    <b>return</b> <b>this</b>-&gt;MergeConv2DWithBiasAdd(g, m, n);</td></tr>
<tr><th id="4070">4070</th><td>  }</td></tr>
<tr><th id="4071">4071</th><td></td></tr>
<tr><th id="4072">4072</th><td>  <b>if</b> (((m-&gt;type_string() == csinfo_.bias_add_grad &amp;&amp;</td></tr>
<tr><th id="4073">4073</th><td>        n-&gt;type_string() == csinfo_.conv2d_grad_filter)) ||</td></tr>
<tr><th id="4074">4074</th><td>      ((n-&gt;type_string() == csinfo_.bias_add_grad &amp;&amp;</td></tr>
<tr><th id="4075">4075</th><td>        m-&gt;type_string() == csinfo_.conv2d_grad_filter))) {</td></tr>
<tr><th id="4076">4076</th><td>    <b>return</b> <b>this</b>-&gt;MergeConv2DBackpropFilterWithBiasAddGrad(g, m, n);</td></tr>
<tr><th id="4077">4077</th><td>  }</td></tr>
<tr><th id="4078">4078</th><td></td></tr>
<tr><th id="4079">4079</th><td>  <b>return</b> Status(error::Code::UNIMPLEMENTED,</td></tr>
<tr><th id="4080">4080</th><td>                <q>"Unimplemented case for node merge optimization."</q>);</td></tr>
<tr><th id="4081">4081</th><td>}</td></tr>
<tr><th id="4082">4082</th><td></td></tr>
<tr><th id="4083">4083</th><td><i>//////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="4084">4084</th><td><i>//           Helper functions for node rewrite</i></td></tr>
<tr><th id="4085">4085</th><td><i>//////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="4086">4086</th><td></td></tr>
<tr><th id="4087">4087</th><td>Status MklLayoutRewritePass::RewriteNode(std::unique_ptr&lt;Graph&gt;* g,</td></tr>
<tr><th id="4088">4088</th><td>                                         Node* orig_node,</td></tr>
<tr><th id="4089">4089</th><td>                                         <em>const</em> RewriteInfo* ri) {</td></tr>
<tr><th id="4090">4090</th><td>  CHECK_NOTNULL(ri);</td></tr>
<tr><th id="4091">4091</th><td>  CHECK_NOTNULL(orig_node);</td></tr>
<tr><th id="4092">4092</th><td></td></tr>
<tr><th id="4093">4093</th><td>  VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: Original node:"</q> &lt;&lt; orig_node-&gt;DebugString();</td></tr>
<tr><th id="4094">4094</th><td></td></tr>
<tr><th id="4095">4095</th><td>  <i>// Get all inputs.</i></td></tr>
<tr><th id="4096">4096</th><td>  <em>int</em> num_inputs = orig_node-&gt;in_edges().size();</td></tr>
<tr><th id="4097">4097</th><td></td></tr>
<tr><th id="4098">4098</th><td>  <i>// Drop count for control edges from inputs</i></td></tr>
<tr><th id="4099">4099</th><td>  <b>for</b> (<em>const</em> Edge* e : orig_node-&gt;in_edges()) {</td></tr>
<tr><th id="4100">4100</th><td>    <b>if</b> (e-&gt;IsControlEdge()) {</td></tr>
<tr><th id="4101">4101</th><td>      num_inputs--;</td></tr>
<tr><th id="4102">4102</th><td>    }</td></tr>
<tr><th id="4103">4103</th><td>  }</td></tr>
<tr><th id="4104">4104</th><td></td></tr>
<tr><th id="4105">4105</th><td>  gtl::InlinedVector&lt;Node*, <var>4</var>&gt; control_edges;</td></tr>
<tr><th id="4106">4106</th><td>  gtl::InlinedVector&lt;std::pair&lt;Node*, <em>int</em>&gt;, <var>4</var>&gt; inputs(num_inputs);</td></tr>
<tr><th id="4107">4107</th><td>  FillInputs(orig_node, &amp;control_edges, &amp;inputs);</td></tr>
<tr><th id="4108">4108</th><td></td></tr>
<tr><th id="4109">4109</th><td>  <i>// Build new node. We use same name as original node, but change the op name.</i></td></tr>
<tr><th id="4110">4110</th><td>  NodeBuilder nb(orig_node-&gt;name().c_str(), ri-&gt;new_name.c_str());</td></tr>
<tr><th id="4111">4111</th><td>  <i>// Copy user-specified device assigned to original node to new node.</i></td></tr>
<tr><th id="4112">4112</th><td>  nb.Device(orig_node-&gt;def().device());</td></tr>
<tr><th id="4113">4113</th><td>  <i>// Set up new inputs to the rewritten node.</i></td></tr>
<tr><th id="4114">4114</th><td>  Status s = SetUpInputs(g, inputs, &amp;nb, orig_node);</td></tr>
<tr><th id="4115">4115</th><td>  <b>if</b> (s != Status::OK()) {</td></tr>
<tr><th id="4116">4116</th><td>    <b>return</b> s;</td></tr>
<tr><th id="4117">4117</th><td>  }</td></tr>
<tr><th id="4118">4118</th><td></td></tr>
<tr><th id="4119">4119</th><td>  ri-&gt;copy_attrs(<b>const_cast</b>&lt;<em>const</em> Node*&gt;(orig_node), &amp;nb);</td></tr>
<tr><th id="4120">4120</th><td>  <i>// Set the Mkl layer label for this op.</i></td></tr>
<tr><th id="4121">4121</th><td>  nb.Attr(<q>"_kernel"</q>, mkl_op_registry::kMklOpLabel);</td></tr>
<tr><th id="4122">4122</th><td></td></tr>
<tr><th id="4123">4123</th><td>  <i>// Finalize graph and get new node.</i></td></tr>
<tr><th id="4124">4124</th><td>  Node* new_node = <b>nullptr</b>;</td></tr>
<tr><th id="4125">4125</th><td>  TF_CHECK_OK(nb.Finalize(&amp;**g, &amp;new_node));</td></tr>
<tr><th id="4126">4126</th><td>  CHECK_NOTNULL(new_node);</td></tr>
<tr><th id="4127">4127</th><td></td></tr>
<tr><th id="4128">4128</th><td>  <i>// Incoming data edges from 'orig_node' node to new 'new_node' node are</i></td></tr>
<tr><th id="4129">4129</th><td><i>  // already copied in BuildNode. We need to handle control edges now.</i></td></tr>
<tr><th id="4130">4130</th><td>  <b>for</b> (<em>const</em> Edge* e : orig_node-&gt;in_edges()) {</td></tr>
<tr><th id="4131">4131</th><td>    <b>if</b> (e-&gt;IsControlEdge()) {</td></tr>
<tr><th id="4132">4132</th><td>      <i>// Allow duplicate while adding control edge as it would fail (return</i></td></tr>
<tr><th id="4133">4133</th><td><i>      // NULL) if we try to add duplicate edge.</i></td></tr>
<tr><th id="4134">4134</th><td>      CHECK_NOTNULL((*g)-&gt;AddControlEdge(e-&gt;src(), new_node, <b>true</b>));</td></tr>
<tr><th id="4135">4135</th><td>    }</td></tr>
<tr><th id="4136">4136</th><td>  }</td></tr>
<tr><th id="4137">4137</th><td></td></tr>
<tr><th id="4138">4138</th><td>  <i>// Copy outgoing edges from 'orig_node' node to new</i></td></tr>
<tr><th id="4139">4139</th><td><i>  // 'new_node' node, since the output also follows same ordering among</i></td></tr>
<tr><th id="4140">4140</th><td><i>  // Tensorflow tensors and Mkl tensors. We need to connect Tensorflow</i></td></tr>
<tr><th id="4141">4141</th><td><i>  // tensors appropriately. Specifically, nth output of the original node</i></td></tr>
<tr><th id="4142">4142</th><td><i>  // will become 2*nth output of the Mkl node for the interleaved ordering</i></td></tr>
<tr><th id="4143">4143</th><td><i>  // of the tensors. For the contiguous ordering of the tensors, it will be n.</i></td></tr>
<tr><th id="4144">4144</th><td><i>  // GetTensorDataIndex provides this mapping function.</i></td></tr>
<tr><th id="4145">4145</th><td>  <b>for</b> (<em>const</em> Edge* e : orig_node-&gt;out_edges()) {</td></tr>
<tr><th id="4146">4146</th><td>    <b>if</b> (e-&gt;IsControlEdge()) {</td></tr>
<tr><th id="4147">4147</th><td>      <i>// Allow duplicate while adding control edge as it would fail (return</i></td></tr>
<tr><th id="4148">4148</th><td><i>      // NULL) if we try to add duplicate edge.</i></td></tr>
<tr><th id="4149">4149</th><td>      CHECK_NOTNULL((*g)-&gt;AddControlEdge(new_node, e-&gt;dst(), <b>true</b>));</td></tr>
<tr><th id="4150">4150</th><td>    } <b>else</b> {</td></tr>
<tr><th id="4151">4151</th><td>      CHECK_NOTNULL((*g)-&gt;AddEdge(</td></tr>
<tr><th id="4152">4152</th><td>          new_node,</td></tr>
<tr><th id="4153">4153</th><td>          GetTensorDataIndex(e-&gt;src_output(), e-&gt;src()-&gt;num_outputs()),</td></tr>
<tr><th id="4154">4154</th><td>          e-&gt;dst(), e-&gt;dst_input()));</td></tr>
<tr><th id="4155">4155</th><td>    }</td></tr>
<tr><th id="4156">4156</th><td>  }</td></tr>
<tr><th id="4157">4157</th><td></td></tr>
<tr><th id="4158">4158</th><td>  <i>// Copy the runtime device assigned from original code to new node.</i></td></tr>
<tr><th id="4159">4159</th><td>  new_node-&gt;set_assigned_device_name(orig_node-&gt;assigned_device_name());</td></tr>
<tr><th id="4160">4160</th><td></td></tr>
<tr><th id="4161">4161</th><td>  <i>// Delete original node and mark new node as rewritten.</i></td></tr>
<tr><th id="4162">4162</th><td>  (*g)-&gt;RemoveNode(orig_node);</td></tr>
<tr><th id="4163">4163</th><td></td></tr>
<tr><th id="4164">4164</th><td>  VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: New node:"</q> &lt;&lt; new_node-&gt;DebugString();</td></tr>
<tr><th id="4165">4165</th><td>  <b>return</b> Status::OK();</td></tr>
<tr><th id="4166">4166</th><td>}</td></tr>
<tr><th id="4167">4167</th><td></td></tr>
<tr><th id="4168">4168</th><td><em>const</em> MklLayoutRewritePass::RewriteInfo*</td></tr>
<tr><th id="4169">4169</th><td>MklLayoutRewritePass::CheckForNodeRewrite(<em>const</em> Node* n) <em>const</em> {</td></tr>
<tr><th id="4170">4170</th><td>  CHECK_NOTNULL(n);</td></tr>
<tr><th id="4171">4171</th><td></td></tr>
<tr><th id="4172">4172</th><td>  <i>// First check if node along with its type is supported by MKL layer.</i></td></tr>
<tr><th id="4173">4173</th><td><i>  // We do not want to rewrite an op into Mkl op if types are not supported.</i></td></tr>
<tr><th id="4174">4174</th><td><i>  // E.g., MklRelu does not support INT32. So we cannot rewrite Relu to</i></td></tr>
<tr><th id="4175">4175</th><td><i>  // MklRelu if type is INT32.</i></td></tr>
<tr><th id="4176">4176</th><td>  DataType T;</td></tr>
<tr><th id="4177">4177</th><td>  <b>if</b> (!GetNodeAttr(n-&gt;def(), <q>"T"</q>, &amp;T).ok()) {</td></tr>
<tr><th id="4178">4178</th><td>    <b>return</b> <b>nullptr</b>;</td></tr>
<tr><th id="4179">4179</th><td>  }</td></tr>
<tr><th id="4180">4180</th><td></td></tr>
<tr><th id="4181">4181</th><td>  <i>// We make an exception for __MklDummyConv2DWithBias and</i></td></tr>
<tr><th id="4182">4182</th><td><i>  // __MklConv2DBackpropFilterWithBias since their names do not match Mkl node</i></td></tr>
<tr><th id="4183">4183</th><td><i>  // names.</i></td></tr>
<tr><th id="4184">4184</th><td>  <b>if</b> (n-&gt;type_string() != csinfo_.conv2d_with_bias &amp;&amp;</td></tr>
<tr><th id="4185">4185</th><td>      n-&gt;type_string() != csinfo_.conv2d_grad_filter_with_bias &amp;&amp;</td></tr>
<tr><th id="4186">4186</th><td>      !mkl_op_registry::IsMklOp(mkl_op_registry::GetMklOpName(n-&gt;type_string()),</td></tr>
<tr><th id="4187">4187</th><td>                                T)) {</td></tr>
<tr><th id="4188">4188</th><td>    <b>return</b> <b>nullptr</b>;</td></tr>
<tr><th id="4189">4189</th><td>  }</td></tr>
<tr><th id="4190">4190</th><td></td></tr>
<tr><th id="4191">4191</th><td>  <i>// For elementwise node, we reuse the Eigen implementation and pass the MKL</i></td></tr>
<tr><th id="4192">4192</th><td><i>  // metadata tensor through so we can avoid conversions. However, if all</i></td></tr>
<tr><th id="4193">4193</th><td><i>  // incoming edges are in TF format, we don't need all this overhead, so</i></td></tr>
<tr><th id="4194">4194</th><td><i>  // replace the elementwise node only if at least one of its parents is a MKL</i></td></tr>
<tr><th id="4195">4195</th><td><i>  // node.</i></td></tr>
<tr><th id="4196">4196</th><td><i>  //</i></td></tr>
<tr><th id="4197">4197</th><td><i>  // Identity nodes can also skip replacement if they are not being served by</i></td></tr>
<tr><th id="4198">4198</th><td><i>  // any MKL nodes.</i></td></tr>
<tr><th id="4199">4199</th><td><i>  //</i></td></tr>
<tr><th id="4200">4200</th><td><i>  // TODO(vrane): Add implementation for element-wise ops that doesn't reuse</i></td></tr>
<tr><th id="4201">4201</th><td><i>  // eigen code to reduce cross-library dependency.</i></td></tr>
<tr><th id="4202">4202</th><td>  VLOG(<var>1</var>) &lt;&lt; <q>"ELEMENTWISE: checking op: "</q> &lt;&lt; n-&gt;type_string();</td></tr>
<tr><th id="4203">4203</th><td>  <b>if</b> (mkl_op_registry::IsMklElementWiseOp(</td></tr>
<tr><th id="4204">4204</th><td>          mkl_op_registry::GetMklOpName(n-&gt;type_string()), T) ||</td></tr>
<tr><th id="4205">4205</th><td>      n-&gt;type_string().find(<q>"Identity"</q>) != string::npos) {</td></tr>
<tr><th id="4206">4206</th><td>    VLOG(<var>1</var>) &lt;&lt; <q>"ELEMENTWISE: op is elementwise: "</q> &lt;&lt; n-&gt;type_string();</td></tr>
<tr><th id="4207">4207</th><td>    <em>bool</em> incoming_mkl_edge = <b>false</b>;</td></tr>
<tr><th id="4208">4208</th><td>    <em>int</em> num_parent = <var>0</var>;</td></tr>
<tr><th id="4209">4209</th><td>    <b>for</b> (<em>auto</em> parent : n-&gt;in_edges()) {</td></tr>
<tr><th id="4210">4210</th><td>      <b>if</b> (mkl_op_registry::IsMklOp(parent-&gt;src()-&gt;type_string(), T)) {</td></tr>
<tr><th id="4211">4211</th><td>        VLOG(<var>1</var>) &lt;&lt; <q>"ELEMENTWISE: parent "</q> &lt;&lt; num_parent++</td></tr>
<tr><th id="4212">4212</th><td>                &lt;&lt; <q>" is MKL op: "</q> &lt;&lt; parent-&gt;src()-&gt;type_string();</td></tr>
<tr><th id="4213">4213</th><td>        incoming_mkl_edge = <b>true</b>;</td></tr>
<tr><th id="4214">4214</th><td>        <b>break</b>;</td></tr>
<tr><th id="4215">4215</th><td>      } <b>else</b> {</td></tr>
<tr><th id="4216">4216</th><td>        VLOG(<var>1</var>) &lt;&lt; <q>"ELEMENTWISE: parent "</q> &lt;&lt; num_parent++</td></tr>
<tr><th id="4217">4217</th><td>                &lt;&lt; <q>" is NON-MKL op: "</q> &lt;&lt; parent-&gt;src()-&gt;type_string();</td></tr>
<tr><th id="4218">4218</th><td>      }</td></tr>
<tr><th id="4219">4219</th><td>    }</td></tr>
<tr><th id="4220">4220</th><td>    <b>if</b> (incoming_mkl_edge == <b>false</b>) {</td></tr>
<tr><th id="4221">4221</th><td>      VLOG(<var>1</var>) &lt;&lt; <q>"ELEMENTWISE: Skipping replacement of elementwise node which "</q></td></tr>
<tr><th id="4222">4222</th><td>                 <q>"has no MKL "</q></td></tr>
<tr><th id="4223">4223</th><td>                 <q>"parents."</q>;</td></tr>
<tr><th id="4224">4224</th><td>      <b>return</b> <b>nullptr</b>;</td></tr>
<tr><th id="4225">4225</th><td>    } <b>else</b> {</td></tr>
<tr><th id="4226">4226</th><td>      VLOG(<var>1</var>) &lt;&lt; <q>"ELEMENTWISE: Replacing elementwise node "</q> &lt;&lt; n-&gt;type_string()</td></tr>
<tr><th id="4227">4227</th><td>              &lt;&lt; <q>" which has MKL parents"</q>;</td></tr>
<tr><th id="4228">4228</th><td>    }</td></tr>
<tr><th id="4229">4229</th><td>  }</td></tr>
<tr><th id="4230">4230</th><td></td></tr>
<tr><th id="4231">4231</th><td>  <i>// We now check if rewrite rule applies for this op. If rewrite rule passes</i></td></tr>
<tr><th id="4232">4232</th><td><i>  // for this op, then we rewrite it to Mkl op.</i></td></tr>
<tr><th id="4233">4233</th><td><i>  // Find matching RewriteInfo and then check that rewrite rule applies.</i></td></tr>
<tr><th id="4234">4234</th><td>  <b>for</b> (<em>auto</em> ri = rinfo_.cbegin(); ri != rinfo_.cend(); ++ri) {</td></tr>
<tr><th id="4235">4235</th><td>    <b>if</b> (n-&gt;type_string().compare(ri-&gt;name) == <var>0</var> &amp;&amp; ri-&gt;rewrite_rule(n)) {</td></tr>
<tr><th id="4236">4236</th><td>      <b>return</b> &amp;*ri;</td></tr>
<tr><th id="4237">4237</th><td>    }</td></tr>
<tr><th id="4238">4238</th><td>  }</td></tr>
<tr><th id="4239">4239</th><td></td></tr>
<tr><th id="4240">4240</th><td>  <i>// Else return not found.</i></td></tr>
<tr><th id="4241">4241</th><td>  <b>return</b> <b>nullptr</b>;</td></tr>
<tr><th id="4242">4242</th><td>}</td></tr>
<tr><th id="4243">4243</th><td></td></tr>
<tr><th id="4244">4244</th><td><i>///////////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="4245">4245</th><td><i>//              Run function for the pass</i></td></tr>
<tr><th id="4246">4246</th><td><i>///////////////////////////////////////////////////////////////////////////////</i></td></tr>
<tr><th id="4247">4247</th><td></td></tr>
<tr><th id="4248">4248</th><td><em>bool</em> MklLayoutRewritePass::RunPass(std::unique_ptr&lt;Graph&gt;* g) {</td></tr>
<tr><th id="4249">4249</th><td>  <em>bool</em> result = <b>false</b>;</td></tr>
<tr><th id="4250">4250</th><td>  CHECK_NOTNULL(g);</td></tr>
<tr><th id="4251">4251</th><td></td></tr>
<tr><th id="4252">4252</th><td>  DumpGraph(<q>"Before running MklLayoutRewritePass"</q>, &amp;**g);</td></tr>
<tr><th id="4253">4253</th><td></td></tr>
<tr><th id="4254">4254</th><td>  std::vector&lt;Node*&gt; order;</td></tr>
<tr><th id="4255">4255</th><td>  GetReversePostOrder(**g, &amp;order);  <i>// This will give us topological sort.</i></td></tr>
<tr><th id="4256">4256</th><td>  <b>for</b> (Node* n : order) {</td></tr>
<tr><th id="4257">4257</th><td>    <i>// If node is not an op or it cannot run on CPU device, then skip.</i></td></tr>
<tr><th id="4258">4258</th><td>    <b>if</b> (!n-&gt;IsOp() || !CanOpRunOnCPUDevice(n)) {</td></tr>
<tr><th id="4259">4259</th><td>      <b>continue</b>;</td></tr>
<tr><th id="4260">4260</th><td>    }</td></tr>
<tr><th id="4261">4261</th><td></td></tr>
<tr><th id="4262">4262</th><td>    Node* m = <b>nullptr</b>;</td></tr>
<tr><th id="4263">4263</th><td>    <b>if</b> ((m = CheckForNodeMerge(n)) != <b>nullptr</b> &amp;&amp; CanOpRunOnCPUDevice(m)) {</td></tr>
<tr><th id="4264">4264</th><td>      <i>// Check if the node 'n' can be merged with any other node. If it can</i></td></tr>
<tr><th id="4265">4265</th><td><i>      // be 'm' contains the node with which it can be merged.</i></td></tr>
<tr><th id="4266">4266</th><td>      string n1_name = n-&gt;name();</td></tr>
<tr><th id="4267">4267</th><td>      string n2_name = m-&gt;name();</td></tr>
<tr><th id="4268">4268</th><td></td></tr>
<tr><th id="4269">4269</th><td>      VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: Scheduled nodes "</q> &lt;&lt; n1_name &lt;&lt; <q>" and "</q></td></tr>
<tr><th id="4270">4270</th><td>              &lt;&lt; n2_name &lt;&lt; <q>" for merging"</q>;</td></tr>
<tr><th id="4271">4271</th><td></td></tr>
<tr><th id="4272">4272</th><td>      <b>if</b> (MergeNode(g, n, m) == Status::OK()) {</td></tr>
<tr><th id="4273">4273</th><td>        VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: Merged nodes "</q> &lt;&lt; n1_name &lt;&lt; <q>" and "</q></td></tr>
<tr><th id="4274">4274</th><td>                &lt;&lt; n2_name;</td></tr>
<tr><th id="4275">4275</th><td>        result = <b>true</b>;</td></tr>
<tr><th id="4276">4276</th><td>      }</td></tr>
<tr><th id="4277">4277</th><td>    }</td></tr>
<tr><th id="4278">4278</th><td>  }</td></tr>
<tr><th id="4279">4279</th><td></td></tr>
<tr><th id="4280">4280</th><td>  DumpGraph(<q>"After running MklLayoutRewritePass(NodeMerge)"</q>, &amp;**g);</td></tr>
<tr><th id="4281">4281</th><td></td></tr>
<tr><th id="4282">4282</th><td>  order.clear();</td></tr>
<tr><th id="4283">4283</th><td>  GetReversePostOrder(**g, &amp;order);  <i>// This will give us topological sort.</i></td></tr>
<tr><th id="4284">4284</th><td>  <b>for</b> (Node* n : order) {</td></tr>
<tr><th id="4285">4285</th><td>    <i>// If node is not an op or it cannot run on CPU device, then skip.</i></td></tr>
<tr><th id="4286">4286</th><td>    <b>if</b> (!n-&gt;IsOp() || !CanOpRunOnCPUDevice(n)) {</td></tr>
<tr><th id="4287">4287</th><td>      <b>continue</b>;</td></tr>
<tr><th id="4288">4288</th><td>    }</td></tr>
<tr><th id="4289">4289</th><td></td></tr>
<tr><th id="4290">4290</th><td>    <em>const</em> RewriteInfo* ri = <b>nullptr</b>;</td></tr>
<tr><th id="4291">4291</th><td>    <i>// We will first search if node is to be rewritten.</i></td></tr>
<tr><th id="4292">4292</th><td>    <b>if</b> ((ri = CheckForNodeRewrite(n)) != <b>nullptr</b>) {</td></tr>
<tr><th id="4293">4293</th><td>      string node_name = n-&gt;name();</td></tr>
<tr><th id="4294">4294</th><td>      string op_name = n-&gt;type_string();</td></tr>
<tr><th id="4295">4295</th><td></td></tr>
<tr><th id="4296">4296</th><td>      VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: Scheduled node "</q> &lt;&lt; node_name</td></tr>
<tr><th id="4297">4297</th><td>              &lt;&lt; <q>" with op "</q> &lt;&lt; op_name &lt;&lt; <q>" for rewrite using"</q></td></tr>
<tr><th id="4298">4298</th><td>              &lt;&lt; <q>" layout optimization."</q>;</td></tr>
<tr><th id="4299">4299</th><td></td></tr>
<tr><th id="4300">4300</th><td>      <b>if</b> (RewriteNode(g, n, ri) == Status::OK()) {</td></tr>
<tr><th id="4301">4301</th><td>        VLOG(<var>1</var>) &lt;&lt; <q>"MklLayoutRewritePass: rewrote node "</q> &lt;&lt; node_name</td></tr>
<tr><th id="4302">4302</th><td>                &lt;&lt; <q>" with op "</q> &lt;&lt; op_name &lt;&lt; <q>" for Mkl layout optimization."</q>;</td></tr>
<tr><th id="4303">4303</th><td>        result = <b>true</b>;</td></tr>
<tr><th id="4304">4304</th><td>      }</td></tr>
<tr><th id="4305">4305</th><td>    }</td></tr>
<tr><th id="4306">4306</th><td>  }</td></tr>
<tr><th id="4307">4307</th><td></td></tr>
<tr><th id="4308">4308</th><td>  DumpGraph(<q>"After running MklLayoutRewritePass(NodeMerge+Rewrite)"</q>, &amp;**g);</td></tr>
<tr><th id="4309">4309</th><td></td></tr>
<tr><th id="4310">4310</th><td>  <b>return</b> result;</td></tr>
<tr><th id="4311">4311</th><td>}</td></tr>
<tr><th id="4312">4312</th><td></td></tr>
<tr><th id="4313">4313</th><td><em>bool</em> RunMklLayoutRewritePass(std::unique_ptr&lt;Graph&gt;* g) {</td></tr>
<tr><th id="4314">4314</th><td>  <b>return</b> MklLayoutRewritePass().RunPass(g);</td></tr>
<tr><th id="4315">4315</th><td>}</td></tr>
<tr><th id="4316">4316</th><td></td></tr>
<tr><th id="4317">4317</th><td>Status MklLayoutRewritePass::Run(<em>const</em> GraphOptimizationPassOptions&amp; options) {</td></tr>
<tr><th id="4318">4318</th><td>  <b>if</b> (options.graph == <b>nullptr</b> &amp;&amp; options.partition_graphs == <b>nullptr</b>) {</td></tr>
<tr><th id="4319">4319</th><td>    <b>return</b> Status::OK();</td></tr>
<tr><th id="4320">4320</th><td>  }</td></tr>
<tr><th id="4321">4321</th><td></td></tr>
<tr><th id="4322">4322</th><td>  <em>auto</em> process_graph = [&amp;](std::unique_ptr&lt;Graph&gt;* g) {</td></tr>
<tr><th id="4323">4323</th><td>    <i>// Get the ownership of a graph</i></td></tr>
<tr><th id="4324">4324</th><td>    std::unique_ptr&lt;Graph&gt;* ng = std::move(g);</td></tr>
<tr><th id="4325">4325</th><td>    RunPass(ng);</td></tr>
<tr><th id="4326">4326</th><td>    <i>// Return the ownership of a graph back</i></td></tr>
<tr><th id="4327">4327</th><td>    g-&gt;reset(ng-&gt;release());</td></tr>
<tr><th id="4328">4328</th><td>  };</td></tr>
<tr><th id="4329">4329</th><td></td></tr>
<tr><th id="4330">4330</th><td>  <b>if</b> (kMklLayoutRewritePassGroup !=</td></tr>
<tr><th id="4331">4331</th><td>      OptimizationPassRegistry::POST_PARTITIONING) {</td></tr>
<tr><th id="4332">4332</th><td>    <i>// For any pre-partitioning phase, a graph is stored in options.graph.</i></td></tr>
<tr><th id="4333">4333</th><td>    process_graph(options.graph);</td></tr>
<tr><th id="4334">4334</th><td>  } <b>else</b> {</td></tr>
<tr><th id="4335">4335</th><td>    <i>// For post partitioning phase, graphs are stored in</i></td></tr>
<tr><th id="4336">4336</th><td><i>    // options.partition_graphs.</i></td></tr>
<tr><th id="4337">4337</th><td>    <b>for</b> (<em>auto</em>&amp; pg : *options.partition_graphs) {</td></tr>
<tr><th id="4338">4338</th><td>      process_graph(&amp;pg.second);</td></tr>
<tr><th id="4339">4339</th><td>    }</td></tr>
<tr><th id="4340">4340</th><td>  }</td></tr>
<tr><th id="4341">4341</th><td></td></tr>
<tr><th id="4342">4342</th><td>  <b>return</b> Status::OK();</td></tr>
<tr><th id="4343">4343</th><td>}</td></tr>
<tr><th id="4344">4344</th><td><u>#endif  // INTEL_MKL_ML</u></td></tr>
<tr><th id="4345">4345</th><td>}  <i>// namespace tensorflow</i></td></tr>
<tr><th id="4346">4346</th><td></td></tr>
<tr><th id="4347">4347</th><td><u>#<span data-ppcond="18">endif</span></u></td></tr>
<tr><th id="4348">4348</th><td></td></tr>
</table><hr/><p id='footer'>
Generated on <em>2018-Aug-09</em> from project tensorflow revision <em>v1.8</em><br />Powered by <a href='https://woboq.com'><img alt='Woboq' src='https://code.woboq.org/woboq-16.png' width='41' height='16' /></a> <a href='https://code.woboq.org'>Code Browser</a> 2.1
<br/>Generator usage only permitted with license.</p>
</div></body></html>
