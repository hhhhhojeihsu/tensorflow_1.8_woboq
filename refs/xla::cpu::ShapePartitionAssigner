<def f='tensorflow/tensorflow/compiler/xla/service/cpu/shape_partition.h' l='65' ll='78'/>
<size>8</size>
<doc f='tensorflow/tensorflow/compiler/xla/service/cpu/shape_partition.h' l='26'>// ShapePartitionAssigner partitions the most-major dimensions of &apos;shape&apos; such
// that the total partition count &lt;= &apos;target_partition_count&apos;.
//
// Example 1:
//
//   Let &apos;shape&apos; = [8, 16, 32] and &apos;target_partition_count&apos; = 6.
//
//   Because the most-major dimension size is &lt;= &apos;target_partition_count&apos;, we
//   can generate our target number of partitions by partition the most-major
//   dimensions.
//
//   This will result in the following partitions of the most-major dimension:
//
//     [0, 1), [1, 2), [2, 3), [3, 4), [4, 5) [5, 8)
//
//   Note that the last partition has residule because the dimension size is
//   not a multiple of the partition count.
//
//
// Example 2:
//
//   Let &apos;shape&apos; = [8, 16, 32] and &apos;target_partition_count&apos; = 16.
//
//   Because the most-major dimension only has size 8, we must also partition
//   the next most-major dimension to generate the target of 16 partitions.
//   We factor &apos;target_partition_count&apos; by the number of most-major dimensions
//   we need to partition, to get a per-dimension target partition count:
//
//     target_dimension_partition_count = 16 ^ (1 / 2) == 4
//
//   This will result in the following partitions of the most-major dimension:
//
//     [0, 2), [2, 4), [4, 6), [6, 8)
//
//   This will result in the following partitions of the second most-major
//   dimension:
//
//     [0, 4), [4, 8), [8, 12), [12, 16)
//</doc>
<fun r='_ZN3xla3cpu22ShapePartitionAssignerC1ERKNS_5ShapeE'/>
<fun r='_ZN3xla3cpu22ShapePartitionAssigner3RunEx'/>
<fun r='_ZN3xla3cpu22ShapePartitionAssigner22GetTotalPartitionCountERKSt6vectorIxSaIxEE'/>
<mbr r='xla::cpu::ShapePartitionAssigner::shape_' o='0' t='const xla::Shape &amp;'/>
