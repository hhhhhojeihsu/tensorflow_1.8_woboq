<def f='tensorflow/tensorflow/tools/graph_transforms/quantize_nodes.cc' l='262' ll='307' type='tensorflow::Status tensorflow::graph_transforms::RemoveRedundantQuantizations(const tensorflow::GraphDef &amp; input_graph_def, const tensorflow::graph_transforms::TransformFuncContext &amp; context, tensorflow::GraphDef * output_graph_def)'/>
<use f='tensorflow/tensorflow/tools/graph_transforms/quantize_nodes.cc' l='941' u='c' c='_ZN10tensorflow16graph_transforms13QuantizeNodesERKNS_8GraphDefERKNS0_20TransformFuncContextEPS1_'/>
<doc f='tensorflow/tensorflow/tools/graph_transforms/quantize_nodes.cc' l='254'>// Looks for the patterns that indicate there are two eight-bit ops feeding into
// each other, separated by a conversion up to float and back again. These occur
// during the initial conversion of ops to their quantized forms. Because we&apos;re
// only looking at an individual op in that phase and don&apos;t know if its inputs
// and outputs are eight-bit-capable, we start by converting the actual op into
// quantized form, but add float conversions before and after. This pass gets
// rid of those conversions if it turns out we do have adjacent ops capable of
// eight-bit processing.</doc>
