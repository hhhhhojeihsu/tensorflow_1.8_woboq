<dec f='tensorflow/tensorflow/core/kernels/batching_util/shared_batch_scheduler.h' l='152' type='int64'/>
<use f='tensorflow/tensorflow/core/kernels/batch_kernels.cc' l='223' u='w' c='_ZN10tensorflow13BatchResource6CreateEiiiiRKSt6vectorIiSaIiEEPSt10unique_ptrIS0_St14default_deleteIS0_EE'/>
<doc f='tensorflow/tensorflow/core/kernels/batching_util/shared_batch_scheduler.h' l='140'>// If a task has been enqueued for this amount of time (in microseconds),
    // and a thread is available, the scheduler will immediately form a batch
    // from enqueued tasks and assign the batch to the thread for processing,
    // even if the batch&apos;s size is below &apos;max_batch_size&apos;.
    //
    // This parameter offers a way to bound queue latency, so that a task isn&apos;t
    // stuck in the queue indefinitely waiting for enough tasks to arrive to
    // make a full batch. (The latency bound is given in the class documentation
    // above.)
    //
    // The goal is to smooth out batch sizes under low request rates, and thus
    // avoid latency spikes.</doc>
