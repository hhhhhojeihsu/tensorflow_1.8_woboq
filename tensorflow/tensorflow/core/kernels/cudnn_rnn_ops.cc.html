<!doctype html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><title>cudnn_rnn_ops.cc source code [tensorflow/tensorflow/core/kernels/cudnn_rnn_ops.cc] - Woboq Code Browser</title>
<link rel="stylesheet" href="https://code.woboq.org/data/qtcreator.css" title="QtCreator"/>
<link rel="alternate stylesheet" href="https://code.woboq.org/data/kdevelop.css" title="KDevelop"/>
<script type="text/javascript" src="https://code.woboq.org/data/jquery/jquery.min.js"></script>
<script type="text/javascript" src="https://code.woboq.org/data/jquery/jquery-ui.min.js"></script>
<script>var file = 'tensorflow/tensorflow/core/kernels/cudnn_rnn_ops.cc'; var root_path = '../../../..'; var data_path = 'https://code.woboq.org/data';</script>
<script src='https://code.woboq.org/data/codebrowser.js'></script>
</head>
<body><div id='header'><h1 id='breadcrumb'><span>Browse the source code of </span><a href='../../..'>tensorflow</a>/<a href='../..'>tensorflow</a>/<a href='..'>core</a>/<a href='./'>kernels</a>/<a href='cudnn_rnn_ops.cc.html'>cudnn_rnn_ops.cc</a></h1></div>
<hr/><div id='content'><table class="code">
<tr><th id="1">1</th><td><i>/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.</i></td></tr>
<tr><th id="2">2</th><td><i></i></td></tr>
<tr><th id="3">3</th><td><i>Licensed under the Apache License, Version 2.0 (the "License");</i></td></tr>
<tr><th id="4">4</th><td><i>you may not use this file except in compliance with the License.</i></td></tr>
<tr><th id="5">5</th><td><i>You may obtain a copy of the License at</i></td></tr>
<tr><th id="6">6</th><td><i></i></td></tr>
<tr><th id="7">7</th><td><i>    <a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></i></td></tr>
<tr><th id="8">8</th><td><i></i></td></tr>
<tr><th id="9">9</th><td><i>Unless required by applicable law or agreed to in writing, software</i></td></tr>
<tr><th id="10">10</th><td><i>distributed under the License is distributed on an "AS IS" BASIS,</i></td></tr>
<tr><th id="11">11</th><td><i>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</i></td></tr>
<tr><th id="12">12</th><td><i>See the License for the specific language governing permissions and</i></td></tr>
<tr><th id="13">13</th><td><i>limitations under the License.</i></td></tr>
<tr><th id="14">14</th><td><i>==============================================================================*/</i></td></tr>
<tr><th id="15">15</th><td><u>#define <dfn class="macro" id="_M/EIGEN_USE_THREADS" data-ref="_M/EIGEN_USE_THREADS">EIGEN_USE_THREADS</dfn></u></td></tr>
<tr><th id="16">16</th><td></td></tr>
<tr><th id="17">17</th><td><u>#include &lt;stddef.h&gt;</u></td></tr>
<tr><th id="18">18</th><td><u>#include <a href="../../../../include/c++/5/atomic.html">&lt;atomic&gt;</a></u></td></tr>
<tr><th id="19">19</th><td><u>#include <a href="../../../../include/c++/5/cmath.html">&lt;cmath&gt;</a></u></td></tr>
<tr><th id="20">20</th><td><u>#include <a href="../../../../include/c++/5/functional.html">&lt;functional&gt;</a></u></td></tr>
<tr><th id="21">21</th><td><u>#include <a href="../../../../include/c++/5/limits.html">&lt;limits&gt;</a></u></td></tr>
<tr><th id="22">22</th><td><u>#include <a href="../../../../include/c++/5/string.html">&lt;string&gt;</a></u></td></tr>
<tr><th id="23">23</th><td><u>#include <a href="../../../../include/c++/5/unordered_set.html">&lt;unordered_set&gt;</a></u></td></tr>
<tr><th id="24">24</th><td></td></tr>
<tr><th id="25">25</th><td><u>#include <a href="../../../third_party/eigen3/unsupported/Eigen/CXX11/Tensor.html">"third_party/eigen3/unsupported/Eigen/CXX11/Tensor"</a></u></td></tr>
<tr><th id="26">26</th><td><u>#include <a href="../framework/device_base.h.html">"tensorflow/core/framework/device_base.h"</a></u></td></tr>
<tr><th id="27">27</th><td><u>#include <a href="../framework/kernel_def_builder.h.html">"tensorflow/core/framework/kernel_def_builder.h"</a></u></td></tr>
<tr><th id="28">28</th><td><u>#include <a href="../framework/op.h.html">"tensorflow/core/framework/op.h"</a></u></td></tr>
<tr><th id="29">29</th><td><u>#include <a href="../framework/op_def_builder.h.html">"tensorflow/core/framework/op_def_builder.h"</a></u></td></tr>
<tr><th id="30">30</th><td><u>#include <a href="../framework/op_kernel.h.html">"tensorflow/core/framework/op_kernel.h"</a></u></td></tr>
<tr><th id="31">31</th><td><u>#include <a href="../framework/register_types.h.html">"tensorflow/core/framework/register_types.h"</a></u></td></tr>
<tr><th id="32">32</th><td><u>#include <a href="../framework/tensor.h.html">"tensorflow/core/framework/tensor.h"</a></u></td></tr>
<tr><th id="33">33</th><td><u>#include <a href="../framework/tensor_shape.h.html">"tensorflow/core/framework/tensor_shape.h"</a></u></td></tr>
<tr><th id="34">34</th><td><u>#include <a href="../framework/tensor_types.h.html">"tensorflow/core/framework/tensor_types.h"</a></u></td></tr>
<tr><th id="35">35</th><td><u>#include <a href="../framework/types.h.html">"tensorflow/core/framework/types.h"</a></u></td></tr>
<tr><th id="36">36</th><td><u>#include <a href="../lib/core/errors.h.html">"tensorflow/core/lib/core/errors.h"</a></u></td></tr>
<tr><th id="37">37</th><td><u>#include <a href="../lib/core/status.h.html">"tensorflow/core/lib/core/status.h"</a></u></td></tr>
<tr><th id="38">38</th><td><u>#include <a href="../lib/core/stringpiece.h.html">"tensorflow/core/lib/core/stringpiece.h"</a></u></td></tr>
<tr><th id="39">39</th><td><u>#include <a href="../lib/gtl/inlined_vector.h.html">"tensorflow/core/lib/gtl/inlined_vector.h"</a></u></td></tr>
<tr><th id="40">40</th><td><u>#include <a href="../lib/hash/hash.h.html">"tensorflow/core/lib/hash/hash.h"</a></u></td></tr>
<tr><th id="41">41</th><td><u>#include <a href="../lib/strings/stringprintf.h.html">"tensorflow/core/lib/strings/stringprintf.h"</a></u></td></tr>
<tr><th id="42">42</th><td><u>#include <a href="../platform/fingerprint.h.html">"tensorflow/core/platform/fingerprint.h"</a></u></td></tr>
<tr><th id="43">43</th><td><u>#include <a href="../platform/mutex.h.html">"tensorflow/core/platform/mutex.h"</a></u></td></tr>
<tr><th id="44">44</th><td><u>#include <a href="../platform/types.h.html">"tensorflow/core/platform/types.h"</a></u></td></tr>
<tr><th id="45">45</th><td><u>#include <a href="../util/env_var.h.html">"tensorflow/core/util/env_var.h"</a></u></td></tr>
<tr><th id="46">46</th><td></td></tr>
<tr><th id="47">47</th><td><u>#<span data-ppcond="47">if</span> GOOGLE_CUDA</u></td></tr>
<tr><th id="48">48</th><td><u>#include "tensorflow/core/platform/stream_executor.h"</u></td></tr>
<tr><th id="49">49</th><td><u>#include "tensorflow/core/util/stream_executor_util.h"</u></td></tr>
<tr><th id="50">50</th><td><u>#<span data-ppcond="47">endif</span>  // GOOGLE_CUDA</u></td></tr>
<tr><th id="51">51</th><td></td></tr>
<tr><th id="52">52</th><td><i>/*</i></td></tr>
<tr><th id="53">53</th><td><i> * This module implements ops that fuse a multi-layer multi-step RNN/LSTM model</i></td></tr>
<tr><th id="54">54</th><td><i> * using the underlying Cudnn library.</i></td></tr>
<tr><th id="55">55</th><td><i> *</i></td></tr>
<tr><th id="56">56</th><td><i> * Cudnn RNN library exposes an opaque parameter buffer with unknown layout and</i></td></tr>
<tr><th id="57">57</th><td><i> * format. And it is very likely that if saved, they cannot be used across</i></td></tr>
<tr><th id="58">58</th><td><i> * different GPUs. So users need to first query the size of the opaque</i></td></tr>
<tr><th id="59">59</th><td><i> * parameter buffer, and convert it to and from its canonical forms. But each</i></td></tr>
<tr><th id="60">60</th><td><i> * actual training step is carried out with the parameter buffer.</i></td></tr>
<tr><th id="61">61</th><td><i> *</i></td></tr>
<tr><th id="62">62</th><td><i> * Similar to many other ops, the forward op has two flavors: training and</i></td></tr>
<tr><th id="63">63</th><td><i> * inference. When training is specified, additional data in reserve_space will</i></td></tr>
<tr><th id="64">64</th><td><i> * be produced for the backward pass. So there is a performance penalty.</i></td></tr>
<tr><th id="65">65</th><td><i> *</i></td></tr>
<tr><th id="66">66</th><td><i> * In addition to the actual data and reserve_space, Cudnn also needs more</i></td></tr>
<tr><th id="67">67</th><td><i> * memory as temporary workspace. The memory management to and from</i></td></tr>
<tr><th id="68">68</th><td><i> * stream-executor is done through ScratchAllocator. In general,</i></td></tr>
<tr><th id="69">69</th><td><i> * stream-executor is responsible for creating the memory of proper size. And</i></td></tr>
<tr><th id="70">70</th><td><i> * TensorFlow is responsible for making sure the memory is alive long enough</i></td></tr>
<tr><th id="71">71</th><td><i> * and recycles afterwards.</i></td></tr>
<tr><th id="72">72</th><td><i> *</i></td></tr>
<tr><th id="73">73</th><td><i> */</i></td></tr>
<tr><th id="74">74</th><td><b>namespace</b> <span class="namespace">tensorflow</span> {</td></tr>
<tr><th id="75">75</th><td></td></tr>
<tr><th id="76">76</th><td><b>using</b> <dfn class="typedef" id="tensorflow::CPUDevice" title='tensorflow::CPUDevice' data-type='Eigen::ThreadPoolDevice' data-ref="tensorflow::CPUDevice">CPUDevice</dfn> = <span class="namespace">Eigen::</span><span class='type' title='Eigen::ThreadPoolDevice' data-ref="Eigen::ThreadPoolDevice">ThreadPoolDevice</span>;</td></tr>
<tr><th id="77">77</th><td></td></tr>
<tr><th id="78">78</th><td><u>#<span data-ppcond="78">if</span> GOOGLE_CUDA</u></td></tr>
<tr><th id="79">79</th><td></td></tr>
<tr><th id="80">80</th><td><b>using</b> GPUDevice = Eigen::GpuDevice;</td></tr>
<tr><th id="81">81</th><td></td></tr>
<tr><th id="82">82</th><td><b>template</b> &lt;<b>typename</b> Device, <b>typename</b> T, <b>typename</b> Index&gt;</td></tr>
<tr><th id="83">83</th><td><b>class</b> CudnnRNNParamsSizeOp;</td></tr>
<tr><th id="84">84</th><td></td></tr>
<tr><th id="85">85</th><td><b>template</b> &lt;<b>typename</b> Device, <b>typename</b> T&gt;</td></tr>
<tr><th id="86">86</th><td><b>class</b> CudnnRNNParamsToCanonical;</td></tr>
<tr><th id="87">87</th><td></td></tr>
<tr><th id="88">88</th><td><b>template</b> &lt;<b>typename</b> Device, <b>typename</b> T&gt;</td></tr>
<tr><th id="89">89</th><td><b>class</b> CudnnRNNCanonicalToParams;</td></tr>
<tr><th id="90">90</th><td></td></tr>
<tr><th id="91">91</th><td><b>template</b> &lt;<b>typename</b> Device, <b>typename</b> T&gt;</td></tr>
<tr><th id="92">92</th><td><b>class</b> CudnnRNNForwardOp;</td></tr>
<tr><th id="93">93</th><td></td></tr>
<tr><th id="94">94</th><td><b>template</b> &lt;<b>typename</b> Device, <b>typename</b> T&gt;</td></tr>
<tr><th id="95">95</th><td><b>class</b> CudnnRNNBackwardOp;</td></tr>
<tr><th id="96">96</th><td></td></tr>
<tr><th id="97">97</th><td><b>enum</b> <b>class</b> TFRNNInputMode {</td></tr>
<tr><th id="98">98</th><td>  kRNNLinearInput = <var>0</var>,</td></tr>
<tr><th id="99">99</th><td>  kRNNSkipInput = <var>1</var>,</td></tr>
<tr><th id="100">100</th><td>  kAutoSelect = <var>9999999</var></td></tr>
<tr><th id="101">101</th><td>};</td></tr>
<tr><th id="102">102</th><td></td></tr>
<tr><th id="103">103</th><td><b>namespace</b> {</td></tr>
<tr><th id="104">104</th><td><b>using</b> perftools::gputools::DeviceMemory;</td></tr>
<tr><th id="105">105</th><td><b>using</b> perftools::gputools::DeviceMemoryBase;</td></tr>
<tr><th id="106">106</th><td><b>using</b> perftools::gputools::ScratchAllocator;</td></tr>
<tr><th id="107">107</th><td><b>using</b> perftools::gputools::dnn::AlgorithmConfig;</td></tr>
<tr><th id="108">108</th><td><b>using</b> perftools::gputools::dnn::RnnDirectionMode;</td></tr>
<tr><th id="109">109</th><td><b>using</b> perftools::gputools::dnn::RnnInputMode;</td></tr>
<tr><th id="110">110</th><td><b>using</b> perftools::gputools::dnn::RnnMode;</td></tr>
<tr><th id="111">111</th><td><b>using</b> perftools::gputools::dnn::ToDataType;</td></tr>
<tr><th id="112">112</th><td><b>using</b> perftools::gputools::port::StatusOr;</td></tr>
<tr><th id="113">113</th><td></td></tr>
<tr><th id="114">114</th><td>Status ParseRNNMode(<em>const</em> string&amp; str, RnnMode* rnn_mode) {</td></tr>
<tr><th id="115">115</th><td>  <b>if</b> (str == <q>"rnn_relu"</q>) {</td></tr>
<tr><th id="116">116</th><td>    *rnn_mode = RnnMode::kRnnRelu;</td></tr>
<tr><th id="117">117</th><td>    <b>return</b> Status::OK();</td></tr>
<tr><th id="118">118</th><td>  } <b>else</b> <b>if</b> (str == <q>"rnn_tanh"</q>) {</td></tr>
<tr><th id="119">119</th><td>    *rnn_mode = RnnMode::kRnnTanh;</td></tr>
<tr><th id="120">120</th><td>    <b>return</b> Status::OK();</td></tr>
<tr><th id="121">121</th><td>  } <b>else</b> <b>if</b> (str == <q>"lstm"</q>) {</td></tr>
<tr><th id="122">122</th><td>    *rnn_mode = RnnMode::kRnnLstm;</td></tr>
<tr><th id="123">123</th><td>    <b>return</b> Status::OK();</td></tr>
<tr><th id="124">124</th><td>  } <b>else</b> <b>if</b> (str == <q>"gru"</q>) {</td></tr>
<tr><th id="125">125</th><td>    *rnn_mode = RnnMode::kRnnGru;</td></tr>
<tr><th id="126">126</th><td>    <b>return</b> Status::OK();</td></tr>
<tr><th id="127">127</th><td>  }</td></tr>
<tr><th id="128">128</th><td>  <b>return</b> errors::InvalidArgument(<q>"Invalid RNN mode: "</q>, str);</td></tr>
<tr><th id="129">129</th><td>}</td></tr>
<tr><th id="130">130</th><td></td></tr>
<tr><th id="131">131</th><td>Status ParseTFRNNInputMode(<em>const</em> string&amp; str, TFRNNInputMode* rnn_input_mode) {</td></tr>
<tr><th id="132">132</th><td>  <b>if</b> (str == <q>"linear_input"</q>) {</td></tr>
<tr><th id="133">133</th><td>    *rnn_input_mode = TFRNNInputMode::kRNNLinearInput;</td></tr>
<tr><th id="134">134</th><td>    <b>return</b> Status::OK();</td></tr>
<tr><th id="135">135</th><td>  } <b>else</b> <b>if</b> (str == <q>"skip_input"</q>) {</td></tr>
<tr><th id="136">136</th><td>    *rnn_input_mode = TFRNNInputMode::kRNNSkipInput;</td></tr>
<tr><th id="137">137</th><td>    <b>return</b> Status::OK();</td></tr>
<tr><th id="138">138</th><td>  } <b>else</b> <b>if</b> (str == <q>"auto_select"</q>) {</td></tr>
<tr><th id="139">139</th><td>    *rnn_input_mode = TFRNNInputMode::kAutoSelect;</td></tr>
<tr><th id="140">140</th><td>    <b>return</b> Status::OK();</td></tr>
<tr><th id="141">141</th><td>  }</td></tr>
<tr><th id="142">142</th><td>  <b>return</b> errors::InvalidArgument(<q>"Invalid RNN input mode: "</q>, str);</td></tr>
<tr><th id="143">143</th><td>}</td></tr>
<tr><th id="144">144</th><td></td></tr>
<tr><th id="145">145</th><td>Status ParseRNNDirectionMode(<em>const</em> string&amp; str,</td></tr>
<tr><th id="146">146</th><td>                             RnnDirectionMode* rnn_dir_mode) {</td></tr>
<tr><th id="147">147</th><td>  <b>if</b> (str == <q>"unidirectional"</q>) {</td></tr>
<tr><th id="148">148</th><td>    *rnn_dir_mode = RnnDirectionMode::kRnnUnidirectional;</td></tr>
<tr><th id="149">149</th><td>    <b>return</b> Status::OK();</td></tr>
<tr><th id="150">150</th><td>  } <b>else</b> <b>if</b> (str == <q>"bidirectional"</q>) {</td></tr>
<tr><th id="151">151</th><td>    *rnn_dir_mode = RnnDirectionMode::kRnnBidirectional;</td></tr>
<tr><th id="152">152</th><td>    <b>return</b> Status::OK();</td></tr>
<tr><th id="153">153</th><td>  }</td></tr>
<tr><th id="154">154</th><td>  <b>return</b> errors::InvalidArgument(<q>"Invalid RNN direction mode: "</q>, str);</td></tr>
<tr><th id="155">155</th><td>}</td></tr>
<tr><th id="156">156</th><td></td></tr>
<tr><th id="157">157</th><td>Status ToRNNInputMode(TFRNNInputMode tf_input_mode, <em>int</em> num_units,</td></tr>
<tr><th id="158">158</th><td>                      <em>int</em> input_size, RnnInputMode* input_mode) {</td></tr>
<tr><th id="159">159</th><td>  <b>switch</b> (tf_input_mode) {</td></tr>
<tr><th id="160">160</th><td>    <b>case</b> TFRNNInputMode::kRNNLinearInput:</td></tr>
<tr><th id="161">161</th><td>      *input_mode = RnnInputMode::kRnnLinearSkip;</td></tr>
<tr><th id="162">162</th><td>      <b>break</b>;</td></tr>
<tr><th id="163">163</th><td>    <b>case</b> TFRNNInputMode::kRNNSkipInput:</td></tr>
<tr><th id="164">164</th><td>      *input_mode = RnnInputMode::kRnnSkipInput;</td></tr>
<tr><th id="165">165</th><td>      <b>break</b>;</td></tr>
<tr><th id="166">166</th><td>    <b>case</b> TFRNNInputMode::kAutoSelect:</td></tr>
<tr><th id="167">167</th><td>      *input_mode = (input_size == num_units) ? RnnInputMode::kRnnSkipInput</td></tr>
<tr><th id="168">168</th><td>                                              : RnnInputMode::kRnnLinearSkip;</td></tr>
<tr><th id="169">169</th><td>      <b>break</b>;</td></tr>
<tr><th id="170">170</th><td>    <b>default</b>:</td></tr>
<tr><th id="171">171</th><td>      <b>return</b> errors::InvalidArgument(<q>"Invalid TF input mode: "</q>,</td></tr>
<tr><th id="172">172</th><td>                                     <b>static_cast</b>&lt;<em>int</em>&gt;(tf_input_mode));</td></tr>
<tr><th id="173">173</th><td>  }</td></tr>
<tr><th id="174">174</th><td>  <b>return</b> Status::OK();</td></tr>
<tr><th id="175">175</th><td>}</td></tr>
<tr><th id="176">176</th><td></td></tr>
<tr><th id="177">177</th><td><i>// TODO(zhengxq): Merge those into stream_executor_util.h.</i></td></tr>
<tr><th id="178">178</th><td><b>template</b> &lt;<b>typename</b> T&gt;</td></tr>
<tr><th id="179">179</th><td><em>const</em> DeviceMemory&lt;T&gt; AsDeviceMemory(<em>const</em> Tensor* tensor) {</td></tr>
<tr><th id="180">180</th><td>  <b>return</b> DeviceMemory&lt;T&gt;::MakeFromByteSize(</td></tr>
<tr><th id="181">181</th><td>      <b>const_cast</b>&lt;T*&gt;(tensor-&gt;<b>template</b> flat&lt;T&gt;().data()),</td></tr>
<tr><th id="182">182</th><td>      tensor-&gt;<b>template</b> flat&lt;T&gt;().size() * <b>sizeof</b>(T));</td></tr>
<tr><th id="183">183</th><td>}</td></tr>
<tr><th id="184">184</th><td></td></tr>
<tr><th id="185">185</th><td><b>template</b> &lt;<b>typename</b> T&gt;</td></tr>
<tr><th id="186">186</th><td>DeviceMemory&lt;T&gt; AsDeviceMemory(Tensor* tensor) {</td></tr>
<tr><th id="187">187</th><td>  <b>return</b> DeviceMemory&lt;T&gt;::MakeFromByteSize(</td></tr>
<tr><th id="188">188</th><td>      tensor-&gt;<b>template</b> flat&lt;T&gt;().data(),</td></tr>
<tr><th id="189">189</th><td>      tensor-&gt;<b>template</b> flat&lt;T&gt;().size() * <b>sizeof</b>(T));</td></tr>
<tr><th id="190">190</th><td>}</td></tr>
<tr><th id="191">191</th><td></td></tr>
<tr><th id="192">192</th><td><b>template</b> &lt;<b>typename</b> U, <b>typename</b> T&gt;</td></tr>
<tr><th id="193">193</th><td>DeviceMemory&lt;U&gt; CastDeviceMemory(Tensor* tensor) {</td></tr>
<tr><th id="194">194</th><td>  <b>return</b> DeviceMemory&lt;U&gt;::MakeFromByteSize(</td></tr>
<tr><th id="195">195</th><td>      tensor-&gt;<b>template</b> flat&lt;T&gt;().data(),</td></tr>
<tr><th id="196">196</th><td>      tensor-&gt;<b>template</b> flat&lt;T&gt;().size() * <b>sizeof</b>(T));</td></tr>
<tr><th id="197">197</th><td>}</td></tr>
<tr><th id="198">198</th><td></td></tr>
<tr><th id="199">199</th><td>DeviceMemoryBase SliceDeviceMemory(<em>const</em> DeviceMemoryBase&amp; device_memory,</td></tr>
<tr><th id="200">200</th><td>                                   int64 offset, int64 size) {</td></tr>
<tr><th id="201">201</th><td>  <em>const</em> <em>void</em>* base_ptr = device_memory.opaque();</td></tr>
<tr><th id="202">202</th><td>  <em>void</em>* offset_ptr =</td></tr>
<tr><th id="203">203</th><td>      <b>const_cast</b>&lt;<em>char</em>*&gt;(<b>reinterpret_cast</b>&lt;<em>const</em> <em>char</em>*&gt;(base_ptr) + offset);</td></tr>
<tr><th id="204">204</th><td>  CHECK(offset + size &lt;= device_memory.size())</td></tr>
<tr><th id="205">205</th><td>      &lt;&lt; <q>"The slice is not within the region of DeviceMemory."</q>;</td></tr>
<tr><th id="206">206</th><td>  <b>return</b> DeviceMemoryBase(offset_ptr, size);</td></tr>
<tr><th id="207">207</th><td>}</td></tr>
<tr><th id="208">208</th><td></td></tr>
<tr><th id="209">209</th><td><b>inline</b> Status FromExecutorStatus(<em>const</em> perftools::gputools::port::Status&amp; s) {</td></tr>
<tr><th id="210">210</th><td>  <b>return</b> s.ok() ? Status::OK()</td></tr>
<tr><th id="211">211</th><td>                : Status(<b>static_cast</b>&lt;tensorflow::error::Code&gt;(</td></tr>
<tr><th id="212">212</th><td>                             <b>static_cast</b>&lt;<em>int</em>&gt;(s.code())),</td></tr>
<tr><th id="213">213</th><td>                         s.error_message());</td></tr>
<tr><th id="214">214</th><td>}</td></tr>
<tr><th id="215">215</th><td></td></tr>
<tr><th id="216">216</th><td><b>template</b> &lt;<b>typename</b> T&gt;</td></tr>
<tr><th id="217">217</th><td><b>inline</b> Status FromExecutorStatus(</td></tr>
<tr><th id="218">218</th><td>    <em>const</em> perftools::gputools::port::StatusOr&lt;T&gt;&amp; s) {</td></tr>
<tr><th id="219">219</th><td>  <b>return</b> FromExecutorStatus(s.status());</td></tr>
<tr><th id="220">220</th><td>}</td></tr>
<tr><th id="221">221</th><td></td></tr>
<tr><th id="222">222</th><td><b>inline</b> perftools::gputools::port::Status ToExecutorStatus(<em>const</em> Status&amp; s) {</td></tr>
<tr><th id="223">223</th><td>  <b>return</b> s.ok() ? perftools::gputools::port::Status::OK()</td></tr>
<tr><th id="224">224</th><td>                : perftools::gputools::port::Status(</td></tr>
<tr><th id="225">225</th><td>                      <b>static_cast</b>&lt;perftools::gputools::port::error::Code&gt;(</td></tr>
<tr><th id="226">226</th><td>                          <b>static_cast</b>&lt;<em>int</em>&gt;(s.code())),</td></tr>
<tr><th id="227">227</th><td>                      s.error_message());</td></tr>
<tr><th id="228">228</th><td>}</td></tr>
<tr><th id="229">229</th><td></td></tr>
<tr><th id="230">230</th><td><b>template</b> &lt;<b>typename</b>&gt;</td></tr>
<tr><th id="231">231</th><td><b>struct</b> ToTFDataType;</td></tr>
<tr><th id="232">232</th><td></td></tr>
<tr><th id="233">233</th><td><b>template</b> &lt;&gt;</td></tr>
<tr><th id="234">234</th><td><b>struct</b> ToTFDataType&lt;Eigen::half&gt; : std::integral_constant&lt;DataType, DT_HALF&gt; {};</td></tr>
<tr><th id="235">235</th><td></td></tr>
<tr><th id="236">236</th><td><b>template</b> &lt;&gt;</td></tr>
<tr><th id="237">237</th><td><b>struct</b> ToTFDataType&lt;<em>float</em>&gt; : std::integral_constant&lt;DataType, DT_FLOAT&gt; {};</td></tr>
<tr><th id="238">238</th><td></td></tr>
<tr><th id="239">239</th><td><b>template</b> &lt;&gt;</td></tr>
<tr><th id="240">240</th><td><b>struct</b> ToTFDataType&lt;<em>double</em>&gt; : std::integral_constant&lt;DataType, DT_DOUBLE&gt; {};</td></tr>
<tr><th id="241">241</th><td></td></tr>
<tr><th id="242">242</th><td><b>template</b> &lt;&gt;</td></tr>
<tr><th id="243">243</th><td><b>struct</b> ToTFDataType&lt;uint8&gt; : std::integral_constant&lt;DataType, DT_UINT8&gt; {};</td></tr>
<tr><th id="244">244</th><td></td></tr>
<tr><th id="245">245</th><td><i>// A helper to allocate temporary scratch memory for Cudnn RNN models. It</i></td></tr>
<tr><th id="246">246</th><td><i>// takes the ownership of the underlying memory. The expectation is that the</i></td></tr>
<tr><th id="247">247</th><td><i>// memory should be alive for the span of the Cudnn RNN itself.</i></td></tr>
<tr><th id="248">248</th><td><b>template</b> &lt;<b>typename</b> T&gt;</td></tr>
<tr><th id="249">249</th><td><b>class</b> CudnnRnnAllocatorInTemp : <b>public</b> ScratchAllocator {</td></tr>
<tr><th id="250">250</th><td> <b>public</b>:</td></tr>
<tr><th id="251">251</th><td>  ~CudnnRnnAllocatorInTemp() = <b>default</b>;</td></tr>
<tr><th id="252">252</th><td></td></tr>
<tr><th id="253">253</th><td>  <b>explicit</b> CudnnRnnAllocatorInTemp(OpKernelContext* context)</td></tr>
<tr><th id="254">254</th><td>      : context_(context) {}</td></tr>
<tr><th id="255">255</th><td>  int64 GetMemoryLimitInBytes(perftools::gputools::Stream* stream) override {</td></tr>
<tr><th id="256">256</th><td>    <b>return</b> std::numeric_limits&lt;int64&gt;::max();</td></tr>
<tr><th id="257">257</th><td>  }</td></tr>
<tr><th id="258">258</th><td></td></tr>
<tr><th id="259">259</th><td>  StatusOr&lt;DeviceMemory&lt;uint8&gt;&gt; AllocateBytes(</td></tr>
<tr><th id="260">260</th><td>      perftools::gputools::Stream* stream, int64 byte_size) override {</td></tr>
<tr><th id="261">261</th><td>    Tensor temporary_memory;</td></tr>
<tr><th id="262">262</th><td>    <em>const</em> DataType tf_data_type = ToTFDataType&lt;T&gt;::value;</td></tr>
<tr><th id="263">263</th><td>    int64 allocate_count =</td></tr>
<tr><th id="264">264</th><td>        Eigen::divup(byte_size, <b>static_cast</b>&lt;int64&gt;(<b>sizeof</b>(T)));</td></tr>
<tr><th id="265">265</th><td>    Status allocation_status(context_-&gt;allocate_temp(</td></tr>
<tr><th id="266">266</th><td>        tf_data_type, TensorShape({allocate_count}), &amp;temporary_memory));</td></tr>
<tr><th id="267">267</th><td>    <b>if</b> (!allocation_status.ok()) {</td></tr>
<tr><th id="268">268</th><td>      <b>return</b> ToExecutorStatus(allocation_status);</td></tr>
<tr><th id="269">269</th><td>    }</td></tr>
<tr><th id="270">270</th><td>    <i>// Hold the reference of the allocated tensors until the end of the</i></td></tr>
<tr><th id="271">271</th><td><i>    // allocator.</i></td></tr>
<tr><th id="272">272</th><td>    allocated_tensors_.push_back(temporary_memory);</td></tr>
<tr><th id="273">273</th><td>    total_byte_size_ += byte_size;</td></tr>
<tr><th id="274">274</th><td>    <b>return</b> DeviceMemory&lt;uint8&gt;::MakeFromByteSize(</td></tr>
<tr><th id="275">275</th><td>        temporary_memory.<b>template</b> flat&lt;T&gt;().data(),</td></tr>
<tr><th id="276">276</th><td>        temporary_memory.<b>template</b> flat&lt;T&gt;().size() * <b>sizeof</b>(T));</td></tr>
<tr><th id="277">277</th><td>  }</td></tr>
<tr><th id="278">278</th><td></td></tr>
<tr><th id="279">279</th><td>  int64 TotalByteSize() <em>const</em> { <b>return</b> total_byte_size_; }</td></tr>
<tr><th id="280">280</th><td></td></tr>
<tr><th id="281">281</th><td>  Tensor get_allocated_tensor(<em>int</em> index) <em>const</em> {</td></tr>
<tr><th id="282">282</th><td>    <b>return</b> allocated_tensors_[index];</td></tr>
<tr><th id="283">283</th><td>  }</td></tr>
<tr><th id="284">284</th><td></td></tr>
<tr><th id="285">285</th><td> <b>private</b>:</td></tr>
<tr><th id="286">286</th><td>  int64 total_byte_size_ = <var>0</var>;</td></tr>
<tr><th id="287">287</th><td>  OpKernelContext* context_;  <i>// not owned</i></td></tr>
<tr><th id="288">288</th><td>  std::vector&lt;Tensor&gt; allocated_tensors_;</td></tr>
<tr><th id="289">289</th><td>};</td></tr>
<tr><th id="290">290</th><td></td></tr>
<tr><th id="291">291</th><td><i>// A helper to allocate memory for Cudnn RNN models as a kernel output. It is</i></td></tr>
<tr><th id="292">292</th><td><i>// used by forward pass kernel to feed the output to the backward pass.</i></td></tr>
<tr><th id="293">293</th><td><i>// The memory is expected to live long enough after the backward pass is</i></td></tr>
<tr><th id="294">294</th><td><i>// finished.</i></td></tr>
<tr><th id="295">295</th><td><b>template</b> &lt;<b>typename</b> T&gt;</td></tr>
<tr><th id="296">296</th><td><b>class</b> CudnnRnnAllocatorInOutput : <b>public</b> ScratchAllocator {</td></tr>
<tr><th id="297">297</th><td> <b>public</b>:</td></tr>
<tr><th id="298">298</th><td>  ~CudnnRnnAllocatorInOutput() override {}</td></tr>
<tr><th id="299">299</th><td>  CudnnRnnAllocatorInOutput(OpKernelContext* context, <em>int</em> output_index)</td></tr>
<tr><th id="300">300</th><td>      : context_(context), output_index_(output_index) {}</td></tr>
<tr><th id="301">301</th><td>  int64 GetMemoryLimitInBytes(perftools::gputools::Stream* stream) override {</td></tr>
<tr><th id="302">302</th><td>    <b>return</b> std::numeric_limits&lt;int64&gt;::max();</td></tr>
<tr><th id="303">303</th><td>  }</td></tr>
<tr><th id="304">304</th><td>  StatusOr&lt;DeviceMemory&lt;uint8&gt;&gt; AllocateBytes(</td></tr>
<tr><th id="305">305</th><td>      perftools::gputools::Stream* stream, int64 byte_size) override {</td></tr>
<tr><th id="306">306</th><td>    CHECK(total_byte_size_ == <var>0</var>)</td></tr>
<tr><th id="307">307</th><td>        &lt;&lt; <q>"Reserve space allocator can only be called once"</q>;</td></tr>
<tr><th id="308">308</th><td>    int64 allocate_count =</td></tr>
<tr><th id="309">309</th><td>        Eigen::divup(byte_size, <b>static_cast</b>&lt;int64&gt;(<b>sizeof</b>(T)));</td></tr>
<tr><th id="310">310</th><td></td></tr>
<tr><th id="311">311</th><td>    Tensor* temporary_memory = <b>nullptr</b>;</td></tr>
<tr><th id="312">312</th><td>    Status allocation_status(context_-&gt;allocate_output(</td></tr>
<tr><th id="313">313</th><td>        output_index_, TensorShape({allocate_count}), &amp;temporary_memory));</td></tr>
<tr><th id="314">314</th><td>    <b>if</b> (!allocation_status.ok()) {</td></tr>
<tr><th id="315">315</th><td>      <b>return</b> ToExecutorStatus(allocation_status);</td></tr>
<tr><th id="316">316</th><td>    }</td></tr>
<tr><th id="317">317</th><td>    total_byte_size_ += byte_size;</td></tr>
<tr><th id="318">318</th><td>    <em>auto</em> memory_uint8 = DeviceMemory&lt;uint8&gt;::MakeFromByteSize(</td></tr>
<tr><th id="319">319</th><td>        temporary_memory-&gt;<b>template</b> flat&lt;T&gt;().data(),</td></tr>
<tr><th id="320">320</th><td>        temporary_memory-&gt;<b>template</b> flat&lt;T&gt;().size() * <b>sizeof</b>(T));</td></tr>
<tr><th id="321">321</th><td>    <b>return</b> StatusOr&lt;DeviceMemory&lt;uint8&gt;&gt;(memory_uint8);</td></tr>
<tr><th id="322">322</th><td>  }</td></tr>
<tr><th id="323">323</th><td>  int64 TotalByteSize() { <b>return</b> total_byte_size_; }</td></tr>
<tr><th id="324">324</th><td></td></tr>
<tr><th id="325">325</th><td> <b>private</b>:</td></tr>
<tr><th id="326">326</th><td>  int64 total_byte_size_ = <var>0</var>;</td></tr>
<tr><th id="327">327</th><td>  OpKernelContext* context_;  <i>// not owned</i></td></tr>
<tr><th id="328">328</th><td>  <em>int</em> output_index_;</td></tr>
<tr><th id="329">329</th><td>};</td></tr>
<tr><th id="330">330</th><td></td></tr>
<tr><th id="331">331</th><td><i>// A helper to allocate persistent memory for Cudnn RNN models, which is</i></td></tr>
<tr><th id="332">332</th><td><i>// expected to live between kernel invocations.</i></td></tr>
<tr><th id="333">333</th><td><i>// This class is not thread-safe.</i></td></tr>
<tr><th id="334">334</th><td><b>class</b> CudnnRNNPersistentSpaceAllocator : <b>public</b> ScratchAllocator {</td></tr>
<tr><th id="335">335</th><td> <b>public</b>:</td></tr>
<tr><th id="336">336</th><td>  <b>explicit</b> CudnnRNNPersistentSpaceAllocator(OpKernelContext* context)</td></tr>
<tr><th id="337">337</th><td>      : context_(context) {}</td></tr>
<tr><th id="338">338</th><td></td></tr>
<tr><th id="339">339</th><td>  ~CudnnRNNPersistentSpaceAllocator() override {}</td></tr>
<tr><th id="340">340</th><td></td></tr>
<tr><th id="341">341</th><td>  int64 GetMemoryLimitInBytes(perftools::gputools::Stream* stream) override {</td></tr>
<tr><th id="342">342</th><td>    <b>return</b> std::numeric_limits&lt;int64&gt;::max();</td></tr>
<tr><th id="343">343</th><td>  }</td></tr>
<tr><th id="344">344</th><td></td></tr>
<tr><th id="345">345</th><td>  StatusOr&lt;DeviceMemory&lt;uint8&gt;&gt; AllocateBytes(</td></tr>
<tr><th id="346">346</th><td>      perftools::gputools::Stream* stream, int64 byte_size) override {</td></tr>
<tr><th id="347">347</th><td>    <b>if</b> (total_byte_size_ != <var>0</var>) {</td></tr>
<tr><th id="348">348</th><td>      <b>return</b> Status(error::FAILED_PRECONDITION,</td></tr>
<tr><th id="349">349</th><td>                    <q>"Persistent space allocator can only be called once"</q>);</td></tr>
<tr><th id="350">350</th><td>    }</td></tr>
<tr><th id="351">351</th><td></td></tr>
<tr><th id="352">352</th><td>    Status allocation_status = context_-&gt;allocate_persistent(</td></tr>
<tr><th id="353">353</th><td>        DT_UINT8, TensorShape({byte_size}), &amp;handle_, <b>nullptr</b>);</td></tr>
<tr><th id="354">354</th><td>    <b>if</b> (!allocation_status.ok()) {</td></tr>
<tr><th id="355">355</th><td>      <b>return</b> ToExecutorStatus(allocation_status);</td></tr>
<tr><th id="356">356</th><td>    }</td></tr>
<tr><th id="357">357</th><td>    total_byte_size_ += byte_size;</td></tr>
<tr><th id="358">358</th><td>    <b>return</b> AsDeviceMemory&lt;uint8&gt;(handle_.AccessTensor(context_));</td></tr>
<tr><th id="359">359</th><td>  }</td></tr>
<tr><th id="360">360</th><td>  int64 TotalByteSize() { <b>return</b> total_byte_size_; }</td></tr>
<tr><th id="361">361</th><td></td></tr>
<tr><th id="362">362</th><td> <b>private</b>:</td></tr>
<tr><th id="363">363</th><td>  int64 total_byte_size_ = <var>0</var>;</td></tr>
<tr><th id="364">364</th><td>  PersistentTensor handle_;</td></tr>
<tr><th id="365">365</th><td>  OpKernelContext* context_;  <i>// not owned</i></td></tr>
<tr><th id="366">366</th><td>};</td></tr>
<tr><th id="367">367</th><td></td></tr>
<tr><th id="368">368</th><td><b>struct</b> CudnnModelTypes {</td></tr>
<tr><th id="369">369</th><td>  RnnMode rnn_mode;</td></tr>
<tr><th id="370">370</th><td>  TFRNNInputMode rnn_input_mode;</td></tr>
<tr><th id="371">371</th><td>  RnnDirectionMode rnn_direction_mode;</td></tr>
<tr><th id="372">372</th><td>  <em>bool</em> HasInputC() <em>const</em> {</td></tr>
<tr><th id="373">373</th><td>    <i>// For Cudnn 5.0, only LSTM has input-c. All other models use only</i></td></tr>
<tr><th id="374">374</th><td><i>    // input-h.</i></td></tr>
<tr><th id="375">375</th><td>    <b>return</b> rnn_mode == RnnMode::kRnnLstm;</td></tr>
<tr><th id="376">376</th><td>  }</td></tr>
<tr><th id="377">377</th><td>};</td></tr>
<tr><th id="378">378</th><td></td></tr>
<tr><th id="379">379</th><td><i>// A helper class that collects the shapes to describe a RNN model.</i></td></tr>
<tr><th id="380">380</th><td><b>struct</b> CudnnRnnModelShapes {</td></tr>
<tr><th id="381">381</th><td>  <em>int</em> num_layers;</td></tr>
<tr><th id="382">382</th><td>  <em>int</em> input_size;</td></tr>
<tr><th id="383">383</th><td>  <em>int</em> num_units;</td></tr>
<tr><th id="384">384</th><td>  <em>int</em> seq_length;</td></tr>
<tr><th id="385">385</th><td>  <em>int</em> batch_size;</td></tr>
<tr><th id="386">386</th><td>  <em>int</em> dir_count;</td></tr>
<tr><th id="387">387</th><td>  TensorShape input_shape;</td></tr>
<tr><th id="388">388</th><td>  TensorShape output_shape;</td></tr>
<tr><th id="389">389</th><td>  TensorShape hidden_state_shape;</td></tr>
<tr><th id="390">390</th><td>  <i>// At present only fields related to cached RnnDescriptor are concerned.</i></td></tr>
<tr><th id="391">391</th><td>  <em>bool</em> IsCompatibleWith(<em>const</em> CudnnRnnModelShapes&amp; rhs) <em>const</em> {</td></tr>
<tr><th id="392">392</th><td>    <b>return</b> num_layers == rhs.num_layers &amp;&amp; input_size == rhs.input_size &amp;&amp;</td></tr>
<tr><th id="393">393</th><td>           num_units == rhs.num_units &amp;&amp; dir_count == rhs.dir_count;</td></tr>
<tr><th id="394">394</th><td>  }</td></tr>
<tr><th id="395">395</th><td>  string RnnDescDebugString() {</td></tr>
<tr><th id="396">396</th><td>    <b>return</b> strings::Printf(</td></tr>
<tr><th id="397">397</th><td>        <q>"[num_layers, input_size, num_units, dir_count]: [%d, %d, %d, %d]"</q>,</td></tr>
<tr><th id="398">398</th><td>        num_layers, input_size, num_units, dir_count);</td></tr>
<tr><th id="399">399</th><td>  }</td></tr>
<tr><th id="400">400</th><td>};</td></tr>
<tr><th id="401">401</th><td></td></tr>
<tr><th id="402">402</th><td><i>// Utility class for using CudnnRnnModelShapes as a hash table key.</i></td></tr>
<tr><th id="403">403</th><td><b>struct</b> CudnnRnnModelShapesHasher {</td></tr>
<tr><th id="404">404</th><td>  uint64 <b>operator</b>()(<em>const</em> CudnnRnnModelShapes&amp; to_hash) <em>const</em> {</td></tr>
<tr><th id="405">405</th><td>    uint64 hash = <b>static_cast</b>&lt;uint64&gt;(to_hash.num_layers);</td></tr>
<tr><th id="406">406</th><td>    hash = tensorflow::FingerprintCat64(</td></tr>
<tr><th id="407">407</th><td>        hash, <b>static_cast</b>&lt;uint64&gt;(to_hash.input_size));</td></tr>
<tr><th id="408">408</th><td>    hash = tensorflow::FingerprintCat64(hash,</td></tr>
<tr><th id="409">409</th><td>                                        <b>static_cast</b>&lt;uint64&gt;(to_hash.num_units));</td></tr>
<tr><th id="410">410</th><td>    <b>return</b> tensorflow::FingerprintCat64(hash,</td></tr>
<tr><th id="411">411</th><td>                                        <b>static_cast</b>&lt;uint64&gt;(to_hash.dir_count));</td></tr>
<tr><th id="412">412</th><td>  }</td></tr>
<tr><th id="413">413</th><td>};</td></tr>
<tr><th id="414">414</th><td></td></tr>
<tr><th id="415">415</th><td><i>// Utility class for using CudnnRnnModelShapes as a hash table key.</i></td></tr>
<tr><th id="416">416</th><td><b>struct</b> CudnnRnnModelShapesComparator {</td></tr>
<tr><th id="417">417</th><td>  <em>bool</em> <b>operator</b>()(<em>const</em> CudnnRnnModelShapes&amp; first,</td></tr>
<tr><th id="418">418</th><td>                  <em>const</em> CudnnRnnModelShapes&amp; second) <em>const</em> {</td></tr>
<tr><th id="419">419</th><td>    <b>return</b> first.IsCompatibleWith(second);</td></tr>
<tr><th id="420">420</th><td>  }</td></tr>
<tr><th id="421">421</th><td>};</td></tr>
<tr><th id="422">422</th><td></td></tr>
<tr><th id="423">423</th><td><i>// Extract and checks the forward input tensors, parameters, and shapes from</i></td></tr>
<tr><th id="424">424</th><td><i>// the OpKernelContext.</i></td></tr>
<tr><th id="425">425</th><td>Status ExtractForwardInput(OpKernelContext* context,</td></tr>
<tr><th id="426">426</th><td>                           <em>const</em> CudnnModelTypes&amp; model_types,</td></tr>
<tr><th id="427">427</th><td>                           <em>const</em> Tensor** input, <em>const</em> Tensor** input_h,</td></tr>
<tr><th id="428">428</th><td>                           <em>const</em> Tensor** input_c, <em>const</em> Tensor** params,</td></tr>
<tr><th id="429">429</th><td>                           CudnnRnnModelShapes* model_shapes) {</td></tr>
<tr><th id="430">430</th><td>  TF_RETURN_IF_ERROR(context-&gt;input(<q>"input"</q>, input));</td></tr>
<tr><th id="431">431</th><td>  TF_RETURN_IF_ERROR(context-&gt;input(<q>"input_h"</q>, input_h));</td></tr>
<tr><th id="432">432</th><td>  <b>if</b> (model_types.HasInputC()) {</td></tr>
<tr><th id="433">433</th><td>    TF_RETURN_IF_ERROR(context-&gt;input(<q>"input_c"</q>, input_c));</td></tr>
<tr><th id="434">434</th><td>  }</td></tr>
<tr><th id="435">435</th><td>  TF_RETURN_IF_ERROR(context-&gt;input(<q>"params"</q>, params));</td></tr>
<tr><th id="436">436</th><td></td></tr>
<tr><th id="437">437</th><td>  <b>if</b> ((*input)-&gt;dims() != <var>3</var>) {</td></tr>
<tr><th id="438">438</th><td>    <b>return</b> errors::InvalidArgument(<q>"RNN input must be a 3-D vector."</q>);</td></tr>
<tr><th id="439">439</th><td>  }</td></tr>
<tr><th id="440">440</th><td>  model_shapes-&gt;seq_length = (*input)-&gt;dim_size(<var>0</var>);</td></tr>
<tr><th id="441">441</th><td>  model_shapes-&gt;batch_size = (*input)-&gt;dim_size(<var>1</var>);</td></tr>
<tr><th id="442">442</th><td>  model_shapes-&gt;input_size = (*input)-&gt;dim_size(<var>2</var>);</td></tr>
<tr><th id="443">443</th><td>  model_shapes-&gt;input_shape = (*input)-&gt;shape();</td></tr>
<tr><th id="444">444</th><td>  model_shapes-&gt;dir_count =</td></tr>
<tr><th id="445">445</th><td>      (model_types.rnn_direction_mode == RnnDirectionMode::kRnnBidirectional)</td></tr>
<tr><th id="446">446</th><td>          ? <var>2</var></td></tr>
<tr><th id="447">447</th><td>          : <var>1</var>;</td></tr>
<tr><th id="448">448</th><td></td></tr>
<tr><th id="449">449</th><td>  <b>if</b> ((*input_h)-&gt;dims() != <var>3</var>) {</td></tr>
<tr><th id="450">450</th><td>    <b>return</b> errors::InvalidArgument(<q>"RNN input must be a 3-D vector."</q>);</td></tr>
<tr><th id="451">451</th><td>  }</td></tr>
<tr><th id="452">452</th><td>  model_shapes-&gt;num_layers = (*input_h)-&gt;dim_size(<var>0</var>) / model_shapes-&gt;dir_count;</td></tr>
<tr><th id="453">453</th><td>  model_shapes-&gt;num_units = (*input_h)-&gt;dim_size(<var>2</var>);</td></tr>
<tr><th id="454">454</th><td></td></tr>
<tr><th id="455">455</th><td>  model_shapes-&gt;hidden_state_shape =</td></tr>
<tr><th id="456">456</th><td>      TensorShape({model_shapes-&gt;dir_count * model_shapes-&gt;num_layers,</td></tr>
<tr><th id="457">457</th><td>                   model_shapes-&gt;batch_size, model_shapes-&gt;num_units});</td></tr>
<tr><th id="458">458</th><td>  <b>if</b> ((*input_h)-&gt;shape() != model_shapes-&gt;hidden_state_shape) {</td></tr>
<tr><th id="459">459</th><td>    <b>return</b> errors::InvalidArgument(</td></tr>
<tr><th id="460">460</th><td>        <q>"Invalid input_h shape: "</q>, (*input_h)-&gt;shape().DebugString(), <q>" "</q>,</td></tr>
<tr><th id="461">461</th><td>        model_shapes-&gt;hidden_state_shape.DebugString());</td></tr>
<tr><th id="462">462</th><td>  }</td></tr>
<tr><th id="463">463</th><td>  <b>if</b> (model_types.HasInputC()) {</td></tr>
<tr><th id="464">464</th><td>    <b>if</b> ((*input_h)-&gt;shape() != (*input_c)-&gt;shape()) {</td></tr>
<tr><th id="465">465</th><td>      <b>return</b> errors::InvalidArgument(</td></tr>
<tr><th id="466">466</th><td>          <q>"input_h and input_c must have the same shape: "</q>,</td></tr>
<tr><th id="467">467</th><td>          (*input_h)-&gt;shape().DebugString(), <q>" "</q>,</td></tr>
<tr><th id="468">468</th><td>          (*input_c)-&gt;shape().DebugString());</td></tr>
<tr><th id="469">469</th><td>    }</td></tr>
<tr><th id="470">470</th><td>  }</td></tr>
<tr><th id="471">471</th><td>  model_shapes-&gt;output_shape =</td></tr>
<tr><th id="472">472</th><td>      TensorShape({model_shapes-&gt;seq_length, model_shapes-&gt;batch_size,</td></tr>
<tr><th id="473">473</th><td>                   model_shapes-&gt;dir_count * model_shapes-&gt;num_units});</td></tr>
<tr><th id="474">474</th><td>  <b>return</b> Status::OK();</td></tr>
<tr><th id="475">475</th><td>}</td></tr>
<tr><th id="476">476</th><td></td></tr>
<tr><th id="477">477</th><td><b>using</b> perftools::gputools::dnn::RnnDescriptor;</td></tr>
<tr><th id="478">478</th><td></td></tr>
<tr><th id="479">479</th><td><b>template</b> &lt;<b>typename</b> T&gt;</td></tr>
<tr><th id="480">480</th><td><em>void</em> RestoreParams(<em>const</em> OpInputList params_input,</td></tr>
<tr><th id="481">481</th><td>                   <em>const</em> std::vector&lt;RnnDescriptor::ParamsRegion&gt;&amp; params,</td></tr>
<tr><th id="482">482</th><td>                   DeviceMemoryBase* data_dst,</td></tr>
<tr><th id="483">483</th><td>                   perftools::gputools::Stream* stream) {</td></tr>
<tr><th id="484">484</th><td>  <em>int</em> num_params = params.size();</td></tr>
<tr><th id="485">485</th><td>  CHECK(params_input.size() == num_params)</td></tr>
<tr><th id="486">486</th><td>      &lt;&lt; <q>"Number of params mismatch. Expected "</q> &lt;&lt; params_input.size()</td></tr>
<tr><th id="487">487</th><td>      &lt;&lt; <q>", got "</q> &lt;&lt; num_params;</td></tr>
<tr><th id="488">488</th><td>  <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; params.size(); i++) {</td></tr>
<tr><th id="489">489</th><td>    int64 size_in_bytes = params[i].size;</td></tr>
<tr><th id="490">490</th><td>    int64 size = size_in_bytes / <b>sizeof</b>(T);</td></tr>
<tr><th id="491">491</th><td>    CHECK(size == params_input[i].NumElements())</td></tr>
<tr><th id="492">492</th><td>        &lt;&lt; <q>"Params size mismatch. Expected "</q> &lt;&lt; size &lt;&lt; <q>", got "</q></td></tr>
<tr><th id="493">493</th><td>        &lt;&lt; params_input[i].NumElements();</td></tr>
<tr><th id="494">494</th><td>    <em>auto</em> data_src_ptr = StreamExecutorUtil::AsDeviceMemory&lt;T&gt;(params_input[i]);</td></tr>
<tr><th id="495">495</th><td>    DeviceMemoryBase data_dst_ptr =</td></tr>
<tr><th id="496">496</th><td>        SliceDeviceMemory(*data_dst, params[i].offset, size_in_bytes);</td></tr>
<tr><th id="497">497</th><td>    stream-&gt;ThenMemcpy(&amp;data_dst_ptr, data_src_ptr, size_in_bytes);</td></tr>
<tr><th id="498">498</th><td>  }</td></tr>
<tr><th id="499">499</th><td>}</td></tr>
<tr><th id="500">500</th><td></td></tr>
<tr><th id="501">501</th><td>}  <i>// namespace</i></td></tr>
<tr><th id="502">502</th><td></td></tr>
<tr><th id="503">503</th><td><i>// Note: all following kernels depend on a RnnDescriptor instance, which</i></td></tr>
<tr><th id="504">504</th><td><i>// according to Cudnn official doc should be kept around and reused across all</i></td></tr>
<tr><th id="505">505</th><td><i>// Cudnn kernels in the same model.</i></td></tr>
<tr><th id="506">506</th><td><i>// In Tensorflow, we don't pass the reference across different OpKernels,</i></td></tr>
<tr><th id="507">507</th><td><i>// rather, recreate it separately in each OpKernel, which does no cause issue:</i></td></tr>
<tr><th id="508">508</th><td><i>// CudnnDropoutDescriptor keeps a reference to a memory for</i></td></tr>
<tr><th id="509">509</th><td><i>// random number generator state. During recreation, this state is lost.</i></td></tr>
<tr><th id="510">510</th><td><i>// However, only forward-pass Cudnn APIs make use of the state.</i></td></tr>
<tr><th id="511">511</th><td><i></i></td></tr>
<tr><th id="512">512</th><td><i>// A common base class for RNN kernels. It extracts common attributes and</i></td></tr>
<tr><th id="513">513</th><td><i>// shape validations.</i></td></tr>
<tr><th id="514">514</th><td><b>class</b> CudnnRNNKernelCommon : <b>public</b> OpKernel {</td></tr>
<tr><th id="515">515</th><td> <b>protected</b>:</td></tr>
<tr><th id="516">516</th><td>  <b>explicit</b> CudnnRNNKernelCommon(OpKernelConstruction* context)</td></tr>
<tr><th id="517">517</th><td>      : OpKernel(context) {</td></tr>
<tr><th id="518">518</th><td>    OP_REQUIRES_OK(context, context-&gt;GetAttr(<q>"dropout"</q>, &amp;dropout_));</td></tr>
<tr><th id="519">519</th><td>    OP_REQUIRES_OK(context, context-&gt;GetAttr(<q>"seed"</q>, &amp;seed_));</td></tr>
<tr><th id="520">520</th><td>    OP_REQUIRES_OK(context, context-&gt;GetAttr(<q>"seed2"</q>, &amp;seed2_));</td></tr>
<tr><th id="521">521</th><td>    string str;</td></tr>
<tr><th id="522">522</th><td>    OP_REQUIRES_OK(context, context-&gt;GetAttr(<q>"rnn_mode"</q>, &amp;str));</td></tr>
<tr><th id="523">523</th><td>    OP_REQUIRES_OK(context, ParseRNNMode(str, &amp;model_types_.rnn_mode));</td></tr>
<tr><th id="524">524</th><td>    OP_REQUIRES_OK(context, context-&gt;GetAttr(<q>"input_mode"</q>, &amp;str));</td></tr>
<tr><th id="525">525</th><td>    OP_REQUIRES_OK(context,</td></tr>
<tr><th id="526">526</th><td>                   ParseTFRNNInputMode(str, &amp;model_types_.rnn_input_mode));</td></tr>
<tr><th id="527">527</th><td>    OP_REQUIRES_OK(context, context-&gt;GetAttr(<q>"direction"</q>, &amp;str));</td></tr>
<tr><th id="528">528</th><td>    OP_REQUIRES_OK(</td></tr>
<tr><th id="529">529</th><td>        context, ParseRNNDirectionMode(str, &amp;model_types_.rnn_direction_mode));</td></tr>
<tr><th id="530">530</th><td>    <i>// Reset CudnnRnnDescriptor and related random number generate states in</i></td></tr>
<tr><th id="531">531</th><td><i>    // every Compute() call.</i></td></tr>
<tr><th id="532">532</th><td>    OP_REQUIRES_OK(context, ReadBoolFromEnvVar(<q>"TF_CUDNN_RESET_RND_GEN_STATE"</q>,</td></tr>
<tr><th id="533">533</th><td>                                               <b>false</b>, &amp;reset_rnd_gen_state_));</td></tr>
<tr><th id="534">534</th><td>  }</td></tr>
<tr><th id="535">535</th><td></td></tr>
<tr><th id="536">536</th><td>  <em>bool</em> HasInputC() <em>const</em> { <b>return</b> model_types_.HasInputC(); }</td></tr>
<tr><th id="537">537</th><td>  RnnMode rnn_mode() <em>const</em> { <b>return</b> model_types_.rnn_mode; }</td></tr>
<tr><th id="538">538</th><td>  TFRNNInputMode rnn_input_mode() <em>const</em> { <b>return</b> model_types_.rnn_input_mode; }</td></tr>
<tr><th id="539">539</th><td>  RnnDirectionMode rnn_direction_mode() <em>const</em> {</td></tr>
<tr><th id="540">540</th><td>    <b>return</b> model_types_.rnn_direction_mode;</td></tr>
<tr><th id="541">541</th><td>  }</td></tr>
<tr><th id="542">542</th><td>  CudnnModelTypes model_types() <em>const</em> { <b>return</b> model_types_; }</td></tr>
<tr><th id="543">543</th><td>  <em>float</em> dropout() <em>const</em> { <b>return</b> dropout_; }</td></tr>
<tr><th id="544">544</th><td>  uint64 seed() { <b>return</b> (<b>static_cast</b>&lt;uint64&gt;(seed_) &lt;&lt; <var>32</var>) | seed2_; }</td></tr>
<tr><th id="545">545</th><td>  <em>bool</em> ResetRndGenState() { <b>return</b> reset_rnd_gen_state_; }</td></tr>
<tr><th id="546">546</th><td></td></tr>
<tr><th id="547">547</th><td>  <b>template</b> &lt;<b>typename</b> T&gt;</td></tr>
<tr><th id="548">548</th><td>  Status ExtractCudnnRNNParamsInfo(OpKernelContext* context,</td></tr>
<tr><th id="549">549</th><td>                                   std::unique_ptr&lt;RnnDescriptor&gt;* rnn_desc) {</td></tr>
<tr><th id="550">550</th><td>    <em>const</em> Tensor* num_layers_t = <b>nullptr</b>;</td></tr>
<tr><th id="551">551</th><td>    TF_RETURN_IF_ERROR(context-&gt;input(<q>"num_layers"</q>, &amp;num_layers_t));</td></tr>
<tr><th id="552">552</th><td>    <b>if</b> (!TensorShapeUtils::IsScalar(num_layers_t-&gt;shape())) {</td></tr>
<tr><th id="553">553</th><td>      <b>return</b> errors::InvalidArgument(<q>"num_layers is not a scalar"</q>);</td></tr>
<tr><th id="554">554</th><td>    }</td></tr>
<tr><th id="555">555</th><td>    <em>int</em> num_layers = num_layers_t-&gt;scalar&lt;<em>int</em>&gt;()();</td></tr>
<tr><th id="556">556</th><td>    <em>const</em> Tensor* num_units_t = <b>nullptr</b>;</td></tr>
<tr><th id="557">557</th><td>    TF_RETURN_IF_ERROR(context-&gt;input(<q>"num_units"</q>, &amp;num_units_t));</td></tr>
<tr><th id="558">558</th><td>    <b>if</b> (!TensorShapeUtils::IsScalar(num_units_t-&gt;shape())) {</td></tr>
<tr><th id="559">559</th><td>      <b>return</b> errors::InvalidArgument(<q>"num_units is not a scalar"</q>);</td></tr>
<tr><th id="560">560</th><td>    }</td></tr>
<tr><th id="561">561</th><td>    <em>int</em> num_units = num_units_t-&gt;scalar&lt;<em>int</em>&gt;()();</td></tr>
<tr><th id="562">562</th><td>    <em>const</em> Tensor* input_size_t = <b>nullptr</b>;</td></tr>
<tr><th id="563">563</th><td>    TF_RETURN_IF_ERROR(context-&gt;input(<q>"input_size"</q>, &amp;input_size_t));</td></tr>
<tr><th id="564">564</th><td>    <b>if</b> (!TensorShapeUtils::IsScalar(input_size_t-&gt;shape())) {</td></tr>
<tr><th id="565">565</th><td>      <b>return</b> errors::InvalidArgument(<q>"input_size is not a scalar"</q>);</td></tr>
<tr><th id="566">566</th><td>    }</td></tr>
<tr><th id="567">567</th><td>    <em>int</em> input_size = input_size_t-&gt;scalar&lt;<em>int</em>&gt;()();</td></tr>
<tr><th id="568">568</th><td></td></tr>
<tr><th id="569">569</th><td>    RnnInputMode input_mode;</td></tr>
<tr><th id="570">570</th><td>    TF_RETURN_IF_ERROR(</td></tr>
<tr><th id="571">571</th><td>        ToRNNInputMode(rnn_input_mode(), num_units, input_size, &amp;input_mode));</td></tr>
<tr><th id="572">572</th><td></td></tr>
<tr><th id="573">573</th><td>    <em>auto</em>* stream = context-&gt;op_device_context()-&gt;stream();</td></tr>
<tr><th id="574">574</th><td>    <i>// ExtracCudnnRNNParamsInfo is only called by op_kernels that do not require</i></td></tr>
<tr><th id="575">575</th><td><i>    // random number generator, therefore set state_allocator to nullptr.</i></td></tr>
<tr><th id="576">576</th><td>    <em>const</em> AlgorithmConfig algo_config;</td></tr>
<tr><th id="577">577</th><td>    <em>auto</em> rnn_desc_s = stream-&gt;parent()-&gt;createRnnDescriptor(</td></tr>
<tr><th id="578">578</th><td>        num_layers, num_units, input_size, input_mode, rnn_direction_mode(),</td></tr>
<tr><th id="579">579</th><td>        rnn_mode(), ToDataType&lt;T&gt;::value, algo_config, dropout(), seed(),</td></tr>
<tr><th id="580">580</th><td>        <b>nullptr</b> <i>/* state_allocator */</i>);</td></tr>
<tr><th id="581">581</th><td>    <b>if</b> (!rnn_desc_s.ok()) {</td></tr>
<tr><th id="582">582</th><td>      <b>return</b> FromExecutorStatus(rnn_desc_s);</td></tr>
<tr><th id="583">583</th><td>    }</td></tr>
<tr><th id="584">584</th><td>    *rnn_desc = rnn_desc_s.ConsumeValueOrDie();</td></tr>
<tr><th id="585">585</th><td>    <b>return</b> Status::OK();</td></tr>
<tr><th id="586">586</th><td>  }</td></tr>
<tr><th id="587">587</th><td></td></tr>
<tr><th id="588">588</th><td> <b>private</b>:</td></tr>
<tr><th id="589">589</th><td>  <em>int</em> seed_;</td></tr>
<tr><th id="590">590</th><td>  <em>int</em> seed2_;</td></tr>
<tr><th id="591">591</th><td>  <em>float</em> dropout_;</td></tr>
<tr><th id="592">592</th><td>  <em>bool</em> reset_rnd_gen_state_;</td></tr>
<tr><th id="593">593</th><td></td></tr>
<tr><th id="594">594</th><td>  CudnnModelTypes model_types_;</td></tr>
<tr><th id="595">595</th><td>};</td></tr>
<tr><th id="596">596</th><td></td></tr>
<tr><th id="597">597</th><td><i>// A class that returns the size of the opaque parameter buffer. The user should</i></td></tr>
<tr><th id="598">598</th><td><i>// use that to create the actual parameter buffer for training. However, it</i></td></tr>
<tr><th id="599">599</th><td><i>// should not be used for saving and restoring.</i></td></tr>
<tr><th id="600">600</th><td><b>template</b> &lt;<b>typename</b> T, <b>typename</b> Index&gt;</td></tr>
<tr><th id="601">601</th><td><b>class</b> CudnnRNNParamsSizeOp&lt;GPUDevice, T, Index&gt; : <b>public</b> CudnnRNNKernelCommon {</td></tr>
<tr><th id="602">602</th><td> <b>public</b>:</td></tr>
<tr><th id="603">603</th><td>  <b>typedef</b> GPUDevice Device;</td></tr>
<tr><th id="604">604</th><td>  <b>explicit</b> CudnnRNNParamsSizeOp(OpKernelConstruction* context)</td></tr>
<tr><th id="605">605</th><td>      : CudnnRNNKernelCommon(context) {}</td></tr>
<tr><th id="606">606</th><td></td></tr>
<tr><th id="607">607</th><td>  <em>void</em> Compute(OpKernelContext* context) override {</td></tr>
<tr><th id="608">608</th><td>    std::unique_ptr&lt;RnnDescriptor&gt; rnn_desc;</td></tr>
<tr><th id="609">609</th><td>    OP_REQUIRES_OK(context, ExtractCudnnRNNParamsInfo&lt;T&gt;(context, &amp;rnn_desc));</td></tr>
<tr><th id="610">610</th><td>    int64 params_size_in_bytes = rnn_desc-&gt;ParamsSizeInBytes();</td></tr>
<tr><th id="611">611</th><td>    CHECK(params_size_in_bytes % <b>sizeof</b>(T) == <var>0</var>)</td></tr>
<tr><th id="612">612</th><td>        &lt;&lt; <q>"params_size_in_bytes must be multiple of element size"</q>;</td></tr>
<tr><th id="613">613</th><td>    int64 params_size = params_size_in_bytes / <b>sizeof</b>(T);</td></tr>
<tr><th id="614">614</th><td></td></tr>
<tr><th id="615">615</th><td>    Tensor* output_t = <b>nullptr</b>;</td></tr>
<tr><th id="616">616</th><td>    OP_REQUIRES_OK(context, context-&gt;allocate_output(<var>0</var>, {<var>1</var>}, &amp;output_t));</td></tr>
<tr><th id="617">617</th><td>    *output_t-&gt;<b>template</b> flat&lt;Index&gt;().data() = params_size;</td></tr>
<tr><th id="618">618</th><td>  }</td></tr>
<tr><th id="619">619</th><td>};</td></tr>
<tr><th id="620">620</th><td></td></tr>
<tr><th id="621">621</th><td><u>#define REGISTER_GPU(T)                                    \</u></td></tr>
<tr><th id="622">622</th><td><u>  REGISTER_KERNEL_BUILDER(Name("CudnnRNNParamsSize")       \</u></td></tr>
<tr><th id="623">623</th><td><u>                              .Device(DEVICE_GPU)          \</u></td></tr>
<tr><th id="624">624</th><td><u>                              .HostMemory("num_layers")    \</u></td></tr>
<tr><th id="625">625</th><td><u>                              .HostMemory("num_units")     \</u></td></tr>
<tr><th id="626">626</th><td><u>                              .HostMemory("input_size")    \</u></td></tr>
<tr><th id="627">627</th><td><u>                              .HostMemory("params_size")   \</u></td></tr>
<tr><th id="628">628</th><td><u>                              .TypeConstraint&lt;T&gt;("T")      \</u></td></tr>
<tr><th id="629">629</th><td><u>                              .TypeConstraint&lt;int32&gt;("S"), \</u></td></tr>
<tr><th id="630">630</th><td><u>                          CudnnRNNParamsSizeOp&lt;GPUDevice, T, int32&gt;);</u></td></tr>
<tr><th id="631">631</th><td></td></tr>
<tr><th id="632">632</th><td>TF_CALL_half(REGISTER_GPU);</td></tr>
<tr><th id="633">633</th><td>TF_CALL_float(REGISTER_GPU);</td></tr>
<tr><th id="634">634</th><td>TF_CALL_double(REGISTER_GPU);</td></tr>
<tr><th id="635">635</th><td><u>#undef REGISTER_GPU</u></td></tr>
<tr><th id="636">636</th><td></td></tr>
<tr><th id="637">637</th><td><i>// Convert weight and bias params from a platform-specific layout to the</i></td></tr>
<tr><th id="638">638</th><td><i>// canonical form.</i></td></tr>
<tr><th id="639">639</th><td><b>template</b> &lt;<b>typename</b> T&gt;</td></tr>
<tr><th id="640">640</th><td><b>class</b> CudnnRNNParamsToCanonical&lt;GPUDevice, T&gt; : <b>public</b> CudnnRNNKernelCommon {</td></tr>
<tr><th id="641">641</th><td> <b>public</b>:</td></tr>
<tr><th id="642">642</th><td>  <b>typedef</b> GPUDevice Device;</td></tr>
<tr><th id="643">643</th><td>  <b>explicit</b> CudnnRNNParamsToCanonical(OpKernelConstruction* context)</td></tr>
<tr><th id="644">644</th><td>      : CudnnRNNKernelCommon(context) {</td></tr>
<tr><th id="645">645</th><td>    OP_REQUIRES_OK(context, context-&gt;GetAttr(<q>"num_params"</q>, &amp;num_params_));</td></tr>
<tr><th id="646">646</th><td>  }</td></tr>
<tr><th id="647">647</th><td></td></tr>
<tr><th id="648">648</th><td>  <em>void</em> Compute(OpKernelContext* context) override {</td></tr>
<tr><th id="649">649</th><td>    <em>const</em> Tensor&amp; input = context-&gt;input(<var>3</var>);</td></tr>
<tr><th id="650">650</th><td>    <em>auto</em> input_ptr = StreamExecutorUtil::AsDeviceMemory&lt;T&gt;(input);</td></tr>
<tr><th id="651">651</th><td>    <em>auto</em>* stream = context-&gt;op_device_context()-&gt;stream();</td></tr>
<tr><th id="652">652</th><td></td></tr>
<tr><th id="653">653</th><td>    std::unique_ptr&lt;RnnDescriptor&gt; rnn_desc;</td></tr>
<tr><th id="654">654</th><td>    OP_REQUIRES_OK(context, ExtractCudnnRNNParamsInfo&lt;T&gt;(context, &amp;rnn_desc));</td></tr>
<tr><th id="655">655</th><td>    int64 params_size_in_bytes = rnn_desc-&gt;ParamsSizeInBytes();</td></tr>
<tr><th id="656">656</th><td>    CHECK(params_size_in_bytes % <b>sizeof</b>(T) == <var>0</var>)</td></tr>
<tr><th id="657">657</th><td>        &lt;&lt; <q>"params_size_in_bytes must be multiple of element size"</q>;</td></tr>
<tr><th id="658">658</th><td></td></tr>
<tr><th id="659">659</th><td>    <em>const</em> Tensor* num_units_t = <b>nullptr</b>;</td></tr>
<tr><th id="660">660</th><td>    OP_REQUIRES_OK(context, context-&gt;input(<q>"num_units"</q>, &amp;num_units_t));</td></tr>
<tr><th id="661">661</th><td>    CHECK(TensorShapeUtils::IsScalar(num_units_t-&gt;shape()))</td></tr>
<tr><th id="662">662</th><td>        &lt;&lt; <q>"num_units is not a scalar"</q>;</td></tr>
<tr><th id="663">663</th><td>    <em>int</em> num_units = num_units_t-&gt;scalar&lt;<em>int</em>&gt;()();</td></tr>
<tr><th id="664">664</th><td></td></tr>
<tr><th id="665">665</th><td>    <em>const</em> Tensor* input_size_t = <b>nullptr</b>;</td></tr>
<tr><th id="666">666</th><td>    OP_REQUIRES_OK(context, context-&gt;input(<q>"input_size"</q>, &amp;input_size_t));</td></tr>
<tr><th id="667">667</th><td>    CHECK(TensorShapeUtils::IsScalar(input_size_t-&gt;shape()))</td></tr>
<tr><th id="668">668</th><td>        &lt;&lt; <q>"input_size is not a scalar"</q>;</td></tr>
<tr><th id="669">669</th><td>    <em>int</em> input_size = input_size_t-&gt;scalar&lt;<em>int</em>&gt;()();</td></tr>
<tr><th id="670">670</th><td></td></tr>
<tr><th id="671">671</th><td>    <em>const</em> Tensor* num_layers_t = <b>nullptr</b>;</td></tr>
<tr><th id="672">672</th><td>    OP_REQUIRES_OK(context, context-&gt;input(<q>"num_layers"</q>, &amp;num_layers_t));</td></tr>
<tr><th id="673">673</th><td>    CHECK(TensorShapeUtils::IsScalar(num_layers_t-&gt;shape()))</td></tr>
<tr><th id="674">674</th><td>        &lt;&lt; <q>"num_layers is not a scalar"</q>;</td></tr>
<tr><th id="675">675</th><td>    <em>int</em> num_layers = num_layers_t-&gt;scalar&lt;<em>int</em>&gt;()();</td></tr>
<tr><th id="676">676</th><td>    <em>int</em> num_dirs = <var>1</var>;</td></tr>
<tr><th id="677">677</th><td>    <b>if</b> (rnn_direction_mode() == RnnDirectionMode::kRnnBidirectional) {</td></tr>
<tr><th id="678">678</th><td>      num_dirs = <var>2</var>;</td></tr>
<tr><th id="679">679</th><td>    }</td></tr>
<tr><th id="680">680</th><td>    <em>const</em> <em>int</em> num_params_per_layer = num_params_ / num_layers / num_dirs;</td></tr>
<tr><th id="681">681</th><td>    <i>// Number of params applied on inputs. The rest are applied on recurrent</i></td></tr>
<tr><th id="682">682</th><td><i>    // hidden states.</i></td></tr>
<tr><th id="683">683</th><td>    <em>const</em> <em>int</em> num_params_input_state = num_params_per_layer / <var>2</var>;</td></tr>
<tr><th id="684">684</th><td>    CHECK(num_params_ % (num_layers * num_dirs) == <var>0</var>)</td></tr>
<tr><th id="685">685</th><td>        &lt;&lt; <q>"Number of params is not a multiple of num_layers * num_dirs."</q>;</td></tr>
<tr><th id="686">686</th><td>    CHECK(num_params_per_layer % <var>2</var> == <var>0</var>)</td></tr>
<tr><th id="687">687</th><td>        &lt;&lt; <q>"Number of params per layer is not a even number."</q>;</td></tr>
<tr><th id="688">688</th><td></td></tr>
<tr><th id="689">689</th><td>    CHECK(num_params_ == rnn_desc-&gt;ParamsWeightRegions().size())</td></tr>
<tr><th id="690">690</th><td>        &lt;&lt; <q>"Number of params mismatch. Expected "</q> &lt;&lt; num_params_ &lt;&lt; <q>", got "</q></td></tr>
<tr><th id="691">691</th><td>        &lt;&lt; rnn_desc-&gt;ParamsWeightRegions().size();</td></tr>
<tr><th id="692">692</th><td>    <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; rnn_desc-&gt;ParamsWeightRegions().size(); i++) {</td></tr>
<tr><th id="693">693</th><td>      int64 size_in_bytes = rnn_desc-&gt;ParamsWeightRegions()[i].size;</td></tr>
<tr><th id="694">694</th><td>      int64 size = size_in_bytes / <b>sizeof</b>(T);</td></tr>
<tr><th id="695">695</th><td>      <em>const</em> <em>int</em> layer_idx = i / num_params_per_layer;</td></tr>
<tr><th id="696">696</th><td>      <em>const</em> <em>int</em> index_within_layer = i % num_params_per_layer;</td></tr>
<tr><th id="697">697</th><td>      <em>int</em> width = <var>0</var>, height = num_units;</td></tr>
<tr><th id="698">698</th><td>      <i>// In CuDNN layout, each layer has num_params_per_layer params, with the</i></td></tr>
<tr><th id="699">699</th><td><i>      // first half a.k.a num_params_input_state params applied on the inputs,</i></td></tr>
<tr><th id="700">700</th><td><i>      // and the second half on the recurrent hidden states.</i></td></tr>
<tr><th id="701">701</th><td>      <em>bool</em> apply_on_input_state = index_within_layer &lt; num_params_input_state;</td></tr>
<tr><th id="702">702</th><td>      <b>if</b> (rnn_direction_mode() == RnnDirectionMode::kRnnUnidirectional) {</td></tr>
<tr><th id="703">703</th><td>        <b>if</b> (layer_idx == <var>0</var> &amp;&amp; apply_on_input_state) {</td></tr>
<tr><th id="704">704</th><td>          width = input_size;</td></tr>
<tr><th id="705">705</th><td>        } <b>else</b> {</td></tr>
<tr><th id="706">706</th><td>          width = num_units;</td></tr>
<tr><th id="707">707</th><td>        }</td></tr>
<tr><th id="708">708</th><td>      } <b>else</b> {</td></tr>
<tr><th id="709">709</th><td>        <b>if</b> (apply_on_input_state) {</td></tr>
<tr><th id="710">710</th><td>          <b>if</b> (layer_idx &lt;= <var>1</var>) {</td></tr>
<tr><th id="711">711</th><td>            <i>// First fwd or bak layer.</i></td></tr>
<tr><th id="712">712</th><td>            width = input_size;</td></tr>
<tr><th id="713">713</th><td>          } <b>else</b> {</td></tr>
<tr><th id="714">714</th><td>            <i>// Following layers, cell inputs are concatenated outputs of</i></td></tr>
<tr><th id="715">715</th><td><i>            // its prior layer.</i></td></tr>
<tr><th id="716">716</th><td>            width = <var>2</var> * num_units;</td></tr>
<tr><th id="717">717</th><td>          }</td></tr>
<tr><th id="718">718</th><td>        } <b>else</b> {</td></tr>
<tr><th id="719">719</th><td>          width = num_units;</td></tr>
<tr><th id="720">720</th><td>        }</td></tr>
<tr><th id="721">721</th><td>      }</td></tr>
<tr><th id="722">722</th><td>      CHECK(size == width * height) &lt;&lt; <q>"Params size mismatch. Expected "</q></td></tr>
<tr><th id="723">723</th><td>                                    &lt;&lt; width * height &lt;&lt; <q>", got "</q> &lt;&lt; size;</td></tr>
<tr><th id="724">724</th><td>      Tensor* output = <b>nullptr</b>;</td></tr>
<tr><th id="725">725</th><td>      OP_REQUIRES_OK(context, context-&gt;allocate_output(</td></tr>
<tr><th id="726">726</th><td>                                  i, TensorShape({height, width}), &amp;output));</td></tr>
<tr><th id="727">727</th><td>      DeviceMemoryBase data_src_ptr = SliceDeviceMemory(</td></tr>
<tr><th id="728">728</th><td>          input_ptr, rnn_desc-&gt;ParamsWeightRegions()[i].offset, size_in_bytes);</td></tr>
<tr><th id="729">729</th><td>      <em>auto</em> data_dst_ptr = StreamExecutorUtil::AsDeviceMemory&lt;T&gt;(*output);</td></tr>
<tr><th id="730">730</th><td>      stream-&gt;ThenMemcpy(&amp;data_dst_ptr, data_src_ptr, size_in_bytes);</td></tr>
<tr><th id="731">731</th><td>    }</td></tr>
<tr><th id="732">732</th><td></td></tr>
<tr><th id="733">733</th><td>    OP_REQUIRES(context, num_params_ == rnn_desc-&gt;ParamsBiasRegions().size(),</td></tr>
<tr><th id="734">734</th><td>                errors::InvalidArgument(<q>"Number of params mismatch. Expected "</q>,</td></tr>
<tr><th id="735">735</th><td>                                        num_params_, <q>", got "</q>,</td></tr>
<tr><th id="736">736</th><td>                                        rnn_desc-&gt;ParamsBiasRegions().size()));</td></tr>
<tr><th id="737">737</th><td>    <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; rnn_desc-&gt;ParamsBiasRegions().size(); i++) {</td></tr>
<tr><th id="738">738</th><td>      int64 size_in_bytes = rnn_desc-&gt;ParamsBiasRegions()[i].size;</td></tr>
<tr><th id="739">739</th><td>      int64 size = size_in_bytes / <b>sizeof</b>(T);</td></tr>
<tr><th id="740">740</th><td>      OP_REQUIRES(context, size == num_units,</td></tr>
<tr><th id="741">741</th><td>                  errors::InvalidArgument(<q>"Params size mismatch. Expected "</q>,</td></tr>
<tr><th id="742">742</th><td>                                          num_units, <q>", got "</q>, size));</td></tr>
<tr><th id="743">743</th><td></td></tr>
<tr><th id="744">744</th><td>      Tensor* output = <b>nullptr</b>;</td></tr>
<tr><th id="745">745</th><td>      OP_REQUIRES_OK(context,</td></tr>
<tr><th id="746">746</th><td>                     context-&gt;allocate_output(num_params_ + i,</td></tr>
<tr><th id="747">747</th><td>                                              TensorShape({size}), &amp;output));</td></tr>
<tr><th id="748">748</th><td>      DeviceMemoryBase data_src_ptr = SliceDeviceMemory(</td></tr>
<tr><th id="749">749</th><td>          input_ptr, rnn_desc-&gt;ParamsBiasRegions()[i].offset, size_in_bytes);</td></tr>
<tr><th id="750">750</th><td>      <em>auto</em> data_dst_ptr = StreamExecutorUtil::AsDeviceMemory&lt;T&gt;(*output);</td></tr>
<tr><th id="751">751</th><td>      stream-&gt;ThenMemcpy(&amp;data_dst_ptr, data_src_ptr, size_in_bytes);</td></tr>
<tr><th id="752">752</th><td>    }</td></tr>
<tr><th id="753">753</th><td>  }</td></tr>
<tr><th id="754">754</th><td></td></tr>
<tr><th id="755">755</th><td> <b>private</b>:</td></tr>
<tr><th id="756">756</th><td>  <em>int</em> num_params_;</td></tr>
<tr><th id="757">757</th><td>};</td></tr>
<tr><th id="758">758</th><td></td></tr>
<tr><th id="759">759</th><td><u>#define REGISTER_GPU(T)                                     \</u></td></tr>
<tr><th id="760">760</th><td><u>  REGISTER_KERNEL_BUILDER(Name("CudnnRNNParamsToCanonical") \</u></td></tr>
<tr><th id="761">761</th><td><u>                              .Device(DEVICE_GPU)           \</u></td></tr>
<tr><th id="762">762</th><td><u>                              .HostMemory("num_layers")     \</u></td></tr>
<tr><th id="763">763</th><td><u>                              .HostMemory("num_units")      \</u></td></tr>
<tr><th id="764">764</th><td><u>                              .HostMemory("input_size")     \</u></td></tr>
<tr><th id="765">765</th><td><u>                              .TypeConstraint&lt;T&gt;("T"),      \</u></td></tr>
<tr><th id="766">766</th><td><u>                          CudnnRNNParamsToCanonical&lt;GPUDevice, T&gt;);</u></td></tr>
<tr><th id="767">767</th><td>TF_CALL_half(REGISTER_GPU);</td></tr>
<tr><th id="768">768</th><td>TF_CALL_float(REGISTER_GPU);</td></tr>
<tr><th id="769">769</th><td>TF_CALL_double(REGISTER_GPU);</td></tr>
<tr><th id="770">770</th><td><u>#undef REGISTER_GPU</u></td></tr>
<tr><th id="771">771</th><td></td></tr>
<tr><th id="772">772</th><td><i>// Convert weight and bias params from the canonical form to a</i></td></tr>
<tr><th id="773">773</th><td><i>// platform-specific layout.</i></td></tr>
<tr><th id="774">774</th><td><b>template</b> &lt;<b>typename</b> T&gt;</td></tr>
<tr><th id="775">775</th><td><b>class</b> CudnnRNNCanonicalToParams&lt;GPUDevice, T&gt; : <b>public</b> CudnnRNNKernelCommon {</td></tr>
<tr><th id="776">776</th><td> <b>public</b>:</td></tr>
<tr><th id="777">777</th><td>  <b>typedef</b> GPUDevice Device;</td></tr>
<tr><th id="778">778</th><td>  <b>explicit</b> CudnnRNNCanonicalToParams(OpKernelConstruction* context)</td></tr>
<tr><th id="779">779</th><td>      : CudnnRNNKernelCommon(context) {}</td></tr>
<tr><th id="780">780</th><td></td></tr>
<tr><th id="781">781</th><td>  <em>void</em> Compute(OpKernelContext* context) override {</td></tr>
<tr><th id="782">782</th><td>    std::unique_ptr&lt;RnnDescriptor&gt; rnn_desc;</td></tr>
<tr><th id="783">783</th><td>    OP_REQUIRES_OK(context, ExtractCudnnRNNParamsInfo&lt;T&gt;(context, &amp;rnn_desc));</td></tr>
<tr><th id="784">784</th><td>    int64 params_size_in_bytes = rnn_desc-&gt;ParamsSizeInBytes();</td></tr>
<tr><th id="785">785</th><td>    CHECK(params_size_in_bytes % <b>sizeof</b>(T) == <var>0</var>)</td></tr>
<tr><th id="786">786</th><td>        &lt;&lt; <q>"params_size_in_bytes must be multiple of element size"</q>;</td></tr>
<tr><th id="787">787</th><td>    Tensor* output = <b>nullptr</b>;</td></tr>
<tr><th id="788">788</th><td>    <em>int</em> params_size = params_size_in_bytes / <b>sizeof</b>(T);</td></tr>
<tr><th id="789">789</th><td>    OP_REQUIRES_OK(context,</td></tr>
<tr><th id="790">790</th><td>                   context-&gt;allocate_output(<var>0</var>, {params_size}, &amp;output));</td></tr>
<tr><th id="791">791</th><td>    <em>auto</em> output_ptr = StreamExecutorUtil::AsDeviceMemory&lt;T&gt;(*output);</td></tr>
<tr><th id="792">792</th><td>    <em>auto</em>* stream = context-&gt;op_device_context()-&gt;stream();</td></tr>
<tr><th id="793">793</th><td></td></tr>
<tr><th id="794">794</th><td>    OpInputList weights;</td></tr>
<tr><th id="795">795</th><td>    OP_REQUIRES_OK(context, context-&gt;input_list(<q>"weights"</q>, &amp;weights));</td></tr>
<tr><th id="796">796</th><td>    RestoreParams&lt;T&gt;(weights, rnn_desc-&gt;ParamsWeightRegions(), &amp;output_ptr,</td></tr>
<tr><th id="797">797</th><td>                     stream);</td></tr>
<tr><th id="798">798</th><td></td></tr>
<tr><th id="799">799</th><td>    OpInputList biases;</td></tr>
<tr><th id="800">800</th><td>    OP_REQUIRES_OK(context, context-&gt;input_list(<q>"biases"</q>, &amp;biases));</td></tr>
<tr><th id="801">801</th><td>    RestoreParams&lt;T&gt;(biases, rnn_desc-&gt;ParamsBiasRegions(), &amp;output_ptr,</td></tr>
<tr><th id="802">802</th><td>                     stream);</td></tr>
<tr><th id="803">803</th><td>  }</td></tr>
<tr><th id="804">804</th><td>};</td></tr>
<tr><th id="805">805</th><td></td></tr>
<tr><th id="806">806</th><td><u>#define REGISTER_GPU(T)                                     \</u></td></tr>
<tr><th id="807">807</th><td><u>  REGISTER_KERNEL_BUILDER(Name("CudnnRNNCanonicalToParams") \</u></td></tr>
<tr><th id="808">808</th><td><u>                              .Device(DEVICE_GPU)           \</u></td></tr>
<tr><th id="809">809</th><td><u>                              .HostMemory("num_layers")     \</u></td></tr>
<tr><th id="810">810</th><td><u>                              .HostMemory("num_units")      \</u></td></tr>
<tr><th id="811">811</th><td><u>                              .HostMemory("input_size")     \</u></td></tr>
<tr><th id="812">812</th><td><u>                              .TypeConstraint&lt;T&gt;("T"),      \</u></td></tr>
<tr><th id="813">813</th><td><u>                          CudnnRNNCanonicalToParams&lt;GPUDevice, T&gt;);</u></td></tr>
<tr><th id="814">814</th><td>TF_CALL_half(REGISTER_GPU);</td></tr>
<tr><th id="815">815</th><td>TF_CALL_float(REGISTER_GPU);</td></tr>
<tr><th id="816">816</th><td>TF_CALL_double(REGISTER_GPU);</td></tr>
<tr><th id="817">817</th><td><u>#undef REGISTER_GPU</u></td></tr>
<tr><th id="818">818</th><td></td></tr>
<tr><th id="819">819</th><td><i>// Pointers to RNN scratch space for a specific set of shape parameters (used as</i></td></tr>
<tr><th id="820">820</th><td><i>// a hash table value in CudnnRNNForwardOp and CudnnRNNBackwardOp).</i></td></tr>
<tr><th id="821">821</th><td><b>struct</b> RnnScratchSpace {</td></tr>
<tr><th id="822">822</th><td>  std::unique_ptr&lt;RnnDescriptor&gt; rnn_desc;</td></tr>
<tr><th id="823">823</th><td>  std::unique_ptr&lt;CudnnRNNPersistentSpaceAllocator&gt; dropout_state_allocator;</td></tr>
<tr><th id="824">824</th><td>};</td></tr>
<tr><th id="825">825</th><td></td></tr>
<tr><th id="826">826</th><td><i>// Run the forward operation of the RNN model.</i></td></tr>
<tr><th id="827">827</th><td><b>template</b> &lt;<b>typename</b> T&gt;</td></tr>
<tr><th id="828">828</th><td><b>class</b> CudnnRNNForwardOp&lt;GPUDevice, T&gt; : <b>public</b> CudnnRNNKernelCommon {</td></tr>
<tr><th id="829">829</th><td> <b>public</b>:</td></tr>
<tr><th id="830">830</th><td>  <b>typedef</b> GPUDevice Device;</td></tr>
<tr><th id="831">831</th><td>  <b>explicit</b> CudnnRNNForwardOp(OpKernelConstruction* context)</td></tr>
<tr><th id="832">832</th><td>      : CudnnRNNKernelCommon(context) {</td></tr>
<tr><th id="833">833</th><td>    OP_REQUIRES_OK(context, context-&gt;GetAttr(<q>"is_training"</q>, &amp;is_training_));</td></tr>
<tr><th id="834">834</th><td>  }</td></tr>
<tr><th id="835">835</th><td></td></tr>
<tr><th id="836">836</th><td>  <em>void</em> Compute(OpKernelContext* context) override {</td></tr>
<tr><th id="837">837</th><td>    <em>const</em> Tensor* input = <b>nullptr</b>;</td></tr>
<tr><th id="838">838</th><td>    <em>const</em> Tensor* input_h = <b>nullptr</b>;</td></tr>
<tr><th id="839">839</th><td>    <em>const</em> Tensor* input_c = <b>nullptr</b>;</td></tr>
<tr><th id="840">840</th><td>    <em>const</em> Tensor* params = <b>nullptr</b>;</td></tr>
<tr><th id="841">841</th><td>    CudnnRnnModelShapes model_shapes;</td></tr>
<tr><th id="842">842</th><td>    OP_REQUIRES_OK(context,</td></tr>
<tr><th id="843">843</th><td>                   ExtractForwardInput(context, model_types(), &amp;input, &amp;input_h,</td></tr>
<tr><th id="844">844</th><td>                                       &amp;input_c, &amp;params, &amp;model_shapes));</td></tr>
<tr><th id="845">845</th><td>    <em>const</em> <em>auto</em>&amp; input_shape = model_shapes.input_shape;</td></tr>
<tr><th id="846">846</th><td>    <em>const</em> <em>auto</em>&amp; hidden_state_shape = model_shapes.hidden_state_shape;</td></tr>
<tr><th id="847">847</th><td>    <em>const</em> <em>auto</em>&amp; output_shape = model_shapes.output_shape;</td></tr>
<tr><th id="848">848</th><td></td></tr>
<tr><th id="849">849</th><td>    Tensor* output = <b>nullptr</b>;</td></tr>
<tr><th id="850">850</th><td>    OP_REQUIRES_OK(context, context-&gt;allocate_output(<var>0</var>, output_shape, &amp;output));</td></tr>
<tr><th id="851">851</th><td>    Tensor* output_h = <b>nullptr</b>;</td></tr>
<tr><th id="852">852</th><td>    OP_REQUIRES_OK(context,</td></tr>
<tr><th id="853">853</th><td>                   context-&gt;allocate_output(<var>1</var>, hidden_state_shape, &amp;output_h));</td></tr>
<tr><th id="854">854</th><td>    Tensor* output_c = <b>nullptr</b>;</td></tr>
<tr><th id="855">855</th><td>    <b>if</b> (HasInputC()) {</td></tr>
<tr><th id="856">856</th><td>      <i>// Only LSTM uses input_c and output_c. So for all other models, we only</i></td></tr>
<tr><th id="857">857</th><td><i>      // need to create dummy outputs.</i></td></tr>
<tr><th id="858">858</th><td>      OP_REQUIRES_OK(</td></tr>
<tr><th id="859">859</th><td>          context, context-&gt;allocate_output(<var>2</var>, hidden_state_shape, &amp;output_c));</td></tr>
<tr><th id="860">860</th><td>    } <b>else</b> {</td></tr>
<tr><th id="861">861</th><td>      OP_REQUIRES_OK(context, context-&gt;allocate_output(<var>2</var>, {}, &amp;output_c));</td></tr>
<tr><th id="862">862</th><td>    }</td></tr>
<tr><th id="863">863</th><td></td></tr>
<tr><th id="864">864</th><td>    <em>auto</em>* stream = context-&gt;op_device_context()-&gt;stream();</td></tr>
<tr><th id="865">865</th><td>    <em>auto</em>* executor = stream-&gt;parent();</td></tr>
<tr><th id="866">866</th><td>    RnnInputMode input_mode;</td></tr>
<tr><th id="867">867</th><td>    OP_REQUIRES_OK(context,</td></tr>
<tr><th id="868">868</th><td>                   ToRNNInputMode(rnn_input_mode(), model_shapes.num_units,</td></tr>
<tr><th id="869">869</th><td>                                  model_shapes.input_size, &amp;input_mode));</td></tr>
<tr><th id="870">870</th><td>    <em>auto</em> data_type = ToDataType&lt;T&gt;::value;</td></tr>
<tr><th id="871">871</th><td></td></tr>
<tr><th id="872">872</th><td>    <em>auto</em> input_desc_s = executor-&gt;createRnnSequenceTensorDescriptor(</td></tr>
<tr><th id="873">873</th><td>        input_shape.dim_size(<var>0</var>), input_shape.dim_size(<var>1</var>),</td></tr>
<tr><th id="874">874</th><td>        input_shape.dim_size(<var>2</var>), data_type);</td></tr>
<tr><th id="875">875</th><td>    OP_REQUIRES_OK(context, FromExecutorStatus(input_desc_s));</td></tr>
<tr><th id="876">876</th><td>    <em>auto</em> input_desc = input_desc_s.ConsumeValueOrDie();</td></tr>
<tr><th id="877">877</th><td></td></tr>
<tr><th id="878">878</th><td>    <em>auto</em> hidden_state_desc_s = executor-&gt;createRnnStateTensorDescriptor(</td></tr>
<tr><th id="879">879</th><td>        hidden_state_shape.dim_size(<var>0</var>), hidden_state_shape.dim_size(<var>1</var>),</td></tr>
<tr><th id="880">880</th><td>        hidden_state_shape.dim_size(<var>2</var>), data_type);</td></tr>
<tr><th id="881">881</th><td>    OP_REQUIRES_OK(context, FromExecutorStatus(hidden_state_desc_s));</td></tr>
<tr><th id="882">882</th><td>    <em>auto</em> hidden_state_desc = hidden_state_desc_s.ConsumeValueOrDie();</td></tr>
<tr><th id="883">883</th><td></td></tr>
<tr><th id="884">884</th><td>    <em>auto</em> output_desc_s = executor-&gt;createRnnSequenceTensorDescriptor(</td></tr>
<tr><th id="885">885</th><td>        output_shape.dim_size(<var>0</var>), output_shape.dim_size(<var>1</var>),</td></tr>
<tr><th id="886">886</th><td>        output_shape.dim_size(<var>2</var>), data_type);</td></tr>
<tr><th id="887">887</th><td>    OP_REQUIRES_OK(context, FromExecutorStatus(output_desc_s));</td></tr>
<tr><th id="888">888</th><td>    <em>auto</em> output_desc = output_desc_s.ConsumeValueOrDie();</td></tr>
<tr><th id="889">889</th><td></td></tr>
<tr><th id="890">890</th><td>    <em>auto</em> input_data = AsDeviceMemory&lt;T&gt;(input);</td></tr>
<tr><th id="891">891</th><td>    <em>auto</em> input_h_data = AsDeviceMemory&lt;T&gt;(input_h);</td></tr>
<tr><th id="892">892</th><td>    DeviceMemory&lt;T&gt; input_c_data;</td></tr>
<tr><th id="893">893</th><td>    <b>if</b> (HasInputC()) {</td></tr>
<tr><th id="894">894</th><td>      input_c_data = AsDeviceMemory&lt;T&gt;(input_c);</td></tr>
<tr><th id="895">895</th><td>    }</td></tr>
<tr><th id="896">896</th><td>    <em>auto</em> params_data = AsDeviceMemory&lt;T&gt;(params);</td></tr>
<tr><th id="897">897</th><td>    <em>auto</em> output_data = AsDeviceMemory&lt;T&gt;(output);</td></tr>
<tr><th id="898">898</th><td>    <em>auto</em> output_h_data = AsDeviceMemory&lt;T&gt;(output_h);</td></tr>
<tr><th id="899">899</th><td>    DeviceMemory&lt;T&gt; output_c_data;</td></tr>
<tr><th id="900">900</th><td>    <b>if</b> (HasInputC()) {</td></tr>
<tr><th id="901">901</th><td>      output_c_data = AsDeviceMemory&lt;T&gt;(output_c);</td></tr>
<tr><th id="902">902</th><td>    }</td></tr>
<tr><th id="903">903</th><td></td></tr>
<tr><th id="904">904</th><td>    <i>// Creates a memory callback for the reserve_space. The memory lives in the</i></td></tr>
<tr><th id="905">905</th><td><i>    // output of this kernel. And it will be fed into the backward pass when</i></td></tr>
<tr><th id="906">906</th><td><i>    // needed.</i></td></tr>
<tr><th id="907">907</th><td>    CudnnRnnAllocatorInOutput&lt;T&gt; reserve_space_allocator(context, <var>3</var>);</td></tr>
<tr><th id="908">908</th><td>    <b>if</b> (!is_training_) {</td></tr>
<tr><th id="909">909</th><td>      Tensor* dummy_reserve_space = <b>nullptr</b>;</td></tr>
<tr><th id="910">910</th><td>      OP_REQUIRES_OK(context,</td></tr>
<tr><th id="911">911</th><td>                     context-&gt;allocate_output(<var>3</var>, {}, &amp;dummy_reserve_space));</td></tr>
<tr><th id="912">912</th><td>    }</td></tr>
<tr><th id="913">913</th><td>    <i>// Creates a memory callback for the workspace. The memory lives to the end</i></td></tr>
<tr><th id="914">914</th><td><i>    // of this kernel calls.</i></td></tr>
<tr><th id="915">915</th><td>    CudnnRnnAllocatorInTemp&lt;uint8&gt; workspace_allocator(context);</td></tr>
<tr><th id="916">916</th><td>    <em>bool</em> launch_status = <b>false</b>;</td></tr>
<tr><th id="917">917</th><td>    {</td></tr>
<tr><th id="918">918</th><td>      mutex_lock l(mu_);</td></tr>
<tr><th id="919">919</th><td>      RnnScratchSpace&amp; rnn_state = rnn_state_cache_[model_shapes];</td></tr>
<tr><th id="920">920</th><td>      <b>if</b> (rnn_state.rnn_desc == <b>nullptr</b> || ResetRndGenState()) {</td></tr>
<tr><th id="921">921</th><td>        CudnnRNNPersistentSpaceAllocator* dropout_state_allocator =</td></tr>
<tr><th id="922">922</th><td>            <b>new</b> CudnnRNNPersistentSpaceAllocator(context);</td></tr>
<tr><th id="923">923</th><td>        rnn_state.dropout_state_allocator.reset(dropout_state_allocator);</td></tr>
<tr><th id="924">924</th><td>        <em>const</em> AlgorithmConfig algo_config;</td></tr>
<tr><th id="925">925</th><td>        <em>auto</em> rnn_desc_s = executor-&gt;createRnnDescriptor(</td></tr>
<tr><th id="926">926</th><td>            model_shapes.num_layers, model_shapes.num_units,</td></tr>
<tr><th id="927">927</th><td>            model_shapes.input_size, input_mode, rnn_direction_mode(),</td></tr>
<tr><th id="928">928</th><td>            rnn_mode(), data_type, algo_config, dropout(), seed(),</td></tr>
<tr><th id="929">929</th><td>            dropout_state_allocator);</td></tr>
<tr><th id="930">930</th><td>        OP_REQUIRES_OK(context, FromExecutorStatus(rnn_desc_s));</td></tr>
<tr><th id="931">931</th><td>        rnn_state.rnn_desc = std::move(rnn_desc_s.ConsumeValueOrDie());</td></tr>
<tr><th id="932">932</th><td>      }</td></tr>
<tr><th id="933">933</th><td>      launch_status =</td></tr>
<tr><th id="934">934</th><td>          stream</td></tr>
<tr><th id="935">935</th><td>              -&gt;ThenRnnForward(</td></tr>
<tr><th id="936">936</th><td>                  *rnn_state.rnn_desc, *input_desc, input_data,</td></tr>
<tr><th id="937">937</th><td>                  *hidden_state_desc, input_h_data, *hidden_state_desc,</td></tr>
<tr><th id="938">938</th><td>                  input_c_data, params_data, *output_desc, &amp;output_data,</td></tr>
<tr><th id="939">939</th><td>                  *hidden_state_desc, &amp;output_h_data, *hidden_state_desc,</td></tr>
<tr><th id="940">940</th><td>                  &amp;output_c_data, is_training_, &amp;reserve_space_allocator,</td></tr>
<tr><th id="941">941</th><td>                  &amp;workspace_allocator, <i>/*output_result_profile=*/</i><b>nullptr</b>)</td></tr>
<tr><th id="942">942</th><td>              .ok();</td></tr>
<tr><th id="943">943</th><td>    }</td></tr>
<tr><th id="944">944</th><td>    OP_REQUIRES(context, launch_status,</td></tr>
<tr><th id="945">945</th><td>                errors::Internal(<q>"Failed to call ThenRnnForward"</q>));</td></tr>
<tr><th id="946">946</th><td>  }</td></tr>
<tr><th id="947">947</th><td></td></tr>
<tr><th id="948">948</th><td> <b>private</b>:</td></tr>
<tr><th id="949">949</th><td>  mutex mu_;</td></tr>
<tr><th id="950">950</th><td>  <em>bool</em> is_training_;</td></tr>
<tr><th id="951">951</th><td>  std::unordered_map&lt;CudnnRnnModelShapes, RnnScratchSpace,</td></tr>
<tr><th id="952">952</th><td>                     CudnnRnnModelShapesHasher, CudnnRnnModelShapesComparator&gt;</td></tr>
<tr><th id="953">953</th><td>      rnn_state_cache_ GUARDED_BY(mu_);</td></tr>
<tr><th id="954">954</th><td>};</td></tr>
<tr><th id="955">955</th><td></td></tr>
<tr><th id="956">956</th><td><u>#define REGISTER_GPU(T)                                           \</u></td></tr>
<tr><th id="957">957</th><td><u>  REGISTER_KERNEL_BUILDER(                                        \</u></td></tr>
<tr><th id="958">958</th><td><u>      Name("CudnnRNN").Device(DEVICE_GPU).TypeConstraint&lt;T&gt;("T"), \</u></td></tr>
<tr><th id="959">959</th><td><u>      CudnnRNNForwardOp&lt;GPUDevice, T&gt;);</u></td></tr>
<tr><th id="960">960</th><td></td></tr>
<tr><th id="961">961</th><td>TF_CALL_half(REGISTER_GPU);</td></tr>
<tr><th id="962">962</th><td>TF_CALL_float(REGISTER_GPU);</td></tr>
<tr><th id="963">963</th><td>TF_CALL_double(REGISTER_GPU);</td></tr>
<tr><th id="964">964</th><td><u>#undef REGISTER_GPU</u></td></tr>
<tr><th id="965">965</th><td></td></tr>
<tr><th id="966">966</th><td><i>// Run the backward operation of the RNN model.</i></td></tr>
<tr><th id="967">967</th><td><b>template</b> &lt;<b>typename</b> T&gt;</td></tr>
<tr><th id="968">968</th><td><b>class</b> CudnnRNNBackwardOp&lt;GPUDevice, T&gt; : <b>public</b> CudnnRNNKernelCommon {</td></tr>
<tr><th id="969">969</th><td> <b>public</b>:</td></tr>
<tr><th id="970">970</th><td>  <b>typedef</b> GPUDevice Device;</td></tr>
<tr><th id="971">971</th><td></td></tr>
<tr><th id="972">972</th><td>  <b>explicit</b> CudnnRNNBackwardOp(OpKernelConstruction* context)</td></tr>
<tr><th id="973">973</th><td>      : CudnnRNNKernelCommon(context) {}</td></tr>
<tr><th id="974">974</th><td></td></tr>
<tr><th id="975">975</th><td>  <em>void</em> Compute(OpKernelContext* context) override {</td></tr>
<tr><th id="976">976</th><td>    <em>const</em> Tensor* input = <b>nullptr</b>;</td></tr>
<tr><th id="977">977</th><td>    <em>const</em> Tensor* input_h = <b>nullptr</b>;</td></tr>
<tr><th id="978">978</th><td>    <em>const</em> Tensor* input_c = <b>nullptr</b>;</td></tr>
<tr><th id="979">979</th><td>    <em>const</em> Tensor* params = <b>nullptr</b>;</td></tr>
<tr><th id="980">980</th><td>    CudnnRnnModelShapes model_shapes;</td></tr>
<tr><th id="981">981</th><td>    OP_REQUIRES_OK(context,</td></tr>
<tr><th id="982">982</th><td>                   ExtractForwardInput(context, model_types(), &amp;input, &amp;input_h,</td></tr>
<tr><th id="983">983</th><td>                                       &amp;input_c, &amp;params, &amp;model_shapes));</td></tr>
<tr><th id="984">984</th><td></td></tr>
<tr><th id="985">985</th><td>    <em>const</em> <em>auto</em>&amp; input_shape = model_shapes.input_shape;</td></tr>
<tr><th id="986">986</th><td>    <em>const</em> <em>auto</em>&amp; hidden_state_shape = model_shapes.hidden_state_shape;</td></tr>
<tr><th id="987">987</th><td>    <em>const</em> <em>auto</em>&amp; output_shape = model_shapes.output_shape;</td></tr>
<tr><th id="988">988</th><td></td></tr>
<tr><th id="989">989</th><td>    <em>auto</em> data_type = ToDataType&lt;T&gt;::value;</td></tr>
<tr><th id="990">990</th><td>    <em>const</em> Tensor* output = <b>nullptr</b>;</td></tr>
<tr><th id="991">991</th><td>    OP_REQUIRES_OK(context, context-&gt;input(<q>"output"</q>, &amp;output));</td></tr>
<tr><th id="992">992</th><td>    OP_REQUIRES(context, output_shape == output-&gt;shape(),</td></tr>
<tr><th id="993">993</th><td>                errors::InvalidArgument(</td></tr>
<tr><th id="994">994</th><td>                    <q>"input_h and input_c must have the same shape: "</q>,</td></tr>
<tr><th id="995">995</th><td>                    input_h-&gt;shape().DebugString(), <q>" "</q>,</td></tr>
<tr><th id="996">996</th><td>                    input_c-&gt;shape().DebugString()));</td></tr>
<tr><th id="997">997</th><td>    <em>const</em> Tensor* output_h = <b>nullptr</b>;</td></tr>
<tr><th id="998">998</th><td>    OP_REQUIRES_OK(context, context-&gt;input(<q>"output_h"</q>, &amp;output_h));</td></tr>
<tr><th id="999">999</th><td>    OP_REQUIRES(context, output_h-&gt;shape() == hidden_state_shape,</td></tr>
<tr><th id="1000">1000</th><td>                errors::InvalidArgument(</td></tr>
<tr><th id="1001">1001</th><td>                    <q>"Invalid output_h shape: "</q>, output_h-&gt;shape().DebugString(),</td></tr>
<tr><th id="1002">1002</th><td>                    <q>" "</q>, hidden_state_shape.DebugString()));</td></tr>
<tr><th id="1003">1003</th><td>    <em>const</em> Tensor* output_c = <b>nullptr</b>;</td></tr>
<tr><th id="1004">1004</th><td>    <b>if</b> (HasInputC()) {</td></tr>
<tr><th id="1005">1005</th><td>      <i>// Only LSTM uses input_c and output_c. So for all other models, we only</i></td></tr>
<tr><th id="1006">1006</th><td><i>      // need to create dummy outputs.</i></td></tr>
<tr><th id="1007">1007</th><td>      OP_REQUIRES_OK(context, context-&gt;input(<q>"output_c"</q>, &amp;output_c));</td></tr>
<tr><th id="1008">1008</th><td>      OP_REQUIRES(context, output_c-&gt;shape() == hidden_state_shape,</td></tr>
<tr><th id="1009">1009</th><td>                  errors::InvalidArgument(<q>"Invalid output_c shape: "</q>,</td></tr>
<tr><th id="1010">1010</th><td>                                          output_c-&gt;shape().DebugString(), <q>" "</q>,</td></tr>
<tr><th id="1011">1011</th><td>                                          hidden_state_shape.DebugString()));</td></tr>
<tr><th id="1012">1012</th><td>    }</td></tr>
<tr><th id="1013">1013</th><td></td></tr>
<tr><th id="1014">1014</th><td>    <em>const</em> Tensor* output_backprop = <b>nullptr</b>;</td></tr>
<tr><th id="1015">1015</th><td>    OP_REQUIRES_OK(context,</td></tr>
<tr><th id="1016">1016</th><td>                   context-&gt;input(<q>"output_backprop"</q>, &amp;output_backprop));</td></tr>
<tr><th id="1017">1017</th><td>    OP_REQUIRES(context, output_backprop-&gt;shape() == output_shape,</td></tr>
<tr><th id="1018">1018</th><td>                errors::InvalidArgument(<q>"Invalid output_backprop shapes: "</q>,</td></tr>
<tr><th id="1019">1019</th><td>                                        output_backprop-&gt;shape().DebugString(),</td></tr>
<tr><th id="1020">1020</th><td>                                        <q>" "</q>, output_shape.DebugString()));</td></tr>
<tr><th id="1021">1021</th><td></td></tr>
<tr><th id="1022">1022</th><td>    <em>const</em> Tensor* output_h_backprop = <b>nullptr</b>;</td></tr>
<tr><th id="1023">1023</th><td>    OP_REQUIRES_OK(context,</td></tr>
<tr><th id="1024">1024</th><td>                   context-&gt;input(<q>"output_h_backprop"</q>, &amp;output_h_backprop));</td></tr>
<tr><th id="1025">1025</th><td>    OP_REQUIRES(</td></tr>
<tr><th id="1026">1026</th><td>        context, output_h_backprop-&gt;shape() == hidden_state_shape,</td></tr>
<tr><th id="1027">1027</th><td>        errors::InvalidArgument(<q>"Invalid output_h_backprop shapes: "</q>,</td></tr>
<tr><th id="1028">1028</th><td>                                output_h_backprop-&gt;shape().DebugString(), <q>" "</q>,</td></tr>
<tr><th id="1029">1029</th><td>                                hidden_state_shape.DebugString()));</td></tr>
<tr><th id="1030">1030</th><td>    <em>const</em> Tensor* output_c_backprop = <b>nullptr</b>;</td></tr>
<tr><th id="1031">1031</th><td>    <b>if</b> (HasInputC()) {</td></tr>
<tr><th id="1032">1032</th><td>      OP_REQUIRES_OK(context,</td></tr>
<tr><th id="1033">1033</th><td>                     context-&gt;input(<q>"output_c_backprop"</q>, &amp;output_c_backprop));</td></tr>
<tr><th id="1034">1034</th><td>      OP_REQUIRES(</td></tr>
<tr><th id="1035">1035</th><td>          context, output_c_backprop-&gt;shape() == hidden_state_shape,</td></tr>
<tr><th id="1036">1036</th><td>          errors::InvalidArgument(<q>"Invalid output_c_backprop shapes: "</q>,</td></tr>
<tr><th id="1037">1037</th><td>                                  output_c_backprop-&gt;shape().DebugString(), <q>" "</q>,</td></tr>
<tr><th id="1038">1038</th><td>                                  hidden_state_shape.DebugString()));</td></tr>
<tr><th id="1039">1039</th><td>    }</td></tr>
<tr><th id="1040">1040</th><td>    <em>const</em> Tensor* reserve_space_const = <b>nullptr</b>;</td></tr>
<tr><th id="1041">1041</th><td>    <i>// This is the same "reserve_space" created by the forward op.</i></td></tr>
<tr><th id="1042">1042</th><td><i>    // It can also be modified by this backward operation.</i></td></tr>
<tr><th id="1043">1043</th><td>    OP_REQUIRES_OK(context,</td></tr>
<tr><th id="1044">1044</th><td>                   context-&gt;input(<q>"reserve_space"</q>, &amp;reserve_space_const));</td></tr>
<tr><th id="1045">1045</th><td>    <i>// Cudnn needs the reserve space to be writeable. This is fine because they</i></td></tr>
<tr><th id="1046">1046</th><td><i>    // are opaque.</i></td></tr>
<tr><th id="1047">1047</th><td>    Tensor* reserve_space = <b>const_cast</b>&lt;Tensor*&gt;(reserve_space_const);</td></tr>
<tr><th id="1048">1048</th><td></td></tr>
<tr><th id="1049">1049</th><td>    Tensor* input_backprop = <b>nullptr</b>;</td></tr>
<tr><th id="1050">1050</th><td>    OP_REQUIRES_OK(</td></tr>
<tr><th id="1051">1051</th><td>        context, context-&gt;allocate_output(<var>0</var>, input-&gt;shape(), &amp;input_backprop));</td></tr>
<tr><th id="1052">1052</th><td>    Tensor* input_h_backprop = <b>nullptr</b>;</td></tr>
<tr><th id="1053">1053</th><td>    OP_REQUIRES_OK(context, context-&gt;allocate_output(<var>1</var>, input_h-&gt;shape(),</td></tr>
<tr><th id="1054">1054</th><td>                                                     &amp;input_h_backprop));</td></tr>
<tr><th id="1055">1055</th><td>    Tensor* input_c_backprop = <b>nullptr</b>;</td></tr>
<tr><th id="1056">1056</th><td>    <b>if</b> (HasInputC()) {</td></tr>
<tr><th id="1057">1057</th><td>      OP_REQUIRES_OK(context, context-&gt;allocate_output(<var>2</var>, input_c-&gt;shape(),</td></tr>
<tr><th id="1058">1058</th><td>                                                       &amp;input_c_backprop));</td></tr>
<tr><th id="1059">1059</th><td>    } <b>else</b> {</td></tr>
<tr><th id="1060">1060</th><td>      OP_REQUIRES_OK(context,</td></tr>
<tr><th id="1061">1061</th><td>                     context-&gt;allocate_output(<var>2</var>, {}, &amp;input_c_backprop));</td></tr>
<tr><th id="1062">1062</th><td>    }</td></tr>
<tr><th id="1063">1063</th><td>    Tensor* params_backprop = <b>nullptr</b>;</td></tr>
<tr><th id="1064">1064</th><td>    OP_REQUIRES_OK(context, context-&gt;allocate_output(<var>3</var>, params-&gt;shape(),</td></tr>
<tr><th id="1065">1065</th><td>                                                     &amp;params_backprop));</td></tr>
<tr><th id="1066">1066</th><td></td></tr>
<tr><th id="1067">1067</th><td>    <em>auto</em>* stream = context-&gt;op_device_context()-&gt;stream();</td></tr>
<tr><th id="1068">1068</th><td>    <em>auto</em>* executor = stream-&gt;parent();</td></tr>
<tr><th id="1069">1069</th><td>    RnnInputMode input_mode;</td></tr>
<tr><th id="1070">1070</th><td>    OP_REQUIRES_OK(context,</td></tr>
<tr><th id="1071">1071</th><td>                   ToRNNInputMode(rnn_input_mode(), model_shapes.num_units,</td></tr>
<tr><th id="1072">1072</th><td>                                  model_shapes.input_size, &amp;input_mode));</td></tr>
<tr><th id="1073">1073</th><td></td></tr>
<tr><th id="1074">1074</th><td>    <em>auto</em> input_desc_s = executor-&gt;createRnnSequenceTensorDescriptor(</td></tr>
<tr><th id="1075">1075</th><td>        input_shape.dim_size(<var>0</var>), input_shape.dim_size(<var>1</var>),</td></tr>
<tr><th id="1076">1076</th><td>        input_shape.dim_size(<var>2</var>), data_type);</td></tr>
<tr><th id="1077">1077</th><td>    OP_REQUIRES_OK(context, FromExecutorStatus(input_desc_s));</td></tr>
<tr><th id="1078">1078</th><td>    <em>auto</em> input_desc = input_desc_s.ConsumeValueOrDie();</td></tr>
<tr><th id="1079">1079</th><td></td></tr>
<tr><th id="1080">1080</th><td>    <em>auto</em> hidden_state_desc_s = executor-&gt;createRnnStateTensorDescriptor(</td></tr>
<tr><th id="1081">1081</th><td>        hidden_state_shape.dim_size(<var>0</var>), hidden_state_shape.dim_size(<var>1</var>),</td></tr>
<tr><th id="1082">1082</th><td>        hidden_state_shape.dim_size(<var>2</var>), data_type);</td></tr>
<tr><th id="1083">1083</th><td>    OP_REQUIRES_OK(context, FromExecutorStatus(hidden_state_desc_s));</td></tr>
<tr><th id="1084">1084</th><td>    <em>auto</em> hidden_state_desc = hidden_state_desc_s.ConsumeValueOrDie();</td></tr>
<tr><th id="1085">1085</th><td></td></tr>
<tr><th id="1086">1086</th><td>    <em>auto</em> output_desc_s = executor-&gt;createRnnSequenceTensorDescriptor(</td></tr>
<tr><th id="1087">1087</th><td>        output_shape.dim_size(<var>0</var>), output_shape.dim_size(<var>1</var>),</td></tr>
<tr><th id="1088">1088</th><td>        output_shape.dim_size(<var>2</var>), data_type);</td></tr>
<tr><th id="1089">1089</th><td>    OP_REQUIRES_OK(context, FromExecutorStatus(output_desc_s));</td></tr>
<tr><th id="1090">1090</th><td>    <em>auto</em> output_desc = output_desc_s.ConsumeValueOrDie();</td></tr>
<tr><th id="1091">1091</th><td></td></tr>
<tr><th id="1092">1092</th><td>    <em>auto</em> input_data = AsDeviceMemory&lt;T&gt;(input);</td></tr>
<tr><th id="1093">1093</th><td>    <em>auto</em> input_h_data = AsDeviceMemory&lt;T&gt;(input_h);</td></tr>
<tr><th id="1094">1094</th><td>    DeviceMemory&lt;T&gt; input_c_data;</td></tr>
<tr><th id="1095">1095</th><td>    <b>if</b> (HasInputC()) {</td></tr>
<tr><th id="1096">1096</th><td>      input_c_data = AsDeviceMemory&lt;T&gt;(input_c);</td></tr>
<tr><th id="1097">1097</th><td>    }</td></tr>
<tr><th id="1098">1098</th><td>    <em>auto</em> params_data = AsDeviceMemory&lt;T&gt;(params);</td></tr>
<tr><th id="1099">1099</th><td>    <em>auto</em> output_data = AsDeviceMemory&lt;T&gt;(output);</td></tr>
<tr><th id="1100">1100</th><td>    <em>auto</em> output_h_data = AsDeviceMemory&lt;T&gt;(output_h);</td></tr>
<tr><th id="1101">1101</th><td>    DeviceMemory&lt;T&gt; output_c_data;</td></tr>
<tr><th id="1102">1102</th><td>    <b>if</b> (HasInputC()) {</td></tr>
<tr><th id="1103">1103</th><td>      output_c_data = AsDeviceMemory&lt;T&gt;(output_c);</td></tr>
<tr><th id="1104">1104</th><td>    }</td></tr>
<tr><th id="1105">1105</th><td>    <em>auto</em> output_backprop_data = AsDeviceMemory&lt;T&gt;(output_backprop);</td></tr>
<tr><th id="1106">1106</th><td>    <em>auto</em> output_h_backprop_data = AsDeviceMemory&lt;T&gt;(output_h_backprop);</td></tr>
<tr><th id="1107">1107</th><td>    DeviceMemory&lt;T&gt; output_c_backprop_data;</td></tr>
<tr><th id="1108">1108</th><td>    <b>if</b> (HasInputC()) {</td></tr>
<tr><th id="1109">1109</th><td>      output_c_backprop_data = AsDeviceMemory&lt;T&gt;(output_c_backprop);</td></tr>
<tr><th id="1110">1110</th><td>    }</td></tr>
<tr><th id="1111">1111</th><td>    <em>auto</em> input_backprop_data = AsDeviceMemory&lt;T&gt;(input_backprop);</td></tr>
<tr><th id="1112">1112</th><td>    <em>auto</em> input_h_backprop_data = AsDeviceMemory&lt;T&gt;(input_h_backprop);</td></tr>
<tr><th id="1113">1113</th><td>    DeviceMemory&lt;T&gt; input_c_backprop_data;</td></tr>
<tr><th id="1114">1114</th><td>    <b>if</b> (HasInputC()) {</td></tr>
<tr><th id="1115">1115</th><td>      input_c_backprop_data = AsDeviceMemory&lt;T&gt;(input_c_backprop);</td></tr>
<tr><th id="1116">1116</th><td>    }</td></tr>
<tr><th id="1117">1117</th><td>    <em>auto</em> params_backprop_data = AsDeviceMemory&lt;T&gt;(params_backprop);</td></tr>
<tr><th id="1118">1118</th><td>    <em>auto</em> reserve_space_uint8 = CastDeviceMemory&lt;uint8, T&gt;(reserve_space);</td></tr>
<tr><th id="1119">1119</th><td>    <i>// Creates a memory callback for the workspace. The memory lives to the end</i></td></tr>
<tr><th id="1120">1120</th><td><i>    // of this kernel calls.</i></td></tr>
<tr><th id="1121">1121</th><td>    CudnnRnnAllocatorInTemp&lt;uint8&gt; workspace_allocator(context);</td></tr>
<tr><th id="1122">1122</th><td>    <em>bool</em> launch_status = <b>false</b>;</td></tr>
<tr><th id="1123">1123</th><td>    {</td></tr>
<tr><th id="1124">1124</th><td>      mutex_lock l(mu_);</td></tr>
<tr><th id="1125">1125</th><td>      RnnScratchSpace&amp; rnn_state = rnn_state_cache_[model_shapes];</td></tr>
<tr><th id="1126">1126</th><td>      <b>if</b> (rnn_state.rnn_desc == <b>nullptr</b> || ResetRndGenState()) {</td></tr>
<tr><th id="1127">1127</th><td>        CudnnRNNPersistentSpaceAllocator* dropout_state_allocator =</td></tr>
<tr><th id="1128">1128</th><td>            <b>new</b> CudnnRNNPersistentSpaceAllocator(context);</td></tr>
<tr><th id="1129">1129</th><td>        rnn_state.dropout_state_allocator.reset(dropout_state_allocator);</td></tr>
<tr><th id="1130">1130</th><td>        <em>const</em> AlgorithmConfig algo_config;</td></tr>
<tr><th id="1131">1131</th><td>        <em>auto</em> rnn_desc_s = executor-&gt;createRnnDescriptor(</td></tr>
<tr><th id="1132">1132</th><td>            model_shapes.num_layers, model_shapes.num_units,</td></tr>
<tr><th id="1133">1133</th><td>            model_shapes.input_size, input_mode, rnn_direction_mode(),</td></tr>
<tr><th id="1134">1134</th><td>            rnn_mode(), data_type, algo_config, dropout(), seed(),</td></tr>
<tr><th id="1135">1135</th><td>            dropout_state_allocator);</td></tr>
<tr><th id="1136">1136</th><td>        OP_REQUIRES_OK(context, FromExecutorStatus(rnn_desc_s));</td></tr>
<tr><th id="1137">1137</th><td>        rnn_state.rnn_desc = std::move(rnn_desc_s.ConsumeValueOrDie());</td></tr>
<tr><th id="1138">1138</th><td>      }</td></tr>
<tr><th id="1139">1139</th><td>      launch_status =</td></tr>
<tr><th id="1140">1140</th><td>          stream</td></tr>
<tr><th id="1141">1141</th><td>              -&gt;ThenRnnBackward(</td></tr>
<tr><th id="1142">1142</th><td>                  *rnn_state.rnn_desc, *input_desc, input_data,</td></tr>
<tr><th id="1143">1143</th><td>                  *hidden_state_desc, input_h_data, *hidden_state_desc,</td></tr>
<tr><th id="1144">1144</th><td>                  input_c_data, params_data, *output_desc, output_data,</td></tr>
<tr><th id="1145">1145</th><td>                  *hidden_state_desc, output_h_data, *hidden_state_desc,</td></tr>
<tr><th id="1146">1146</th><td>                  output_c_data, output_backprop_data, output_h_backprop_data,</td></tr>
<tr><th id="1147">1147</th><td>                  output_c_backprop_data, &amp;input_backprop_data,</td></tr>
<tr><th id="1148">1148</th><td>                  &amp;input_h_backprop_data, &amp;input_c_backprop_data,</td></tr>
<tr><th id="1149">1149</th><td>                  &amp;params_backprop_data, &amp;reserve_space_uint8,</td></tr>
<tr><th id="1150">1150</th><td>                  &amp;workspace_allocator, <i>/*output_result_profile=*/</i><b>nullptr</b>)</td></tr>
<tr><th id="1151">1151</th><td>              .ok();</td></tr>
<tr><th id="1152">1152</th><td>    }</td></tr>
<tr><th id="1153">1153</th><td>    OP_REQUIRES(context, launch_status,</td></tr>
<tr><th id="1154">1154</th><td>                errors::Internal(<q>"Failed to call ThenRnnBackward"</q>));</td></tr>
<tr><th id="1155">1155</th><td>  }</td></tr>
<tr><th id="1156">1156</th><td></td></tr>
<tr><th id="1157">1157</th><td> <b>private</b>:</td></tr>
<tr><th id="1158">1158</th><td>  mutex mu_;</td></tr>
<tr><th id="1159">1159</th><td>  std::unordered_map&lt;CudnnRnnModelShapes, RnnScratchSpace,</td></tr>
<tr><th id="1160">1160</th><td>                     CudnnRnnModelShapesHasher, CudnnRnnModelShapesComparator&gt;</td></tr>
<tr><th id="1161">1161</th><td>      rnn_state_cache_ GUARDED_BY(mu_);</td></tr>
<tr><th id="1162">1162</th><td>};</td></tr>
<tr><th id="1163">1163</th><td></td></tr>
<tr><th id="1164">1164</th><td><u>#define REGISTER_GPU(T)                                                   \</u></td></tr>
<tr><th id="1165">1165</th><td><u>  REGISTER_KERNEL_BUILDER(                                                \</u></td></tr>
<tr><th id="1166">1166</th><td><u>      Name("CudnnRNNBackprop").Device(DEVICE_GPU).TypeConstraint&lt;T&gt;("T"), \</u></td></tr>
<tr><th id="1167">1167</th><td><u>      CudnnRNNBackwardOp&lt;GPUDevice, T&gt;);</u></td></tr>
<tr><th id="1168">1168</th><td></td></tr>
<tr><th id="1169">1169</th><td>TF_CALL_half(REGISTER_GPU);</td></tr>
<tr><th id="1170">1170</th><td>TF_CALL_float(REGISTER_GPU);</td></tr>
<tr><th id="1171">1171</th><td>TF_CALL_double(REGISTER_GPU);</td></tr>
<tr><th id="1172">1172</th><td><u>#undef REGISTER_GPU</u></td></tr>
<tr><th id="1173">1173</th><td></td></tr>
<tr><th id="1174">1174</th><td><i>// TODO(zhengxq): Add the conversion of Cudnn RNN Params from and to</i></td></tr>
<tr><th id="1175">1175</th><td><i>// its canonical form.</i></td></tr>
<tr><th id="1176">1176</th><td></td></tr>
<tr><th id="1177">1177</th><td><u>#<span data-ppcond="78">endif</span>  // GOOGLE_CUDA</u></td></tr>
<tr><th id="1178">1178</th><td></td></tr>
<tr><th id="1179">1179</th><td>}  <i>// namespace tensorflow</i></td></tr>
<tr><th id="1180">1180</th><td></td></tr>
</table><hr/><p id='footer'>
Generated on <em>2018-Aug-16</em> from project tensorflow revision <em>v1.8</em><br />Powered by <a href='https://woboq.com'><img alt='Woboq' src='https://code.woboq.org/woboq-16.png' width='41' height='16' /></a> <a href='https://code.woboq.org'>Code Browser</a> 2.1
<br/>Generator usage only permitted with license.</p>
</div></body></html>
