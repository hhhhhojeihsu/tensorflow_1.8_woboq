<!doctype html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><title>runtime_matmul_mkl.cc source code [tensorflow/tensorflow/compiler/xla/service/cpu/runtime_matmul_mkl.cc] - Woboq Code Browser</title>
<link rel="stylesheet" href="https://code.woboq.org/data/qtcreator.css" title="QtCreator"/>
<link rel="alternate stylesheet" href="https://code.woboq.org/data/kdevelop.css" title="KDevelop"/>
<script type="text/javascript" src="https://code.woboq.org/data/jquery/jquery.min.js"></script>
<script type="text/javascript" src="https://code.woboq.org/data/jquery/jquery-ui.min.js"></script>
<script>var file = 'tensorflow/tensorflow/compiler/xla/service/cpu/runtime_matmul_mkl.cc'; var root_path = '../../../../../..'; var data_path = 'https://code.woboq.org/data';</script>
<script src='https://code.woboq.org/data/codebrowser.js'></script>
</head>
<body><div id='header'><h1 id='breadcrumb'><span>Browse the source code of </span><a href='../../../../..'>tensorflow</a>/<a href='../../../..'>tensorflow</a>/<a href='../../..'>compiler</a>/<a href='../..'>xla</a>/<a href='..'>service</a>/<a href='./'>cpu</a>/<a href='runtime_matmul_mkl.cc.html'>runtime_matmul_mkl.cc</a></h1></div>
<hr/><div id='content'><table class="code">
<tr><th id="1">1</th><td><i>/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.</i></td></tr>
<tr><th id="2">2</th><td><i></i></td></tr>
<tr><th id="3">3</th><td><i>Licensed under the Apache License, Version 2.0 (the "License");</i></td></tr>
<tr><th id="4">4</th><td><i>you may not use this file except in compliance with the License.</i></td></tr>
<tr><th id="5">5</th><td><i>You may obtain a copy of the License at</i></td></tr>
<tr><th id="6">6</th><td><i></i></td></tr>
<tr><th id="7">7</th><td><i>    <a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></i></td></tr>
<tr><th id="8">8</th><td><i></i></td></tr>
<tr><th id="9">9</th><td><i>Unless required by applicable law or agreed to in writing, software</i></td></tr>
<tr><th id="10">10</th><td><i>distributed under the License is distributed on an "AS IS" BASIS,</i></td></tr>
<tr><th id="11">11</th><td><i>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</i></td></tr>
<tr><th id="12">12</th><td><i>See the License for the specific language governing permissions and</i></td></tr>
<tr><th id="13">13</th><td><i>limitations under the License.</i></td></tr>
<tr><th id="14">14</th><td><i>==============================================================================*/</i></td></tr>
<tr><th id="15">15</th><td></td></tr>
<tr><th id="16">16</th><td><u>#<span data-ppcond="16">ifdef</span> <span class="macro" data-ref="_M/INTEL_MKL">INTEL_MKL</span></u></td></tr>
<tr><th id="17">17</th><td><u>#include "tensorflow/compiler/xla/service/cpu/runtime_matmul_mkl.h"</u></td></tr>
<tr><th id="18">18</th><td><u>#include "third_party/intel_mkl_ml/include/mkl_cblas.h"</u></td></tr>
<tr><th id="19">19</th><td><u>#include "third_party/intel_mkl_ml/include/mkl_service.h"</u></td></tr>
<tr><th id="20">20</th><td></td></tr>
<tr><th id="21">21</th><td><u>#include "tensorflow/compiler/xla/executable_run_options.h"</u></td></tr>
<tr><th id="22">22</th><td><u>#include "tensorflow/core/platform/types.h"</u></td></tr>
<tr><th id="23">23</th><td></td></tr>
<tr><th id="24">24</th><td><u>#define EIGEN_USE_THREADS</u></td></tr>
<tr><th id="25">25</th><td><u>#include "third_party/eigen3/unsupported/Eigen/CXX11/ThreadPool"</u></td></tr>
<tr><th id="26">26</th><td></td></tr>
<tr><th id="27">27</th><td><b>using</b> tensorflow::int32;</td></tr>
<tr><th id="28">28</th><td><b>using</b> tensorflow::int64;</td></tr>
<tr><th id="29">29</th><td></td></tr>
<tr><th id="30">30</th><td><b>namespace</b> {</td></tr>
<tr><th id="31">31</th><td><i>// BLAS GEMM API for 32-bit Matrix Multiplication.</i></td></tr>
<tr><th id="32">32</th><td><i></i></td></tr>
<tr><th id="33">33</th><td><i>// MatMul function is defined as: c = alpha * op(a) * op(b) + beta * c.</i></td></tr>
<tr><th id="34">34</th><td><i>// Since XLA MatMul does not used alpha, beta, we set them to 1.0 and 0.0.</i></td></tr>
<tr><th id="35">35</th><td><i>// Matrix lhs, rhs and out are all colum-major.</i></td></tr>
<tr><th id="36">36</th><td><em>void</em> MatMulF32(<em>const</em> <em>void</em>* run_options_ptr, <em>float</em>* out, <em>float</em>* lhs, <em>float</em>* rhs,</td></tr>
<tr><th id="37">37</th><td>               int64 m, int64 n, int64 k, int32 transpose_lhs,</td></tr>
<tr><th id="38">38</th><td>               int32 transpose_rhs) {</td></tr>
<tr><th id="39">39</th><td>  <em>const</em> <em>float</em> alpha = <var>1.0f</var>, beta = <var>0.0f</var>;</td></tr>
<tr><th id="40">40</th><td>  <i>// lda, ldb, and ldc are the leading dimensions of matrices a, b, and c,</i></td></tr>
<tr><th id="41">41</th><td><i>  // respectively. For column-major matrices, the leading dimension is the</i></td></tr>
<tr><th id="42">42</th><td><i>  // stride between consecutive columns (which equals the number of rows). If</i></td></tr>
<tr><th id="43">43</th><td><i>  // the matrix is transposed, the leading dimension is the stride between</i></td></tr>
<tr><th id="44">44</th><td><i>  // consecutive rows (which equals the number of columns).</i></td></tr>
<tr><th id="45">45</th><td>  <em>int</em> lda = transpose_lhs ? k : m;</td></tr>
<tr><th id="46">46</th><td>  <em>int</em> ldb = transpose_rhs ? n : k;</td></tr>
<tr><th id="47">47</th><td>  <em>int</em> ldc = m;</td></tr>
<tr><th id="48">48</th><td>  cblas_sgemm(CblasColMajor, transpose_lhs ? CblasTrans : CblasNoTrans,</td></tr>
<tr><th id="49">49</th><td>              transpose_rhs ? CblasTrans : CblasNoTrans, m, n, k, alpha, lhs,</td></tr>
<tr><th id="50">50</th><td>              lda, rhs, ldb, beta, out, ldc);</td></tr>
<tr><th id="51">51</th><td>}</td></tr>
<tr><th id="52">52</th><td></td></tr>
<tr><th id="53">53</th><td><i>// BLAS GEMM API for 64-bit Matrix Multiplication.</i></td></tr>
<tr><th id="54">54</th><td><i></i></td></tr>
<tr><th id="55">55</th><td><i>// MatMul function is defined as: c = alpha * op(a) * op(b) + beta * c.</i></td></tr>
<tr><th id="56">56</th><td><i>// Since XLA MatMul does not used alpha, beta, we set them to 1.0 and 0.0.</i></td></tr>
<tr><th id="57">57</th><td><i>// Matrix lhs, rhs and out are all colum-major.</i></td></tr>
<tr><th id="58">58</th><td><em>void</em> MatMulF64(<em>const</em> <em>void</em>* run_options_ptr, <em>double</em>* out, <em>double</em>* lhs,</td></tr>
<tr><th id="59">59</th><td>               <em>double</em>* rhs, int64 m, int64 n, int64 k, int32 transpose_lhs,</td></tr>
<tr><th id="60">60</th><td>               int32 transpose_rhs) {</td></tr>
<tr><th id="61">61</th><td>  <em>const</em> <em>float</em> alpha = <var>1.0f</var>, beta = <var>0.0f</var>;</td></tr>
<tr><th id="62">62</th><td>  <i>// lda, ldb, and ldc are the leading dimensions of matrices a, b, and c,</i></td></tr>
<tr><th id="63">63</th><td><i>  // respectively. For a column-major matrix, the leading dimension is the</i></td></tr>
<tr><th id="64">64</th><td><i>  // stride between consecutive columns (which equals the number of rows). If</i></td></tr>
<tr><th id="65">65</th><td><i>  // the matrix is transposed, the leading dimension is the stride between</i></td></tr>
<tr><th id="66">66</th><td><i>  // consecutive rows (which equals the number of columns).</i></td></tr>
<tr><th id="67">67</th><td>  <em>int</em> lda = transpose_lhs ? k : m;</td></tr>
<tr><th id="68">68</th><td>  <em>int</em> ldb = transpose_rhs ? n : k;</td></tr>
<tr><th id="69">69</th><td>  <em>int</em> ldc = m;</td></tr>
<tr><th id="70">70</th><td>  cblas_dgemm(CblasColMajor, transpose_lhs ? CblasTrans : CblasNoTrans,</td></tr>
<tr><th id="71">71</th><td>              transpose_rhs ? CblasTrans : CblasNoTrans, m, n, k, alpha, lhs,</td></tr>
<tr><th id="72">72</th><td>              lda, rhs, ldb, beta, out, ldc);</td></tr>
<tr><th id="73">73</th><td>}</td></tr>
<tr><th id="74">74</th><td></td></tr>
<tr><th id="75">75</th><td>}  <i>// namespace</i></td></tr>
<tr><th id="76">76</th><td></td></tr>
<tr><th id="77">77</th><td><em>void</em> __xla_cpu_runtime_MKLMatMulF32(<em>const</em> <em>void</em>* run_options_ptr, <em>float</em>* out,</td></tr>
<tr><th id="78">78</th><td>                                    <em>float</em>* lhs, <em>float</em>* rhs, int64 m, int64 n,</td></tr>
<tr><th id="79">79</th><td>                                    int64 k, int32 transpose_lhs,</td></tr>
<tr><th id="80">80</th><td>                                    int32 transpose_rhs) {</td></tr>
<tr><th id="81">81</th><td>  <em>const</em> xla::ExecutableRunOptions* run_options =</td></tr>
<tr><th id="82">82</th><td>      <b>static_cast</b>&lt;<em>const</em> xla::ExecutableRunOptions*&gt;(run_options_ptr);</td></tr>
<tr><th id="83">83</th><td>  <i>// BLAS GEMM MatMul uses OpenMP for parallelization, so we pass the thread</i></td></tr>
<tr><th id="84">84</th><td><i>  // number specified in intra_op_thread_pool to MKL.</i></td></tr>
<tr><th id="85">85</th><td>  <em>int</em> prev_num_threads = mkl_set_num_threads_local(</td></tr>
<tr><th id="86">86</th><td>      run_options-&gt;intra_op_thread_pool()-&gt;numThreads());</td></tr>
<tr><th id="87">87</th><td>  MatMulF32(<b>nullptr</b>, out, lhs, rhs, m, n, k, transpose_lhs, transpose_rhs);</td></tr>
<tr><th id="88">88</th><td>  <i>// Set thread number back to the previous number.</i></td></tr>
<tr><th id="89">89</th><td>  mkl_set_num_threads_local(prev_num_threads);</td></tr>
<tr><th id="90">90</th><td>}</td></tr>
<tr><th id="91">91</th><td><i>// BLAS GEMM API for 64-bit Matrix Multiplication</i></td></tr>
<tr><th id="92">92</th><td><em>void</em> __xla_cpu_runtime_MKLMatMulF64(<em>const</em> <em>void</em>* run_options_ptr, <em>double</em>* out,</td></tr>
<tr><th id="93">93</th><td>                                    <em>double</em>* lhs, <em>double</em>* rhs, int64 m, int64 n,</td></tr>
<tr><th id="94">94</th><td>                                    int64 k, int32 transpose_lhs,</td></tr>
<tr><th id="95">95</th><td>                                    int32 transpose_rhs) {</td></tr>
<tr><th id="96">96</th><td>  <em>const</em> xla::ExecutableRunOptions* run_options =</td></tr>
<tr><th id="97">97</th><td>      <b>static_cast</b>&lt;<em>const</em> xla::ExecutableRunOptions*&gt;(run_options_ptr);</td></tr>
<tr><th id="98">98</th><td>  <i>// BLAS GEMM MatMul uses OpenMP for parallelization, so we pass the thread</i></td></tr>
<tr><th id="99">99</th><td><i>  // number specified in intra_op_thread_pool to MKL.</i></td></tr>
<tr><th id="100">100</th><td>  <em>int</em> prev_num_threads = mkl_set_num_threads_local(</td></tr>
<tr><th id="101">101</th><td>      run_options-&gt;intra_op_thread_pool()-&gt;numThreads());</td></tr>
<tr><th id="102">102</th><td>  MatMulF64(<b>nullptr</b>, out, lhs, rhs, m, n, k, transpose_lhs, transpose_rhs);</td></tr>
<tr><th id="103">103</th><td>  <i>// Set thread number back to the previous number.</i></td></tr>
<tr><th id="104">104</th><td>  mkl_set_num_threads_local(prev_num_threads);</td></tr>
<tr><th id="105">105</th><td>}</td></tr>
<tr><th id="106">106</th><td><em>void</em> __xla_cpu_runtime_MKLSingleThreadedMatMulF32(<em>const</em> <em>void</em>* run_options_ptr,</td></tr>
<tr><th id="107">107</th><td>                                                  <em>float</em>* out, <em>float</em>* lhs,</td></tr>
<tr><th id="108">108</th><td>                                                  <em>float</em>* rhs, int64 m, int64 n,</td></tr>
<tr><th id="109">109</th><td>                                                  int64 k, int32 transpose_lhs,</td></tr>
<tr><th id="110">110</th><td>                                                  int32 transpose_rhs) {</td></tr>
<tr><th id="111">111</th><td>  <i>// Set the thread number to 1 for single threaded excution.</i></td></tr>
<tr><th id="112">112</th><td>  <em>int</em> prev_num_threads = mkl_set_num_threads_local(<var>1</var>);</td></tr>
<tr><th id="113">113</th><td>  MatMulF32(<b>nullptr</b>, out, lhs, rhs, m, n, k, transpose_lhs, transpose_rhs);</td></tr>
<tr><th id="114">114</th><td>  <i>// Set thread number back to the previous number.</i></td></tr>
<tr><th id="115">115</th><td>  mkl_set_num_threads_local(prev_num_threads);</td></tr>
<tr><th id="116">116</th><td>}</td></tr>
<tr><th id="117">117</th><td><em>void</em> __xla_cpu_runtime_MKLSingleThreadedMatMulF64(<em>const</em> <em>void</em>* run_options_ptr,</td></tr>
<tr><th id="118">118</th><td>                                                  <em>double</em>* out, <em>double</em>* lhs,</td></tr>
<tr><th id="119">119</th><td>                                                  <em>double</em>* rhs, int64 m, int64 n,</td></tr>
<tr><th id="120">120</th><td>                                                  int64 k, int32 transpose_lhs,</td></tr>
<tr><th id="121">121</th><td>                                                  int32 transpose_rhs) {</td></tr>
<tr><th id="122">122</th><td>  <i>// Set the thread number to 1 for single threaded excution.</i></td></tr>
<tr><th id="123">123</th><td>  <em>int</em> prev_num_threads = mkl_set_num_threads_local(<var>1</var>);</td></tr>
<tr><th id="124">124</th><td>  MatMulF64(<b>nullptr</b>, out, lhs, rhs, m, n, k, transpose_lhs, transpose_rhs);</td></tr>
<tr><th id="125">125</th><td>  <i>// Set thread number back to the previous number.</i></td></tr>
<tr><th id="126">126</th><td>  mkl_set_num_threads_local(prev_num_threads);</td></tr>
<tr><th id="127">127</th><td>}</td></tr>
<tr><th id="128">128</th><td><u>#<span data-ppcond="16">endif</span>  // INTEL_MKL</u></td></tr>
<tr><th id="129">129</th><td></td></tr>
</table><hr/><p id='footer'>
Generated on <em>2018-Aug-09</em> from project tensorflow revision <em>v1.8</em><br />Powered by <a href='https://woboq.com'><img alt='Woboq' src='https://code.woboq.org/woboq-16.png' width='41' height='16' /></a> <a href='https://code.woboq.org'>Code Browser</a> 2.1
<br/>Generator usage only permitted with license.</p>
</div></body></html>
